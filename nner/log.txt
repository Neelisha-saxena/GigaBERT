F1 ================== EXP =====================
F1 Target language: ar
batchsize: 32
learning rate: 0.0001000
max epochs: 10
max_seq_length: 128
num_depulicate: 64
warmup proportion: 0.40000
model ckpt will be saved at: bibert_1.0m.ckpt
model config will be saved at: bibert_1.0m.cfg
Reading file:  en/train.txt
Data size:  7634
Loading training data...

Reading file:  en/train.txt
Data size:  7634
Dataset set: train, orig size: 7634
Loading development data...

Reading file:  en/dev.txt
Data size:  1005
Dataset set: dev, orig size: 1005
Loading testing data...

Reading file:  ar/test.txt
Data size:  238
Dataset set: test, orig size: 238
Training started...
Epoch: 0, step: 0, training loss: 0.09197
Epoch: 0, step: 1, training loss: 0.08106
Epoch: 0, step: 2, training loss: 0.12411
Epoch: 0, step: 3, training loss: 0.08772
Epoch: 0, step: 4, training loss: 0.10811
Epoch: 0, step: 5, training loss: 0.09107
Epoch: 0, step: 6, training loss: 0.11589
Epoch: 0, step: 7, training loss: 0.11382
Epoch: 0, step: 8, training loss: 0.07843
Epoch: 0, step: 9, training loss: 0.10312
Epoch: 0, step: 10, training loss: 0.10946
Epoch: 0, step: 11, training loss: 0.10761
Epoch: 0, step: 12, training loss: 0.10653
Epoch: 0, step: 13, training loss: 0.12547
Epoch: 0, step: 14, training loss: 0.10346
Epoch: 0, step: 15, training loss: 0.10820
Epoch: 0, step: 16, training loss: 0.08653
Epoch: 0, step: 17, training loss: 0.09107
Epoch: 0, step: 18, training loss: 0.08730
Epoch: 0, step: 19, training loss: 0.08229
Epoch: 0, step: 20, training loss: 0.07501
Epoch: 0, step: 21, training loss: 0.08483
Epoch: 0, step: 22, training loss: 0.09080
Epoch: 0, step: 23, training loss: 0.08155
Epoch: 0, step: 24, training loss: 0.08912
Epoch: 0, step: 25, training loss: 0.08233
Epoch: 0, step: 26, training loss: 0.07989
Epoch: 0, step: 27, training loss: 0.06830
Epoch: 0, step: 28, training loss: 0.06436
Epoch: 0, step: 29, training loss: 0.08220
Epoch: 0, step: 30, training loss: 0.07857
Epoch: 0, step: 31, training loss: 0.06477
Epoch: 0, step: 32, training loss: 0.06030
Epoch: 0, step: 33, training loss: 0.07134
Epoch: 0, step: 34, training loss: 0.04182
Epoch: 0, step: 35, training loss: 0.04706
Epoch: 0, step: 36, training loss: 0.04914
Epoch: 0, step: 37, training loss: 0.04863
Epoch: 0, step: 38, training loss: 0.04640
Epoch: 0, step: 39, training loss: 0.04308
Epoch: 0, step: 40, training loss: 0.04800
Epoch: 0, step: 41, training loss: 0.03806
Epoch: 0, step: 42, training loss: 0.03475
Epoch: 0, step: 43, training loss: 0.03924
Epoch: 0, step: 44, training loss: 0.03456
Epoch: 0, step: 45, training loss: 0.02957
Epoch: 0, step: 46, training loss: 0.03065
Epoch: 0, step: 47, training loss: 0.02866
Epoch: 0, step: 48, training loss: 0.02798
Epoch: 0, step: 49, training loss: 0.02715
Epoch: 0, step: 50, training loss: 0.03338
Epoch: 0, step: 51, training loss: 0.02873
Epoch: 0, step: 52, training loss: 0.02318
Epoch: 0, step: 53, training loss: 0.02826
Epoch: 0, step: 54, training loss: 0.02648
Epoch: 0, step: 55, training loss: 0.02263
Epoch: 0, step: 56, training loss: 0.02482
Epoch: 0, step: 57, training loss: 0.01990
Epoch: 0, step: 58, training loss: 0.02259
Epoch: 0, step: 59, training loss: 0.02558
Epoch: 0, step: 60, training loss: 0.02018
Epoch: 0, step: 61, training loss: 0.02387
Epoch: 0, step: 62, training loss: 0.02070
Epoch: 0, step: 63, training loss: 0.01794
Epoch: 0, step: 64, training loss: 0.02185
Epoch: 0, step: 65, training loss: 0.01860
Epoch: 0, step: 66, training loss: 0.01953
Epoch: 0, step: 67, training loss: 0.01686
Epoch: 0, step: 68, training loss: 0.01875
Epoch: 0, step: 69, training loss: 0.01946
Epoch: 0, step: 70, training loss: 0.02124
Epoch: 0, step: 71, training loss: 0.01886
Epoch: 0, step: 72, training loss: 0.02119
Epoch: 0, step: 73, training loss: 0.01683
Epoch: 0, step: 74, training loss: 0.01861
Epoch: 0, step: 75, training loss: 0.01848
Epoch: 0, step: 76, training loss: 0.01840
Epoch: 0, step: 77, training loss: 0.01808
Epoch: 0, step: 78, training loss: 0.01670
Epoch: 0, step: 79, training loss: 0.01883
Epoch: 0, step: 80, training loss: 0.01711
Epoch: 0, step: 81, training loss: 0.01665
Epoch: 0, step: 82, training loss: 0.02114
Epoch: 0, step: 83, training loss: 0.01891
Epoch: 0, step: 84, training loss: 0.01869
Epoch: 0, step: 85, training loss: 0.01507
Epoch: 0, step: 86, training loss: 0.01760
Epoch: 0, step: 87, training loss: 0.02147
Epoch: 0, step: 88, training loss: 0.02114
Epoch: 0, step: 89, training loss: 0.01389
Epoch: 0, step: 90, training loss: 0.02074
Epoch: 0, step: 91, training loss: 0.02042
Epoch: 0, step: 92, training loss: 0.01635
Epoch: 0, step: 93, training loss: 0.01631
Epoch: 0, step: 94, training loss: 0.01721
Epoch: 0, step: 95, training loss: 0.01922
Epoch: 0, step: 96, training loss: 0.02041
Epoch: 0, step: 97, training loss: 0.01244
Epoch: 0, step: 98, training loss: 0.01528
Epoch: 0, step: 99, training loss: 0.01475
Epoch: 0, step: 100, training loss: 0.01889
Epoch: 0, step: 101, training loss: 0.01806
Epoch: 0, step: 102, training loss: 0.01539
Epoch: 0, step: 103, training loss: 0.01934
Epoch: 0, step: 104, training loss: 0.01300
Epoch: 0, step: 105, training loss: 0.01873
Epoch: 0, step: 106, training loss: 0.01598
Epoch: 0, step: 107, training loss: 0.01790
Epoch: 0, step: 108, training loss: 0.01481
Epoch: 0, step: 109, training loss: 0.01629
Epoch: 0, step: 110, training loss: 0.01459
Epoch: 0, step: 111, training loss: 0.01773
Epoch: 0, step: 112, training loss: 0.01667
Epoch: 0, step: 113, training loss: 0.02076
Epoch: 0, step: 114, training loss: 0.01540
Epoch: 0, step: 115, training loss: 0.01430
Epoch: 0, step: 116, training loss: 0.01352
Epoch: 0, step: 117, training loss: 0.01524
Epoch: 0, step: 118, training loss: 0.01526
Epoch: 0, step: 119, training loss: 0.01646
Epoch: 0, step: 120, training loss: 0.02066
Epoch: 0, step: 121, training loss: 0.01829
Epoch: 0, step: 122, training loss: 0.01370
Epoch: 0, step: 123, training loss: 0.02022
Epoch: 0, step: 124, training loss: 0.01636
Epoch: 0, step: 125, training loss: 0.01917
Epoch: 0, step: 126, training loss: 0.01947
Epoch: 0, step: 127, training loss: 0.02012
Epoch: 0, step: 128, training loss: 0.02079
Epoch: 0, step: 129, training loss: 0.01733
Epoch: 0, step: 130, training loss: 0.01606
Epoch: 0, step: 131, training loss: 0.01699
Epoch: 0, step: 132, training loss: 0.01386
Epoch: 0, step: 133, training loss: 0.01770
Epoch: 0, step: 134, training loss: 0.01500
Epoch: 0, step: 135, training loss: 0.01523
Epoch: 0, step: 136, training loss: 0.01803
Epoch: 0, step: 137, training loss: 0.01385
Epoch: 0, step: 138, training loss: 0.01589
Epoch: 0, step: 139, training loss: 0.01291
Epoch: 0, step: 140, training loss: 0.01546
Epoch: 0, step: 141, training loss: 0.01953
Epoch: 0, step: 142, training loss: 0.01199
Epoch: 0, step: 143, training loss: 0.01349
Epoch: 0, step: 144, training loss: 0.01468
Epoch: 0, step: 145, training loss: 0.01704
Epoch: 0, step: 146, training loss: 0.01398
Epoch: 0, step: 147, training loss: 0.01434
Epoch: 0, step: 148, training loss: 0.01590
Epoch: 0, step: 149, training loss: 0.01480
Epoch: 0, step: 150, training loss: 0.01587
Epoch: 0, step: 151, training loss: 0.01826
Epoch: 0, step: 152, training loss: 0.01592
Epoch: 0, step: 153, training loss: 0.01488
Epoch: 0, step: 154, training loss: 0.01656
Epoch: 0, step: 155, training loss: 0.01295
Epoch: 0, step: 156, training loss: 0.01746
Epoch: 0, step: 157, training loss: 0.01419
Epoch: 0, step: 158, training loss: 0.01419
Epoch: 0, step: 159, training loss: 0.01693
Epoch: 0, step: 160, training loss: 0.01755
Epoch: 0, step: 161, training loss: 0.01207
Epoch: 0, step: 162, training loss: 0.01306
Epoch: 0, step: 163, training loss: 0.01537
Epoch: 0, step: 164, training loss: 0.01759
Epoch: 0, step: 165, training loss: 0.01206
Epoch: 0, step: 166, training loss: 0.01153
Epoch: 0, step: 167, training loss: 0.01142
Epoch: 0, step: 168, training loss: 0.01248
Epoch: 0, step: 169, training loss: 0.01136
Epoch: 0, step: 170, training loss: 0.01247
Epoch: 0, step: 171, training loss: 0.01360
Epoch: 0, step: 172, training loss: 0.01385
Epoch: 0, step: 173, training loss: 0.01372
Epoch: 0, step: 174, training loss: 0.01109
Epoch: 0, step: 175, training loss: 0.01124
Epoch: 0, step: 176, training loss: 0.01203
Epoch: 0, step: 177, training loss: 0.01699
Epoch: 0, step: 178, training loss: 0.01112
Epoch: 0, step: 179, training loss: 0.01562
Epoch: 0, step: 180, training loss: 0.01596
Epoch: 0, step: 181, training loss: 0.01242
Epoch: 0, step: 182, training loss: 0.01070
Epoch: 0, step: 183, training loss: 0.01735
Epoch: 0, step: 184, training loss: 0.01115
Epoch: 0, step: 185, training loss: 0.01285
Epoch: 0, step: 186, training loss: 0.01369
Epoch: 0, step: 187, training loss: 0.01249
Epoch: 0, step: 188, training loss: 0.01395
Epoch: 0, step: 189, training loss: 0.01579
Epoch: 0, step: 190, training loss: 0.01133
Epoch: 0, step: 191, training loss: 0.01023
Epoch: 0, step: 192, training loss: 0.01092
Epoch: 0, step: 193, training loss: 0.01361
Epoch: 0, step: 194, training loss: 0.01414
Epoch: 0, step: 195, training loss: 0.01356
Epoch: 0, step: 196, training loss: 0.01183
Epoch: 0, step: 197, training loss: 0.01063
Epoch: 0, step: 198, training loss: 0.01505
Epoch: 0, step: 199, training loss: 0.01198
Epoch: 0, step: 200, training loss: 0.01281
Epoch: 0, step: 201, training loss: 0.01196
Epoch: 0, step: 202, training loss: 0.01223
Epoch: 0, step: 203, training loss: 0.01350
Epoch: 0, step: 204, training loss: 0.00968
Epoch: 0, step: 205, training loss: 0.01198
Epoch: 0, step: 206, training loss: 0.01168
Epoch: 0, step: 207, training loss: 0.01254
Epoch: 0, step: 208, training loss: 0.01200
Epoch: 0, step: 209, training loss: 0.01653
Epoch: 0, step: 210, training loss: 0.01347
Epoch: 0, step: 211, training loss: 0.01163
Epoch: 0, step: 212, training loss: 0.01462
Epoch: 0, step: 213, training loss: 0.01433
Epoch: 0, step: 214, training loss: 0.01324
Epoch: 0, step: 215, training loss: 0.01368
Epoch: 0, step: 216, training loss: 0.01276
Epoch: 0, step: 217, training loss: 0.00981
Epoch: 0, step: 218, training loss: 0.00980
Epoch: 0, step: 219, training loss: 0.00991
Epoch: 0, step: 220, training loss: 0.01032
Epoch: 0, step: 221, training loss: 0.01217
Epoch: 0, step: 222, training loss: 0.01155
Epoch: 0, step: 223, training loss: 0.00853
Epoch: 0, step: 224, training loss: 0.01237
Epoch: 0, step: 225, training loss: 0.00924
Epoch: 0, step: 226, training loss: 0.01265
Epoch: 0, step: 227, training loss: 0.00724
Epoch: 0, step: 228, training loss: 0.01167
Epoch: 0, step: 229, training loss: 0.00969
Epoch: 0, step: 230, training loss: 0.01128
Epoch: 0, step: 231, training loss: 0.01383
Epoch: 0, step: 232, training loss: 0.01078
Epoch: 0, step: 233, training loss: 0.01149
Epoch: 0, step: 234, training loss: 0.01191
Epoch: 0, step: 235, training loss: 0.01012
Epoch: 0, step: 236, training loss: 0.00864
Epoch: 0, step: 237, training loss: 0.00999
Epoch: 0, step: 238, training loss: 0.01088
Epoch: 0, average training loss: 0.02780
Epoch: 0, F1: 0.00000, average dev loss: 0.00998
Epoch: 1, step: 0, training loss: 0.00965
Epoch: 1, step: 1, training loss: 0.01005
Epoch: 1, step: 2, training loss: 0.00881
Epoch: 1, step: 3, training loss: 0.00869
Epoch: 1, step: 4, training loss: 0.01494
Epoch: 1, step: 5, training loss: 0.00851
Epoch: 1, step: 6, training loss: 0.01066
Epoch: 1, step: 7, training loss: 0.01136
Epoch: 1, step: 8, training loss: 0.00944
Epoch: 1, step: 9, training loss: 0.01183
Epoch: 1, step: 10, training loss: 0.01024
Epoch: 1, step: 11, training loss: 0.00820
Epoch: 1, step: 12, training loss: 0.00991
Epoch: 1, step: 13, training loss: 0.00876
Epoch: 1, step: 14, training loss: 0.01011
Epoch: 1, step: 15, training loss: 0.01327
Epoch: 1, step: 16, training loss: 0.01099
Epoch: 1, step: 17, training loss: 0.00980
Epoch: 1, step: 18, training loss: 0.01023
Epoch: 1, step: 19, training loss: 0.00795
Epoch: 1, step: 20, training loss: 0.00938
Epoch: 1, step: 21, training loss: 0.01018
Epoch: 1, step: 22, training loss: 0.01247
Epoch: 1, step: 23, training loss: 0.00979
Epoch: 1, step: 24, training loss: 0.01631
Epoch: 1, step: 25, training loss: 0.01246
Epoch: 1, step: 26, training loss: 0.00736
Epoch: 1, step: 27, training loss: 0.00930
Epoch: 1, step: 28, training loss: 0.00910
Epoch: 1, step: 29, training loss: 0.01036
Epoch: 1, step: 30, training loss: 0.00835
Epoch: 1, step: 31, training loss: 0.01128
Epoch: 1, step: 32, training loss: 0.00858
Epoch: 1, step: 33, training loss: 0.00882
Epoch: 1, step: 34, training loss: 0.01219
Epoch: 1, step: 35, training loss: 0.00901
Epoch: 1, step: 36, training loss: 0.01178
Epoch: 1, step: 37, training loss: 0.00951
Epoch: 1, step: 38, training loss: 0.01195
Epoch: 1, step: 39, training loss: 0.00875
Epoch: 1, step: 40, training loss: 0.01143
Epoch: 1, step: 41, training loss: 0.00793
Epoch: 1, step: 42, training loss: 0.00903
Epoch: 1, step: 43, training loss: 0.00893
Epoch: 1, step: 44, training loss: 0.00619
Epoch: 1, step: 45, training loss: 0.00721
Epoch: 1, step: 46, training loss: 0.01296
Epoch: 1, step: 47, training loss: 0.00958
Epoch: 1, step: 48, training loss: 0.00884
Epoch: 1, step: 49, training loss: 0.01056
Epoch: 1, step: 50, training loss: 0.00911
Epoch: 1, step: 51, training loss: 0.01062
Epoch: 1, step: 52, training loss: 0.01100
Epoch: 1, step: 53, training loss: 0.00922
Epoch: 1, step: 54, training loss: 0.00565
Epoch: 1, step: 55, training loss: 0.01354
Epoch: 1, step: 56, training loss: 0.01214
Epoch: 1, step: 57, training loss: 0.00764
Epoch: 1, step: 58, training loss: 0.00956
Epoch: 1, step: 59, training loss: 0.00838
Epoch: 1, step: 60, training loss: 0.00814
Epoch: 1, step: 61, training loss: 0.00785
Epoch: 1, step: 62, training loss: 0.00731
Epoch: 1, step: 63, training loss: 0.01032
Epoch: 1, step: 64, training loss: 0.00848
Epoch: 1, step: 65, training loss: 0.00820
Epoch: 1, step: 66, training loss: 0.00935
Epoch: 1, step: 67, training loss: 0.00855
Epoch: 1, step: 68, training loss: 0.00952
Epoch: 1, step: 69, training loss: 0.00949
Epoch: 1, step: 70, training loss: 0.00781
Epoch: 1, step: 71, training loss: 0.00838
Epoch: 1, step: 72, training loss: 0.00788
Epoch: 1, step: 73, training loss: 0.00811
Epoch: 1, step: 74, training loss: 0.00809
Epoch: 1, step: 75, training loss: 0.00860
Epoch: 1, step: 76, training loss: 0.00802
Epoch: 1, step: 77, training loss: 0.01062
Epoch: 1, step: 78, training loss: 0.00700
Epoch: 1, step: 79, training loss: 0.00905
Epoch: 1, step: 80, training loss: 0.00888
Epoch: 1, step: 81, training loss: 0.00725
Epoch: 1, step: 82, training loss: 0.00799
Epoch: 1, step: 83, training loss: 0.00628
Epoch: 1, step: 84, training loss: 0.00816
Epoch: 1, step: 85, training loss: 0.00847
Epoch: 1, step: 86, training loss: 0.00513
Epoch: 1, step: 87, training loss: 0.00748
Epoch: 1, step: 88, training loss: 0.00857
Epoch: 1, step: 89, training loss: 0.00763
Epoch: 1, step: 90, training loss: 0.00919
Epoch: 1, step: 91, training loss: 0.00689
Epoch: 1, step: 92, training loss: 0.01194
Epoch: 1, step: 93, training loss: 0.01021
Epoch: 1, step: 94, training loss: 0.00604
Epoch: 1, step: 95, training loss: 0.00678
Epoch: 1, step: 96, training loss: 0.00963
Epoch: 1, step: 97, training loss: 0.00674
Epoch: 1, step: 98, training loss: 0.00805
Epoch: 1, step: 99, training loss: 0.00536
Epoch: 1, step: 100, training loss: 0.01052
Epoch: 1, step: 101, training loss: 0.00708
Epoch: 1, step: 102, training loss: 0.00800
Epoch: 1, step: 103, training loss: 0.00724
Epoch: 1, step: 104, training loss: 0.00619
Epoch: 1, step: 105, training loss: 0.00803
Epoch: 1, step: 106, training loss: 0.00781
Epoch: 1, step: 107, training loss: 0.00827
Epoch: 1, step: 108, training loss: 0.00900
Epoch: 1, step: 109, training loss: 0.00531
Epoch: 1, step: 110, training loss: 0.00853
Epoch: 1, step: 111, training loss: 0.00744
Epoch: 1, step: 112, training loss: 0.00750
Epoch: 1, step: 113, training loss: 0.00697
Epoch: 1, step: 114, training loss: 0.00529
Epoch: 1, step: 115, training loss: 0.00702
Epoch: 1, step: 116, training loss: 0.00574
Epoch: 1, step: 117, training loss: 0.00895
Epoch: 1, step: 118, training loss: 0.00829
Epoch: 1, step: 119, training loss: 0.00467
Epoch: 1, step: 120, training loss: 0.00689
Epoch: 1, step: 121, training loss: 0.00650
Epoch: 1, step: 122, training loss: 0.00854
Epoch: 1, step: 123, training loss: 0.00818
Epoch: 1, step: 124, training loss: 0.00806
Epoch: 1, step: 125, training loss: 0.00617
Epoch: 1, step: 126, training loss: 0.00885
Epoch: 1, step: 127, training loss: 0.00694
Epoch: 1, step: 128, training loss: 0.00576
Epoch: 1, step: 129, training loss: 0.00937
Epoch: 1, step: 130, training loss: 0.00871
Epoch: 1, step: 131, training loss: 0.00623
Epoch: 1, step: 132, training loss: 0.00511
Epoch: 1, step: 133, training loss: 0.00691
Epoch: 1, step: 134, training loss: 0.00688
Epoch: 1, step: 135, training loss: 0.00873
Epoch: 1, step: 136, training loss: 0.01005
Epoch: 1, step: 137, training loss: 0.00465
Epoch: 1, step: 138, training loss: 0.00623
Epoch: 1, step: 139, training loss: 0.00527
Epoch: 1, step: 140, training loss: 0.00522
Epoch: 1, step: 141, training loss: 0.00476
Epoch: 1, step: 142, training loss: 0.00742
Epoch: 1, step: 143, training loss: 0.00742
Epoch: 1, step: 144, training loss: 0.00693
Epoch: 1, step: 145, training loss: 0.01068
Epoch: 1, step: 146, training loss: 0.00570
Epoch: 1, step: 147, training loss: 0.00566
Epoch: 1, step: 148, training loss: 0.00730
Epoch: 1, step: 149, training loss: 0.00658
Epoch: 1, step: 150, training loss: 0.00652
Epoch: 1, step: 151, training loss: 0.00510
Epoch: 1, step: 152, training loss: 0.00629
Epoch: 1, step: 153, training loss: 0.00723
Epoch: 1, step: 154, training loss: 0.00556
Epoch: 1, step: 155, training loss: 0.00823
Epoch: 1, step: 156, training loss: 0.00549
Epoch: 1, step: 157, training loss: 0.00806
Epoch: 1, step: 158, training loss: 0.00731
Epoch: 1, step: 159, training loss: 0.00517
Epoch: 1, step: 160, training loss: 0.00756
Epoch: 1, step: 161, training loss: 0.00580
Epoch: 1, step: 162, training loss: 0.00668
Epoch: 1, step: 163, training loss: 0.00808
Epoch: 1, step: 164, training loss: 0.00485
Epoch: 1, step: 165, training loss: 0.00483
Epoch: 1, step: 166, training loss: 0.00761
Epoch: 1, step: 167, training loss: 0.00545
Epoch: 1, step: 168, training loss: 0.00710
Epoch: 1, step: 169, training loss: 0.00542
Epoch: 1, step: 170, training loss: 0.00535
Epoch: 1, step: 171, training loss: 0.00416
Epoch: 1, step: 172, training loss: 0.00690
Epoch: 1, step: 173, training loss: 0.00633
Epoch: 1, step: 174, training loss: 0.00466
Epoch: 1, step: 175, training loss: 0.00701
Epoch: 1, step: 176, training loss: 0.00491
Epoch: 1, step: 177, training loss: 0.00441
Epoch: 1, step: 178, training loss: 0.00475
Epoch: 1, step: 179, training loss: 0.00620
Epoch: 1, step: 180, training loss: 0.00426
Epoch: 1, step: 181, training loss: 0.00385
Epoch: 1, step: 182, training loss: 0.00743
Epoch: 1, step: 183, training loss: 0.00804
Epoch: 1, step: 184, training loss: 0.00529
Epoch: 1, step: 185, training loss: 0.00522
Epoch: 1, step: 186, training loss: 0.00467
Epoch: 1, step: 187, training loss: 0.00577
Epoch: 1, step: 188, training loss: 0.00499
Epoch: 1, step: 189, training loss: 0.00862
Epoch: 1, step: 190, training loss: 0.00583
Epoch: 1, step: 191, training loss: 0.00534
Epoch: 1, step: 192, training loss: 0.00606
Epoch: 1, step: 193, training loss: 0.00546
Epoch: 1, step: 194, training loss: 0.00541
Epoch: 1, step: 195, training loss: 0.00432
Epoch: 1, step: 196, training loss: 0.00531
Epoch: 1, step: 197, training loss: 0.00662
Epoch: 1, step: 198, training loss: 0.00566
Epoch: 1, step: 199, training loss: 0.00398
Epoch: 1, step: 200, training loss: 0.00518
Epoch: 1, step: 201, training loss: 0.00437
Epoch: 1, step: 202, training loss: 0.00533
Epoch: 1, step: 203, training loss: 0.00386
Epoch: 1, step: 204, training loss: 0.00519
Epoch: 1, step: 205, training loss: 0.00855
Epoch: 1, step: 206, training loss: 0.00693
Epoch: 1, step: 207, training loss: 0.00507
Epoch: 1, step: 208, training loss: 0.00567
Epoch: 1, step: 209, training loss: 0.00480
Epoch: 1, step: 210, training loss: 0.00514
Epoch: 1, step: 211, training loss: 0.00439
Epoch: 1, step: 212, training loss: 0.00478
Epoch: 1, step: 213, training loss: 0.00646
Epoch: 1, step: 214, training loss: 0.00404
Epoch: 1, step: 215, training loss: 0.00481
Epoch: 1, step: 216, training loss: 0.00499
Epoch: 1, step: 217, training loss: 0.00528
Epoch: 1, step: 218, training loss: 0.00697
Epoch: 1, step: 219, training loss: 0.00416
Epoch: 1, step: 220, training loss: 0.00652
Epoch: 1, step: 221, training loss: 0.00674
Epoch: 1, step: 222, training loss: 0.00769
Epoch: 1, step: 223, training loss: 0.00468
Epoch: 1, step: 224, training loss: 0.00407
Epoch: 1, step: 225, training loss: 0.00784
Epoch: 1, step: 226, training loss: 0.00505
Epoch: 1, step: 227, training loss: 0.00541
Epoch: 1, step: 228, training loss: 0.00534
Epoch: 1, step: 229, training loss: 0.00372
Epoch: 1, step: 230, training loss: 0.00702
Epoch: 1, step: 231, training loss: 0.00404
Epoch: 1, step: 232, training loss: 0.00474
Epoch: 1, step: 233, training loss: 0.00577
Epoch: 1, step: 234, training loss: 0.00507
Epoch: 1, step: 235, training loss: 0.00503
Epoch: 1, step: 236, training loss: 0.00535
Epoch: 1, step: 237, training loss: 0.00728
Epoch: 1, step: 238, training loss: 0.00833
Epoch: 1, average training loss: 0.00752
Epoch: 1, F1: 61.68871, average dev loss: 0.00470
Epoch: 2, step: 0, training loss: 0.00564
Epoch: 2, step: 1, training loss: 0.00465
Epoch: 2, step: 2, training loss: 0.00549
Epoch: 2, step: 3, training loss: 0.00372
Epoch: 2, step: 4, training loss: 0.00350
Epoch: 2, step: 5, training loss: 0.00369
Epoch: 2, step: 6, training loss: 0.00468
Epoch: 2, step: 7, training loss: 0.00340
Epoch: 2, step: 8, training loss: 0.00537
Epoch: 2, step: 9, training loss: 0.00349
Epoch: 2, step: 10, training loss: 0.00387
Epoch: 2, step: 11, training loss: 0.00521
Epoch: 2, step: 12, training loss: 0.00345
Epoch: 2, step: 13, training loss: 0.00567
Epoch: 2, step: 14, training loss: 0.00595
Epoch: 2, step: 15, training loss: 0.00386
Epoch: 2, step: 16, training loss: 0.00378
Epoch: 2, step: 17, training loss: 0.00370
Epoch: 2, step: 18, training loss: 0.00476
Epoch: 2, step: 19, training loss: 0.00625
Epoch: 2, step: 20, training loss: 0.00399
Epoch: 2, step: 21, training loss: 0.00504
Epoch: 2, step: 22, training loss: 0.00465
Epoch: 2, step: 23, training loss: 0.00352
Epoch: 2, step: 24, training loss: 0.00602
Epoch: 2, step: 25, training loss: 0.00422
Epoch: 2, step: 26, training loss: 0.00286
Epoch: 2, step: 27, training loss: 0.00480
Epoch: 2, step: 28, training loss: 0.00536
Epoch: 2, step: 29, training loss: 0.00358
Epoch: 2, step: 30, training loss: 0.00357
Epoch: 2, step: 31, training loss: 0.00435
Epoch: 2, step: 32, training loss: 0.00429
Epoch: 2, step: 33, training loss: 0.00374
Epoch: 2, step: 34, training loss: 0.00595
Epoch: 2, step: 35, training loss: 0.00368
Epoch: 2, step: 36, training loss: 0.00396
Epoch: 2, step: 37, training loss: 0.00545
Epoch: 2, step: 38, training loss: 0.00415
Epoch: 2, step: 39, training loss: 0.00405
Epoch: 2, step: 40, training loss: 0.00309
Epoch: 2, step: 41, training loss: 0.00495
Epoch: 2, step: 42, training loss: 0.00387
Epoch: 2, step: 43, training loss: 0.00416
Epoch: 2, step: 44, training loss: 0.00428
Epoch: 2, step: 45, training loss: 0.00434
Epoch: 2, step: 46, training loss: 0.00373
Epoch: 2, step: 47, training loss: 0.00639
Epoch: 2, step: 48, training loss: 0.00372
Epoch: 2, step: 49, training loss: 0.00485
Epoch: 2, step: 50, training loss: 0.00413
Epoch: 2, step: 51, training loss: 0.00548
Epoch: 2, step: 52, training loss: 0.00432
Epoch: 2, step: 53, training loss: 0.00513
Epoch: 2, step: 54, training loss: 0.00325
Epoch: 2, step: 55, training loss: 0.00327
Epoch: 2, step: 56, training loss: 0.00382
Epoch: 2, step: 57, training loss: 0.00486
Epoch: 2, step: 58, training loss: 0.00483
Epoch: 2, step: 59, training loss: 0.00454
Epoch: 2, step: 60, training loss: 0.00504
Epoch: 2, step: 61, training loss: 0.00435
Epoch: 2, step: 62, training loss: 0.00475
Epoch: 2, step: 63, training loss: 0.00290
Epoch: 2, step: 64, training loss: 0.00579
Epoch: 2, step: 65, training loss: 0.00347
Epoch: 2, step: 66, training loss: 0.00436
Epoch: 2, step: 67, training loss: 0.00407
Epoch: 2, step: 68, training loss: 0.00422
Epoch: 2, step: 69, training loss: 0.00331
Epoch: 2, step: 70, training loss: 0.00218
Epoch: 2, step: 71, training loss: 0.00293
Epoch: 2, step: 72, training loss: 0.00352
Epoch: 2, step: 73, training loss: 0.00330
Epoch: 2, step: 74, training loss: 0.00405
Epoch: 2, step: 75, training loss: 0.00481
Epoch: 2, step: 76, training loss: 0.00315
Epoch: 2, step: 77, training loss: 0.00469
Epoch: 2, step: 78, training loss: 0.00497
Epoch: 2, step: 79, training loss: 0.00405
Epoch: 2, step: 80, training loss: 0.00334
Epoch: 2, step: 81, training loss: 0.00645
Epoch: 2, step: 82, training loss: 0.00495
Epoch: 2, step: 83, training loss: 0.00254
Epoch: 2, step: 84, training loss: 0.00330
Epoch: 2, step: 85, training loss: 0.00447
Epoch: 2, step: 86, training loss: 0.00343
Epoch: 2, step: 87, training loss: 0.00347
Epoch: 2, step: 88, training loss: 0.00353
Epoch: 2, step: 89, training loss: 0.00268
Epoch: 2, step: 90, training loss: 0.00408
Epoch: 2, step: 91, training loss: 0.00362
Epoch: 2, step: 92, training loss: 0.00453
Epoch: 2, step: 93, training loss: 0.00385
Epoch: 2, step: 94, training loss: 0.00351
Epoch: 2, step: 95, training loss: 0.00467
Epoch: 2, step: 96, training loss: 0.00485
Epoch: 2, step: 97, training loss: 0.00367
Epoch: 2, step: 98, training loss: 0.00332
Epoch: 2, step: 99, training loss: 0.00492
Epoch: 2, step: 100, training loss: 0.00357
Epoch: 2, step: 101, training loss: 0.00331
Epoch: 2, step: 102, training loss: 0.00423
Epoch: 2, step: 103, training loss: 0.00366
Epoch: 2, step: 104, training loss: 0.00413
Epoch: 2, step: 105, training loss: 0.00430
Epoch: 2, step: 106, training loss: 0.00421
Epoch: 2, step: 107, training loss: 0.00345
Epoch: 2, step: 108, training loss: 0.00331
Epoch: 2, step: 109, training loss: 0.00416
Epoch: 2, step: 110, training loss: 0.00295
Epoch: 2, step: 111, training loss: 0.00531
Epoch: 2, step: 112, training loss: 0.00359
Epoch: 2, step: 113, training loss: 0.00386
Epoch: 2, step: 114, training loss: 0.00338
Epoch: 2, step: 115, training loss: 0.00331
Epoch: 2, step: 116, training loss: 0.00403
Epoch: 2, step: 117, training loss: 0.00321
Epoch: 2, step: 118, training loss: 0.00388
Epoch: 2, step: 119, training loss: 0.00415
Epoch: 2, step: 120, training loss: 0.00345
Epoch: 2, step: 121, training loss: 0.00411
Epoch: 2, step: 122, training loss: 0.00283
Epoch: 2, step: 123, training loss: 0.00408
Epoch: 2, step: 124, training loss: 0.00345
Epoch: 2, step: 125, training loss: 0.00435
Epoch: 2, step: 126, training loss: 0.00307
Epoch: 2, step: 127, training loss: 0.00368
Epoch: 2, step: 128, training loss: 0.00429
Epoch: 2, step: 129, training loss: 0.00287
Epoch: 2, step: 130, training loss: 0.00327
Epoch: 2, step: 131, training loss: 0.00394
Epoch: 2, step: 132, training loss: 0.00382
Epoch: 2, step: 133, training loss: 0.00485
Epoch: 2, step: 134, training loss: 0.00256
Epoch: 2, step: 135, training loss: 0.00410
Epoch: 2, step: 136, training loss: 0.00416
Epoch: 2, step: 137, training loss: 0.00379
Epoch: 2, step: 138, training loss: 0.00334
Epoch: 2, step: 139, training loss: 0.00216
Epoch: 2, step: 140, training loss: 0.00351
Epoch: 2, step: 141, training loss: 0.00360
Epoch: 2, step: 142, training loss: 0.00414
Epoch: 2, step: 143, training loss: 0.00385
Epoch: 2, step: 144, training loss: 0.00324
Epoch: 2, step: 145, training loss: 0.00400
Epoch: 2, step: 146, training loss: 0.00432
Epoch: 2, step: 147, training loss: 0.00346
Epoch: 2, step: 148, training loss: 0.00429
Epoch: 2, step: 149, training loss: 0.00515
Epoch: 2, step: 150, training loss: 0.00203
Epoch: 2, step: 151, training loss: 0.00209
Epoch: 2, step: 152, training loss: 0.00332
Epoch: 2, step: 153, training loss: 0.00285
Epoch: 2, step: 154, training loss: 0.00305
Epoch: 2, step: 155, training loss: 0.00275
Epoch: 2, step: 156, training loss: 0.00328
Epoch: 2, step: 157, training loss: 0.00599
Epoch: 2, step: 158, training loss: 0.00264
Epoch: 2, step: 159, training loss: 0.00256
Epoch: 2, step: 160, training loss: 0.00237
Epoch: 2, step: 161, training loss: 0.00432
Epoch: 2, step: 162, training loss: 0.00296
Epoch: 2, step: 163, training loss: 0.00278
Epoch: 2, step: 164, training loss: 0.00313
Epoch: 2, step: 165, training loss: 0.00312
Epoch: 2, step: 166, training loss: 0.00333
Epoch: 2, step: 167, training loss: 0.00316
Epoch: 2, step: 168, training loss: 0.00224
Epoch: 2, step: 169, training loss: 0.00288
Epoch: 2, step: 170, training loss: 0.00400
Epoch: 2, step: 171, training loss: 0.00318
Epoch: 2, step: 172, training loss: 0.00438
Epoch: 2, step: 173, training loss: 0.00382
Epoch: 2, step: 174, training loss: 0.00384
Epoch: 2, step: 175, training loss: 0.00332
Epoch: 2, step: 176, training loss: 0.00355
Epoch: 2, step: 177, training loss: 0.00444
Epoch: 2, step: 178, training loss: 0.00481
Epoch: 2, step: 179, training loss: 0.00472
Epoch: 2, step: 180, training loss: 0.00254
Epoch: 2, step: 181, training loss: 0.00243
Epoch: 2, step: 182, training loss: 0.00552
Epoch: 2, step: 183, training loss: 0.00285
Epoch: 2, step: 184, training loss: 0.00395
Epoch: 2, step: 185, training loss: 0.00385
Epoch: 2, step: 186, training loss: 0.00384
Epoch: 2, step: 187, training loss: 0.00446
Epoch: 2, step: 188, training loss: 0.00367
Epoch: 2, step: 189, training loss: 0.00332
Epoch: 2, step: 190, training loss: 0.00597
Epoch: 2, step: 191, training loss: 0.00372
Epoch: 2, step: 192, training loss: 0.00457
Epoch: 2, step: 193, training loss: 0.00309
Epoch: 2, step: 194, training loss: 0.00453
Epoch: 2, step: 195, training loss: 0.00291
Epoch: 2, step: 196, training loss: 0.00523
Epoch: 2, step: 197, training loss: 0.00319
Epoch: 2, step: 198, training loss: 0.00550
Epoch: 2, step: 199, training loss: 0.00359
Epoch: 2, step: 200, training loss: 0.00323
Epoch: 2, step: 201, training loss: 0.00477
Epoch: 2, step: 202, training loss: 0.00361
Epoch: 2, step: 203, training loss: 0.00338
Epoch: 2, step: 204, training loss: 0.00247
Epoch: 2, step: 205, training loss: 0.00404
Epoch: 2, step: 206, training loss: 0.00409
Epoch: 2, step: 207, training loss: 0.00361
Epoch: 2, step: 208, training loss: 0.00410
Epoch: 2, step: 209, training loss: 0.00347
Epoch: 2, step: 210, training loss: 0.00273
Epoch: 2, step: 211, training loss: 0.00336
Epoch: 2, step: 212, training loss: 0.00372
Epoch: 2, step: 213, training loss: 0.00377
Epoch: 2, step: 214, training loss: 0.00341
Epoch: 2, step: 215, training loss: 0.00198
Epoch: 2, step: 216, training loss: 0.00263
Epoch: 2, step: 217, training loss: 0.00292
Epoch: 2, step: 218, training loss: 0.00410
Epoch: 2, step: 219, training loss: 0.00358
Epoch: 2, step: 220, training loss: 0.00516
Epoch: 2, step: 221, training loss: 0.00236
Epoch: 2, step: 222, training loss: 0.00251
Epoch: 2, step: 223, training loss: 0.00437
Epoch: 2, step: 224, training loss: 0.00378
Epoch: 2, step: 225, training loss: 0.00217
Epoch: 2, step: 226, training loss: 0.00176
Epoch: 2, step: 227, training loss: 0.00248
Epoch: 2, step: 228, training loss: 0.00413
Epoch: 2, step: 229, training loss: 0.00327
Epoch: 2, step: 230, training loss: 0.00195
Epoch: 2, step: 231, training loss: 0.00249
Epoch: 2, step: 232, training loss: 0.00295
Epoch: 2, step: 233, training loss: 0.00342
Epoch: 2, step: 234, training loss: 0.00303
Epoch: 2, step: 235, training loss: 0.00398
Epoch: 2, step: 236, training loss: 0.00327
Epoch: 2, step: 237, training loss: 0.00421
Epoch: 2, step: 238, training loss: 0.00486
Epoch: 2, average training loss: 0.00386
Epoch: 2, F1: 76.66428, average dev loss: 0.00312
Epoch: 3, step: 0, training loss: 0.00282
Epoch: 3, step: 1, training loss: 0.00288
Epoch: 3, step: 2, training loss: 0.00275
Epoch: 3, step: 3, training loss: 0.00306
Epoch: 3, step: 4, training loss: 0.00304
Epoch: 3, step: 5, training loss: 0.00263
Epoch: 3, step: 6, training loss: 0.00221
Epoch: 3, step: 7, training loss: 0.00302
Epoch: 3, step: 8, training loss: 0.00243
Epoch: 3, step: 9, training loss: 0.00192
Epoch: 3, step: 10, training loss: 0.00332
Epoch: 3, step: 11, training loss: 0.00287
Epoch: 3, step: 12, training loss: 0.00439
Epoch: 3, step: 13, training loss: 0.00283
Epoch: 3, step: 14, training loss: 0.00469
Epoch: 3, step: 15, training loss: 0.00313
Epoch: 3, step: 16, training loss: 0.00230
Epoch: 3, step: 17, training loss: 0.00321
Epoch: 3, step: 18, training loss: 0.00214
Epoch: 3, step: 19, training loss: 0.00182
Epoch: 3, step: 20, training loss: 0.00185
Epoch: 3, step: 21, training loss: 0.00196
Epoch: 3, step: 22, training loss: 0.00249
Epoch: 3, step: 23, training loss: 0.00159
Epoch: 3, step: 24, training loss: 0.00316
Epoch: 3, step: 25, training loss: 0.00297
Epoch: 3, step: 26, training loss: 0.00264
Epoch: 3, step: 27, training loss: 0.00284
Epoch: 3, step: 28, training loss: 0.00254
Epoch: 3, step: 29, training loss: 0.00269
Epoch: 3, step: 30, training loss: 0.00365
Epoch: 3, step: 31, training loss: 0.00255
Epoch: 3, step: 32, training loss: 0.00237
Epoch: 3, step: 33, training loss: 0.00217
Epoch: 3, step: 34, training loss: 0.00251
Epoch: 3, step: 35, training loss: 0.00199
Epoch: 3, step: 36, training loss: 0.00273
Epoch: 3, step: 37, training loss: 0.00174
Epoch: 3, step: 38, training loss: 0.00309
Epoch: 3, step: 39, training loss: 0.00201
Epoch: 3, step: 40, training loss: 0.00221
Epoch: 3, step: 41, training loss: 0.00214
Epoch: 3, step: 42, training loss: 0.00189
Epoch: 3, step: 43, training loss: 0.00404
Epoch: 3, step: 44, training loss: 0.00246
Epoch: 3, step: 45, training loss: 0.00254
Epoch: 3, step: 46, training loss: 0.00178
Epoch: 3, step: 47, training loss: 0.00194
Epoch: 3, step: 48, training loss: 0.00245
Epoch: 3, step: 49, training loss: 0.00252
Epoch: 3, step: 50, training loss: 0.00449
Epoch: 3, step: 51, training loss: 0.00177
Epoch: 3, step: 52, training loss: 0.00344
Epoch: 3, step: 53, training loss: 0.00148
Epoch: 3, step: 54, training loss: 0.00271
Epoch: 3, step: 55, training loss: 0.00168
Epoch: 3, step: 56, training loss: 0.00192
Epoch: 3, step: 57, training loss: 0.00226
Epoch: 3, step: 58, training loss: 0.00333
Epoch: 3, step: 59, training loss: 0.00213
Epoch: 3, step: 60, training loss: 0.00345
Epoch: 3, step: 61, training loss: 0.00266
Epoch: 3, step: 62, training loss: 0.00258
Epoch: 3, step: 63, training loss: 0.00241
Epoch: 3, step: 64, training loss: 0.00135
Epoch: 3, step: 65, training loss: 0.00346
Epoch: 3, step: 66, training loss: 0.00310
Epoch: 3, step: 67, training loss: 0.00142
Epoch: 3, step: 68, training loss: 0.00269
Epoch: 3, step: 69, training loss: 0.00213
Epoch: 3, step: 70, training loss: 0.00158
Epoch: 3, step: 71, training loss: 0.00214
Epoch: 3, step: 72, training loss: 0.00277
Epoch: 3, step: 73, training loss: 0.00182
Epoch: 3, step: 74, training loss: 0.00262
Epoch: 3, step: 75, training loss: 0.00188
Epoch: 3, step: 76, training loss: 0.00186
Epoch: 3, step: 77, training loss: 0.00186
Epoch: 3, step: 78, training loss: 0.00273
Epoch: 3, step: 79, training loss: 0.00344
Epoch: 3, step: 80, training loss: 0.00196
Epoch: 3, step: 81, training loss: 0.00243
Epoch: 3, step: 82, training loss: 0.00303
Epoch: 3, step: 83, training loss: 0.00273
Epoch: 3, step: 84, training loss: 0.00381
Epoch: 3, step: 85, training loss: 0.00135
Epoch: 3, step: 86, training loss: 0.00260
Epoch: 3, step: 87, training loss: 0.00240
Epoch: 3, step: 88, training loss: 0.00303
Epoch: 3, step: 89, training loss: 0.00301
Epoch: 3, step: 90, training loss: 0.00184
Epoch: 3, step: 91, training loss: 0.00170
Epoch: 3, step: 92, training loss: 0.00254
Epoch: 3, step: 93, training loss: 0.00148
Epoch: 3, step: 94, training loss: 0.00314
Epoch: 3, step: 95, training loss: 0.00232
Epoch: 3, step: 96, training loss: 0.00152
Epoch: 3, step: 97, training loss: 0.00206
Epoch: 3, step: 98, training loss: 0.00193
Epoch: 3, step: 99, training loss: 0.00184
Epoch: 3, step: 100, training loss: 0.00261
Epoch: 3, step: 101, training loss: 0.00386
Epoch: 3, step: 102, training loss: 0.00254
Epoch: 3, step: 103, training loss: 0.00234
Epoch: 3, step: 104, training loss: 0.00130
Epoch: 3, step: 105, training loss: 0.00110
Epoch: 3, step: 106, training loss: 0.00199
Epoch: 3, step: 107, training loss: 0.00301
Epoch: 3, step: 108, training loss: 0.00280
Epoch: 3, step: 109, training loss: 0.00454
Epoch: 3, step: 110, training loss: 0.00219
Epoch: 3, step: 111, training loss: 0.00244
Epoch: 3, step: 112, training loss: 0.00282
Epoch: 3, step: 113, training loss: 0.00310
Epoch: 3, step: 114, training loss: 0.00132
Epoch: 3, step: 115, training loss: 0.00324
Epoch: 3, step: 116, training loss: 0.00312
Epoch: 3, step: 117, training loss: 0.00241
Epoch: 3, step: 118, training loss: 0.00378
Epoch: 3, step: 119, training loss: 0.00414
Epoch: 3, step: 120, training loss: 0.00295
Epoch: 3, step: 121, training loss: 0.00198
Epoch: 3, step: 122, training loss: 0.00356
Epoch: 3, step: 123, training loss: 0.00122
Epoch: 3, step: 124, training loss: 0.00453
Epoch: 3, step: 125, training loss: 0.00312
Epoch: 3, step: 126, training loss: 0.00189
Epoch: 3, step: 127, training loss: 0.00424
Epoch: 3, step: 128, training loss: 0.00282
Epoch: 3, step: 129, training loss: 0.00201
Epoch: 3, step: 130, training loss: 0.00290
Epoch: 3, step: 131, training loss: 0.00254
Epoch: 3, step: 132, training loss: 0.00235
Epoch: 3, step: 133, training loss: 0.00279
Epoch: 3, step: 134, training loss: 0.00228
Epoch: 3, step: 135, training loss: 0.00213
Epoch: 3, step: 136, training loss: 0.00169
Epoch: 3, step: 137, training loss: 0.00219
Epoch: 3, step: 138, training loss: 0.00262
Epoch: 3, step: 139, training loss: 0.00229
Epoch: 3, step: 140, training loss: 0.00206
Epoch: 3, step: 141, training loss: 0.00185
Epoch: 3, step: 142, training loss: 0.00110
Epoch: 3, step: 143, training loss: 0.00210
Epoch: 3, step: 144, training loss: 0.00511
Epoch: 3, step: 145, training loss: 0.00195
Epoch: 3, step: 146, training loss: 0.00339
Epoch: 3, step: 147, training loss: 0.00270
Epoch: 3, step: 148, training loss: 0.00250
Epoch: 3, step: 149, training loss: 0.00303
Epoch: 3, step: 150, training loss: 0.00307
Epoch: 3, step: 151, training loss: 0.00161
Epoch: 3, step: 152, training loss: 0.00267
Epoch: 3, step: 153, training loss: 0.00166
Epoch: 3, step: 154, training loss: 0.00244
Epoch: 3, step: 155, training loss: 0.00177
Epoch: 3, step: 156, training loss: 0.00178
Epoch: 3, step: 157, training loss: 0.00330
Epoch: 3, step: 158, training loss: 0.00221
Epoch: 3, step: 159, training loss: 0.00357
Epoch: 3, step: 160, training loss: 0.00162
Epoch: 3, step: 161, training loss: 0.00158
Epoch: 3, step: 162, training loss: 0.00302
Epoch: 3, step: 163, training loss: 0.00235
Epoch: 3, step: 164, training loss: 0.00242
Epoch: 3, step: 165, training loss: 0.00290
Epoch: 3, step: 166, training loss: 0.00279
Epoch: 3, step: 167, training loss: 0.00223
Epoch: 3, step: 168, training loss: 0.00109
Epoch: 3, step: 169, training loss: 0.00252
Epoch: 3, step: 170, training loss: 0.00216
Epoch: 3, step: 171, training loss: 0.00346
Epoch: 3, step: 172, training loss: 0.00332
Epoch: 3, step: 173, training loss: 0.00208
Epoch: 3, step: 174, training loss: 0.00192
Epoch: 3, step: 175, training loss: 0.00195
Epoch: 3, step: 176, training loss: 0.00312
Epoch: 3, step: 177, training loss: 0.00132
Epoch: 3, step: 178, training loss: 0.00306
Epoch: 3, step: 179, training loss: 0.00250
Epoch: 3, step: 180, training loss: 0.00224
Epoch: 3, step: 181, training loss: 0.00244
Epoch: 3, step: 182, training loss: 0.00178
Epoch: 3, step: 183, training loss: 0.00262
Epoch: 3, step: 184, training loss: 0.00207
Epoch: 3, step: 185, training loss: 0.00179
Epoch: 3, step: 186, training loss: 0.00283
Epoch: 3, step: 187, training loss: 0.00151
Epoch: 3, step: 188, training loss: 0.00326
Epoch: 3, step: 189, training loss: 0.00180
Epoch: 3, step: 190, training loss: 0.00319
Epoch: 3, step: 191, training loss: 0.00209
Epoch: 3, step: 192, training loss: 0.00250
Epoch: 3, step: 193, training loss: 0.00228
Epoch: 3, step: 194, training loss: 0.00143
Epoch: 3, step: 195, training loss: 0.00380
Epoch: 3, step: 196, training loss: 0.00296
Epoch: 3, step: 197, training loss: 0.00266
Epoch: 3, step: 198, training loss: 0.00261
Epoch: 3, step: 199, training loss: 0.00301
Epoch: 3, step: 200, training loss: 0.00212
Epoch: 3, step: 201, training loss: 0.00410
Epoch: 3, step: 202, training loss: 0.00234
Epoch: 3, step: 203, training loss: 0.00283
Epoch: 3, step: 204, training loss: 0.00231
Epoch: 3, step: 205, training loss: 0.00242
Epoch: 3, step: 206, training loss: 0.00250
Epoch: 3, step: 207, training loss: 0.00343
Epoch: 3, step: 208, training loss: 0.00259
Epoch: 3, step: 209, training loss: 0.00123
Epoch: 3, step: 210, training loss: 0.00211
Epoch: 3, step: 211, training loss: 0.00404
Epoch: 3, step: 212, training loss: 0.00225
Epoch: 3, step: 213, training loss: 0.00221
Epoch: 3, step: 214, training loss: 0.00222
Epoch: 3, step: 215, training loss: 0.00224
Epoch: 3, step: 216, training loss: 0.00305
Epoch: 3, step: 217, training loss: 0.00145
Epoch: 3, step: 218, training loss: 0.00271
Epoch: 3, step: 219, training loss: 0.00203
Epoch: 3, step: 220, training loss: 0.00278
Epoch: 3, step: 221, training loss: 0.00271
Epoch: 3, step: 222, training loss: 0.00124
Epoch: 3, step: 223, training loss: 0.00132
Epoch: 3, step: 224, training loss: 0.00211
Epoch: 3, step: 225, training loss: 0.00192
Epoch: 3, step: 226, training loss: 0.00218
Epoch: 3, step: 227, training loss: 0.00347
Epoch: 3, step: 228, training loss: 0.00193
Epoch: 3, step: 229, training loss: 0.00131
Epoch: 3, step: 230, training loss: 0.00147
Epoch: 3, step: 231, training loss: 0.00209
Epoch: 3, step: 232, training loss: 0.00229
Epoch: 3, step: 233, training loss: 0.00235
Epoch: 3, step: 234, training loss: 0.00194
Epoch: 3, step: 235, training loss: 0.00261
Epoch: 3, step: 236, training loss: 0.00164
Epoch: 3, step: 237, training loss: 0.00198
Epoch: 3, step: 238, training loss: 0.00199
Epoch: 3, average training loss: 0.00248
Epoch: 3, F1: 79.93911, average dev loss: 0.00272
Epoch: 4, step: 0, training loss: 0.00188
Epoch: 4, step: 1, training loss: 0.00171
Epoch: 4, step: 2, training loss: 0.00140
Epoch: 4, step: 3, training loss: 0.00152
Epoch: 4, step: 4, training loss: 0.00224
Epoch: 4, step: 5, training loss: 0.00236
Epoch: 4, step: 6, training loss: 0.00317
Epoch: 4, step: 7, training loss: 0.00167
Epoch: 4, step: 8, training loss: 0.00140
Epoch: 4, step: 9, training loss: 0.00177
Epoch: 4, step: 10, training loss: 0.00160
Epoch: 4, step: 11, training loss: 0.00113
Epoch: 4, step: 12, training loss: 0.00143
Epoch: 4, step: 13, training loss: 0.00172
Epoch: 4, step: 14, training loss: 0.00222
Epoch: 4, step: 15, training loss: 0.00151
Epoch: 4, step: 16, training loss: 0.00190
Epoch: 4, step: 17, training loss: 0.00159
Epoch: 4, step: 18, training loss: 0.00180
Epoch: 4, step: 19, training loss: 0.00204
Epoch: 4, step: 20, training loss: 0.00166
Epoch: 4, step: 21, training loss: 0.00131
Epoch: 4, step: 22, training loss: 0.00151
Epoch: 4, step: 23, training loss: 0.00136
Epoch: 4, step: 24, training loss: 0.00163
Epoch: 4, step: 25, training loss: 0.00157
Epoch: 4, step: 26, training loss: 0.00165
Epoch: 4, step: 27, training loss: 0.00239
Epoch: 4, step: 28, training loss: 0.00153
Epoch: 4, step: 29, training loss: 0.00114
Epoch: 4, step: 30, training loss: 0.00172
Epoch: 4, step: 31, training loss: 0.00257
Epoch: 4, step: 32, training loss: 0.00161
Epoch: 4, step: 33, training loss: 0.00299
Epoch: 4, step: 34, training loss: 0.00189
Epoch: 4, step: 35, training loss: 0.00180
Epoch: 4, step: 36, training loss: 0.00149
Epoch: 4, step: 37, training loss: 0.00156
Epoch: 4, step: 38, training loss: 0.00131
Epoch: 4, step: 39, training loss: 0.00196
Epoch: 4, step: 40, training loss: 0.00220
Epoch: 4, step: 41, training loss: 0.00213
Epoch: 4, step: 42, training loss: 0.00278
Epoch: 4, step: 43, training loss: 0.00218
Epoch: 4, step: 44, training loss: 0.00177
Epoch: 4, step: 45, training loss: 0.00171
Epoch: 4, step: 46, training loss: 0.00109
Epoch: 4, step: 47, training loss: 0.00197
Epoch: 4, step: 48, training loss: 0.00150
Epoch: 4, step: 49, training loss: 0.00249
Epoch: 4, step: 50, training loss: 0.00103
Epoch: 4, step: 51, training loss: 0.00080
Epoch: 4, step: 52, training loss: 0.00233
Epoch: 4, step: 53, training loss: 0.00263
Epoch: 4, step: 54, training loss: 0.00235
Epoch: 4, step: 55, training loss: 0.00095
Epoch: 4, step: 56, training loss: 0.00108
Epoch: 4, step: 57, training loss: 0.00119
Epoch: 4, step: 58, training loss: 0.00208
Epoch: 4, step: 59, training loss: 0.00190
Epoch: 4, step: 60, training loss: 0.00123
Epoch: 4, step: 61, training loss: 0.00159
Epoch: 4, step: 62, training loss: 0.00166
Epoch: 4, step: 63, training loss: 0.00144
Epoch: 4, step: 64, training loss: 0.00154
Epoch: 4, step: 65, training loss: 0.00206
Epoch: 4, step: 66, training loss: 0.00138
Epoch: 4, step: 67, training loss: 0.00248
Epoch: 4, step: 68, training loss: 0.00184
Epoch: 4, step: 69, training loss: 0.00133
Epoch: 4, step: 70, training loss: 0.00150
Epoch: 4, step: 71, training loss: 0.00055
Epoch: 4, step: 72, training loss: 0.00185
Epoch: 4, step: 73, training loss: 0.00180
Epoch: 4, step: 74, training loss: 0.00119
Epoch: 4, step: 75, training loss: 0.00118
Epoch: 4, step: 76, training loss: 0.00328
Epoch: 4, step: 77, training loss: 0.00218
Epoch: 4, step: 78, training loss: 0.00242
Epoch: 4, step: 79, training loss: 0.00161
Epoch: 4, step: 80, training loss: 0.00142
Epoch: 4, step: 81, training loss: 0.00128
Epoch: 4, step: 82, training loss: 0.00266
Epoch: 4, step: 83, training loss: 0.00130
Epoch: 4, step: 84, training loss: 0.00157
Epoch: 4, step: 85, training loss: 0.00124
Epoch: 4, step: 86, training loss: 0.00164
Epoch: 4, step: 87, training loss: 0.00170
Epoch: 4, step: 88, training loss: 0.00191
Epoch: 4, step: 89, training loss: 0.00139
Epoch: 4, step: 90, training loss: 0.00098
Epoch: 4, step: 91, training loss: 0.00178
Epoch: 4, step: 92, training loss: 0.00216
Epoch: 4, step: 93, training loss: 0.00149
Epoch: 4, step: 94, training loss: 0.00118
Epoch: 4, step: 95, training loss: 0.00165
Epoch: 4, step: 96, training loss: 0.00106
Epoch: 4, step: 97, training loss: 0.00214
Epoch: 4, step: 98, training loss: 0.00195
Epoch: 4, step: 99, training loss: 0.00137
Epoch: 4, step: 100, training loss: 0.00080
Epoch: 4, step: 101, training loss: 0.00175
Epoch: 4, step: 102, training loss: 0.00191
Epoch: 4, step: 103, training loss: 0.00196
Epoch: 4, step: 104, training loss: 0.00097
Epoch: 4, step: 105, training loss: 0.00189
Epoch: 4, step: 106, training loss: 0.00117
Epoch: 4, step: 107, training loss: 0.00156
Epoch: 4, step: 108, training loss: 0.00241
Epoch: 4, step: 109, training loss: 0.00178
Epoch: 4, step: 110, training loss: 0.00168
Epoch: 4, step: 111, training loss: 0.00215
Epoch: 4, step: 112, training loss: 0.00094
Epoch: 4, step: 113, training loss: 0.00192
Epoch: 4, step: 114, training loss: 0.00197
Epoch: 4, step: 115, training loss: 0.00132
Epoch: 4, step: 116, training loss: 0.00163
Epoch: 4, step: 117, training loss: 0.00334
Epoch: 4, step: 118, training loss: 0.00156
Epoch: 4, step: 119, training loss: 0.00156
Epoch: 4, step: 120, training loss: 0.00193
Epoch: 4, step: 121, training loss: 0.00207
Epoch: 4, step: 122, training loss: 0.00220
Epoch: 4, step: 123, training loss: 0.00192
Epoch: 4, step: 124, training loss: 0.00188
Epoch: 4, step: 125, training loss: 0.00108
Epoch: 4, step: 126, training loss: 0.00134
Epoch: 4, step: 127, training loss: 0.00172
Epoch: 4, step: 128, training loss: 0.00138
Epoch: 4, step: 129, training loss: 0.00124
Epoch: 4, step: 130, training loss: 0.00229
Epoch: 4, step: 131, training loss: 0.00195
Epoch: 4, step: 132, training loss: 0.00159
Epoch: 4, step: 133, training loss: 0.00143
Epoch: 4, step: 134, training loss: 0.00120
Epoch: 4, step: 135, training loss: 0.00136
Epoch: 4, step: 136, training loss: 0.00169
Epoch: 4, step: 137, training loss: 0.00116
Epoch: 4, step: 138, training loss: 0.00227
Epoch: 4, step: 139, training loss: 0.00156
Epoch: 4, step: 140, training loss: 0.00101
Epoch: 4, step: 141, training loss: 0.00143
Epoch: 4, step: 142, training loss: 0.00163
Epoch: 4, step: 143, training loss: 0.00107
Epoch: 4, step: 144, training loss: 0.00178
Epoch: 4, step: 145, training loss: 0.00113
Epoch: 4, step: 146, training loss: 0.00118
Epoch: 4, step: 147, training loss: 0.00173
Epoch: 4, step: 148, training loss: 0.00115
Epoch: 4, step: 149, training loss: 0.00137
Epoch: 4, step: 150, training loss: 0.00213
Epoch: 4, step: 151, training loss: 0.00204
Epoch: 4, step: 152, training loss: 0.00104
Epoch: 4, step: 153, training loss: 0.00137
Epoch: 4, step: 154, training loss: 0.00124
Epoch: 4, step: 155, training loss: 0.00142
Epoch: 4, step: 156, training loss: 0.00166
Epoch: 4, step: 157, training loss: 0.00175
Epoch: 4, step: 158, training loss: 0.00247
Epoch: 4, step: 159, training loss: 0.00134
Epoch: 4, step: 160, training loss: 0.00171
Epoch: 4, step: 161, training loss: 0.00145
Epoch: 4, step: 162, training loss: 0.00139
Epoch: 4, step: 163, training loss: 0.00208
Epoch: 4, step: 164, training loss: 0.00147
Epoch: 4, step: 165, training loss: 0.00186
Epoch: 4, step: 166, training loss: 0.00201
Epoch: 4, step: 167, training loss: 0.00194
Epoch: 4, step: 168, training loss: 0.00206
Epoch: 4, step: 169, training loss: 0.00259
Epoch: 4, step: 170, training loss: 0.00132
Epoch: 4, step: 171, training loss: 0.00173
Epoch: 4, step: 172, training loss: 0.00165
Epoch: 4, step: 173, training loss: 0.00242
Epoch: 4, step: 174, training loss: 0.00134
Epoch: 4, step: 175, training loss: 0.00150
Epoch: 4, step: 176, training loss: 0.00131
Epoch: 4, step: 177, training loss: 0.00123
Epoch: 4, step: 178, training loss: 0.00167
Epoch: 4, step: 179, training loss: 0.00205
Epoch: 4, step: 180, training loss: 0.00234
Epoch: 4, step: 181, training loss: 0.00169
Epoch: 4, step: 182, training loss: 0.00168
Epoch: 4, step: 183, training loss: 0.00214
Epoch: 4, step: 184, training loss: 0.00116
Epoch: 4, step: 185, training loss: 0.00126
Epoch: 4, step: 186, training loss: 0.00235
Epoch: 4, step: 187, training loss: 0.00215
Epoch: 4, step: 188, training loss: 0.00094
Epoch: 4, step: 189, training loss: 0.00086
Epoch: 4, step: 190, training loss: 0.00113
Epoch: 4, step: 191, training loss: 0.00158
Epoch: 4, step: 192, training loss: 0.00218
Epoch: 4, step: 193, training loss: 0.00185
Epoch: 4, step: 194, training loss: 0.00195
Epoch: 4, step: 195, training loss: 0.00173
Epoch: 4, step: 196, training loss: 0.00136
Epoch: 4, step: 197, training loss: 0.00121
Epoch: 4, step: 198, training loss: 0.00198
Epoch: 4, step: 199, training loss: 0.00161
Epoch: 4, step: 200, training loss: 0.00142
Epoch: 4, step: 201, training loss: 0.00271
Epoch: 4, step: 202, training loss: 0.00101
Epoch: 4, step: 203, training loss: 0.00202
Epoch: 4, step: 204, training loss: 0.00272
Epoch: 4, step: 205, training loss: 0.00129
Epoch: 4, step: 206, training loss: 0.00106
Epoch: 4, step: 207, training loss: 0.00100
Epoch: 4, step: 208, training loss: 0.00232
Epoch: 4, step: 209, training loss: 0.00108
Epoch: 4, step: 210, training loss: 0.00084
Epoch: 4, step: 211, training loss: 0.00142
Epoch: 4, step: 212, training loss: 0.00118
Epoch: 4, step: 213, training loss: 0.00134
Epoch: 4, step: 214, training loss: 0.00172
Epoch: 4, step: 215, training loss: 0.00138
Epoch: 4, step: 216, training loss: 0.00167
Epoch: 4, step: 217, training loss: 0.00185
Epoch: 4, step: 218, training loss: 0.00131
Epoch: 4, step: 219, training loss: 0.00165
Epoch: 4, step: 220, training loss: 0.00096
Epoch: 4, step: 221, training loss: 0.00155
Epoch: 4, step: 222, training loss: 0.00147
Epoch: 4, step: 223, training loss: 0.00182
Epoch: 4, step: 224, training loss: 0.00112
Epoch: 4, step: 225, training loss: 0.00212
Epoch: 4, step: 226, training loss: 0.00234
Epoch: 4, step: 227, training loss: 0.00119
Epoch: 4, step: 228, training loss: 0.00094
Epoch: 4, step: 229, training loss: 0.00115
Epoch: 4, step: 230, training loss: 0.00133
Epoch: 4, step: 231, training loss: 0.00116
Epoch: 4, step: 232, training loss: 0.00130
Epoch: 4, step: 233, training loss: 0.00090
Epoch: 4, step: 234, training loss: 0.00141
Epoch: 4, step: 235, training loss: 0.00085
Epoch: 4, step: 236, training loss: 0.00186
Epoch: 4, step: 237, training loss: 0.00160
Epoch: 4, step: 238, training loss: 0.00128
Epoch: 4, average training loss: 0.00166
Epoch: 4, F1: 82.67359, average dev loss: 0.00270
Epoch: 5, step: 0, training loss: 0.00117
Epoch: 5, step: 1, training loss: 0.00101
Epoch: 5, step: 2, training loss: 0.00094
Epoch: 5, step: 3, training loss: 0.00213
Epoch: 5, step: 4, training loss: 0.00163
Epoch: 5, step: 5, training loss: 0.00157
Epoch: 5, step: 6, training loss: 0.00114
Epoch: 5, step: 7, training loss: 0.00135
Epoch: 5, step: 8, training loss: 0.00075
Epoch: 5, step: 9, training loss: 0.00100
Epoch: 5, step: 10, training loss: 0.00066
Epoch: 5, step: 11, training loss: 0.00064
Epoch: 5, step: 12, training loss: 0.00090
Epoch: 5, step: 13, training loss: 0.00066
Epoch: 5, step: 14, training loss: 0.00110
Epoch: 5, step: 15, training loss: 0.00087
Epoch: 5, step: 16, training loss: 0.00106
Epoch: 5, step: 17, training loss: 0.00121
Epoch: 5, step: 18, training loss: 0.00196
Epoch: 5, step: 19, training loss: 0.00130
Epoch: 5, step: 20, training loss: 0.00140
Epoch: 5, step: 21, training loss: 0.00091
Epoch: 5, step: 22, training loss: 0.00116
Epoch: 5, step: 23, training loss: 0.00139
Epoch: 5, step: 24, training loss: 0.00111
Epoch: 5, step: 25, training loss: 0.00129
Epoch: 5, step: 26, training loss: 0.00069
Epoch: 5, step: 27, training loss: 0.00096
Epoch: 5, step: 28, training loss: 0.00058
Epoch: 5, step: 29, training loss: 0.00136
Epoch: 5, step: 30, training loss: 0.00132
Epoch: 5, step: 31, training loss: 0.00066
Epoch: 5, step: 32, training loss: 0.00108
Epoch: 5, step: 33, training loss: 0.00063
Epoch: 5, step: 34, training loss: 0.00072
Epoch: 5, step: 35, training loss: 0.00131
Epoch: 5, step: 36, training loss: 0.00087
Epoch: 5, step: 37, training loss: 0.00113
Epoch: 5, step: 38, training loss: 0.00132
Epoch: 5, step: 39, training loss: 0.00080
Epoch: 5, step: 40, training loss: 0.00244
Epoch: 5, step: 41, training loss: 0.00054
Epoch: 5, step: 42, training loss: 0.00119
Epoch: 5, step: 43, training loss: 0.00107
Epoch: 5, step: 44, training loss: 0.00109
Epoch: 5, step: 45, training loss: 0.00135
Epoch: 5, step: 46, training loss: 0.00089
Epoch: 5, step: 47, training loss: 0.00097
Epoch: 5, step: 48, training loss: 0.00155
Epoch: 5, step: 49, training loss: 0.00073
Epoch: 5, step: 50, training loss: 0.00084
Epoch: 5, step: 51, training loss: 0.00079
Epoch: 5, step: 52, training loss: 0.00067
Epoch: 5, step: 53, training loss: 0.00100
Epoch: 5, step: 54, training loss: 0.00103
Epoch: 5, step: 55, training loss: 0.00196
Epoch: 5, step: 56, training loss: 0.00112
Epoch: 5, step: 57, training loss: 0.00143
Epoch: 5, step: 58, training loss: 0.00110
Epoch: 5, step: 59, training loss: 0.00171
Epoch: 5, step: 60, training loss: 0.00086
Epoch: 5, step: 61, training loss: 0.00122
Epoch: 5, step: 62, training loss: 0.00117
Epoch: 5, step: 63, training loss: 0.00154
Epoch: 5, step: 64, training loss: 0.00079
Epoch: 5, step: 65, training loss: 0.00196
Epoch: 5, step: 66, training loss: 0.00088
Epoch: 5, step: 67, training loss: 0.00105
Epoch: 5, step: 68, training loss: 0.00136
Epoch: 5, step: 69, training loss: 0.00081
Epoch: 5, step: 70, training loss: 0.00182
Epoch: 5, step: 71, training loss: 0.00128
Epoch: 5, step: 72, training loss: 0.00150
Epoch: 5, step: 73, training loss: 0.00135
Epoch: 5, step: 74, training loss: 0.00106
Epoch: 5, step: 75, training loss: 0.00229
Epoch: 5, step: 76, training loss: 0.00212
Epoch: 5, step: 77, training loss: 0.00129
Epoch: 5, step: 78, training loss: 0.00133
Epoch: 5, step: 79, training loss: 0.00137
Epoch: 5, step: 80, training loss: 0.00126
Epoch: 5, step: 81, training loss: 0.00088
Epoch: 5, step: 82, training loss: 0.00165
Epoch: 5, step: 83, training loss: 0.00083
Epoch: 5, step: 84, training loss: 0.00071
Epoch: 5, step: 85, training loss: 0.00119
Epoch: 5, step: 86, training loss: 0.00123
Epoch: 5, step: 87, training loss: 0.00101
Epoch: 5, step: 88, training loss: 0.00069
Epoch: 5, step: 89, training loss: 0.00164
Epoch: 5, step: 90, training loss: 0.00093
Epoch: 5, step: 91, training loss: 0.00234
Epoch: 5, step: 92, training loss: 0.00173
Epoch: 5, step: 93, training loss: 0.00122
Epoch: 5, step: 94, training loss: 0.00073
Epoch: 5, step: 95, training loss: 0.00137
Epoch: 5, step: 96, training loss: 0.00191
Epoch: 5, step: 97, training loss: 0.00108
Epoch: 5, step: 98, training loss: 0.00152
Epoch: 5, step: 99, training loss: 0.00149
Epoch: 5, step: 100, training loss: 0.00138
Epoch: 5, step: 101, training loss: 0.00165
Epoch: 5, step: 102, training loss: 0.00071
Epoch: 5, step: 103, training loss: 0.00111
Epoch: 5, step: 104, training loss: 0.00234
Epoch: 5, step: 105, training loss: 0.00098
Epoch: 5, step: 106, training loss: 0.00111
Epoch: 5, step: 107, training loss: 0.00143
Epoch: 5, step: 108, training loss: 0.00154
Epoch: 5, step: 109, training loss: 0.00168
Epoch: 5, step: 110, training loss: 0.00100
Epoch: 5, step: 111, training loss: 0.00080
Epoch: 5, step: 112, training loss: 0.00094
Epoch: 5, step: 113, training loss: 0.00106
Epoch: 5, step: 114, training loss: 0.00114
Epoch: 5, step: 115, training loss: 0.00097
Epoch: 5, step: 116, training loss: 0.00094
Epoch: 5, step: 117, training loss: 0.00120
Epoch: 5, step: 118, training loss: 0.00109
Epoch: 5, step: 119, training loss: 0.00155
Epoch: 5, step: 120, training loss: 0.00228
Epoch: 5, step: 121, training loss: 0.00106
Epoch: 5, step: 122, training loss: 0.00076
Epoch: 5, step: 123, training loss: 0.00101
Epoch: 5, step: 124, training loss: 0.00092
Epoch: 5, step: 125, training loss: 0.00211
Epoch: 5, step: 126, training loss: 0.00154
Epoch: 5, step: 127, training loss: 0.00068
Epoch: 5, step: 128, training loss: 0.00099
Epoch: 5, step: 129, training loss: 0.00292
Epoch: 5, step: 130, training loss: 0.00126
Epoch: 5, step: 131, training loss: 0.00106
Epoch: 5, step: 132, training loss: 0.00147
Epoch: 5, step: 133, training loss: 0.00097
Epoch: 5, step: 134, training loss: 0.00105
Epoch: 5, step: 135, training loss: 0.00202
Epoch: 5, step: 136, training loss: 0.00090
Epoch: 5, step: 137, training loss: 0.00116
Epoch: 5, step: 138, training loss: 0.00127
Epoch: 5, step: 139, training loss: 0.00115
Epoch: 5, step: 140, training loss: 0.00118
Epoch: 5, step: 141, training loss: 0.00093
Epoch: 5, step: 142, training loss: 0.00110
Epoch: 5, step: 143, training loss: 0.00115
Epoch: 5, step: 144, training loss: 0.00122
Epoch: 5, step: 145, training loss: 0.00142
Epoch: 5, step: 146, training loss: 0.00157
Epoch: 5, step: 147, training loss: 0.00086
Epoch: 5, step: 148, training loss: 0.00116
Epoch: 5, step: 149, training loss: 0.00095
Epoch: 5, step: 150, training loss: 0.00114
Epoch: 5, step: 151, training loss: 0.00062
Epoch: 5, step: 152, training loss: 0.00137
Epoch: 5, step: 153, training loss: 0.00093
Epoch: 5, step: 154, training loss: 0.00129
Epoch: 5, step: 155, training loss: 0.00151
Epoch: 5, step: 156, training loss: 0.00121
Epoch: 5, step: 157, training loss: 0.00080
Epoch: 5, step: 158, training loss: 0.00153
Epoch: 5, step: 159, training loss: 0.00099
Epoch: 5, step: 160, training loss: 0.00098
Epoch: 5, step: 161, training loss: 0.00088
Epoch: 5, step: 162, training loss: 0.00164
Epoch: 5, step: 163, training loss: 0.00112
Epoch: 5, step: 164, training loss: 0.00128
Epoch: 5, step: 165, training loss: 0.00099
Epoch: 5, step: 166, training loss: 0.00090
Epoch: 5, step: 167, training loss: 0.00119
Epoch: 5, step: 168, training loss: 0.00106
Epoch: 5, step: 169, training loss: 0.00181
Epoch: 5, step: 170, training loss: 0.00136
Epoch: 5, step: 171, training loss: 0.00095
Epoch: 5, step: 172, training loss: 0.00092
Epoch: 5, step: 173, training loss: 0.00119
Epoch: 5, step: 174, training loss: 0.00119
Epoch: 5, step: 175, training loss: 0.00179
Epoch: 5, step: 176, training loss: 0.00122
Epoch: 5, step: 177, training loss: 0.00155
Epoch: 5, step: 178, training loss: 0.00084
Epoch: 5, step: 179, training loss: 0.00167
Epoch: 5, step: 180, training loss: 0.00063
Epoch: 5, step: 181, training loss: 0.00127
Epoch: 5, step: 182, training loss: 0.00100
Epoch: 5, step: 183, training loss: 0.00147
Epoch: 5, step: 184, training loss: 0.00099
Epoch: 5, step: 185, training loss: 0.00088
Epoch: 5, step: 186, training loss: 0.00174
Epoch: 5, step: 187, training loss: 0.00091
Epoch: 5, step: 188, training loss: 0.00142
Epoch: 5, step: 189, training loss: 0.00126
Epoch: 5, step: 190, training loss: 0.00137
Epoch: 5, step: 191, training loss: 0.00117
Epoch: 5, step: 192, training loss: 0.00117
Epoch: 5, step: 193, training loss: 0.00103
Epoch: 5, step: 194, training loss: 0.00167
Epoch: 5, step: 195, training loss: 0.00090
Epoch: 5, step: 196, training loss: 0.00074
Epoch: 5, step: 197, training loss: 0.00080
Epoch: 5, step: 198, training loss: 0.00084
Epoch: 5, step: 199, training loss: 0.00148
Epoch: 5, step: 200, training loss: 0.00091
Epoch: 5, step: 201, training loss: 0.00083
Epoch: 5, step: 202, training loss: 0.00075
Epoch: 5, step: 203, training loss: 0.00128
Epoch: 5, step: 204, training loss: 0.00110
Epoch: 5, step: 205, training loss: 0.00121
Epoch: 5, step: 206, training loss: 0.00065
Epoch: 5, step: 207, training loss: 0.00097
Epoch: 5, step: 208, training loss: 0.00129
Epoch: 5, step: 209, training loss: 0.00090
Epoch: 5, step: 210, training loss: 0.00108
Epoch: 5, step: 211, training loss: 0.00135
Epoch: 5, step: 212, training loss: 0.00081
Epoch: 5, step: 213, training loss: 0.00113
Epoch: 5, step: 214, training loss: 0.00105
Epoch: 5, step: 215, training loss: 0.00079
Epoch: 5, step: 216, training loss: 0.00101
Epoch: 5, step: 217, training loss: 0.00087
Epoch: 5, step: 218, training loss: 0.00098
Epoch: 5, step: 219, training loss: 0.00085
Epoch: 5, step: 220, training loss: 0.00122
Epoch: 5, step: 221, training loss: 0.00074
Epoch: 5, step: 222, training loss: 0.00038
Epoch: 5, step: 223, training loss: 0.00050
Epoch: 5, step: 224, training loss: 0.00078
Epoch: 5, step: 225, training loss: 0.00154
Epoch: 5, step: 226, training loss: 0.00146
Epoch: 5, step: 227, training loss: 0.00152
Epoch: 5, step: 228, training loss: 0.00073
Epoch: 5, step: 229, training loss: 0.00066
Epoch: 5, step: 230, training loss: 0.00085
Epoch: 5, step: 231, training loss: 0.00155
Epoch: 5, step: 232, training loss: 0.00120
Epoch: 5, step: 233, training loss: 0.00107
Epoch: 5, step: 234, training loss: 0.00242
Epoch: 5, step: 235, training loss: 0.00115
Epoch: 5, step: 236, training loss: 0.00106
Epoch: 5, step: 237, training loss: 0.00064
Epoch: 5, step: 238, training loss: 0.00120
Epoch: 5, average training loss: 0.00118
Epoch: 5, F1: 83.60390, average dev loss: 0.00259
Epoch: 6, step: 0, training loss: 0.00049
Epoch: 6, step: 1, training loss: 0.00071
Epoch: 6, step: 2, training loss: 0.00037
Epoch: 6, step: 3, training loss: 0.00063
Epoch: 6, step: 4, training loss: 0.00079
Epoch: 6, step: 5, training loss: 0.00094
Epoch: 6, step: 6, training loss: 0.00068
Epoch: 6, step: 7, training loss: 0.00086
Epoch: 6, step: 8, training loss: 0.00092
Epoch: 6, step: 9, training loss: 0.00089
Epoch: 6, step: 10, training loss: 0.00064
Epoch: 6, step: 11, training loss: 0.00068
Epoch: 6, step: 12, training loss: 0.00083
Epoch: 6, step: 13, training loss: 0.00065
Epoch: 6, step: 14, training loss: 0.00054
Epoch: 6, step: 15, training loss: 0.00088
Epoch: 6, step: 16, training loss: 0.00155
Epoch: 6, step: 17, training loss: 0.00090
Epoch: 6, step: 18, training loss: 0.00095
Epoch: 6, step: 19, training loss: 0.00067
Epoch: 6, step: 20, training loss: 0.00089
Epoch: 6, step: 21, training loss: 0.00057
Epoch: 6, step: 22, training loss: 0.00080
Epoch: 6, step: 23, training loss: 0.00066
Epoch: 6, step: 24, training loss: 0.00060
Epoch: 6, step: 25, training loss: 0.00059
Epoch: 6, step: 26, training loss: 0.00058
Epoch: 6, step: 27, training loss: 0.00039
Epoch: 6, step: 28, training loss: 0.00037
Epoch: 6, step: 29, training loss: 0.00097
Epoch: 6, step: 30, training loss: 0.00071
Epoch: 6, step: 31, training loss: 0.00087
Epoch: 6, step: 32, training loss: 0.00075
Epoch: 6, step: 33, training loss: 0.00127
Epoch: 6, step: 34, training loss: 0.00097
Epoch: 6, step: 35, training loss: 0.00064
Epoch: 6, step: 36, training loss: 0.00149
Epoch: 6, step: 37, training loss: 0.00126
Epoch: 6, step: 38, training loss: 0.00080
Epoch: 6, step: 39, training loss: 0.00057
Epoch: 6, step: 40, training loss: 0.00054
Epoch: 6, step: 41, training loss: 0.00078
Epoch: 6, step: 42, training loss: 0.00067
Epoch: 6, step: 43, training loss: 0.00075
Epoch: 6, step: 44, training loss: 0.00131
Epoch: 6, step: 45, training loss: 0.00101
Epoch: 6, step: 46, training loss: 0.00092
Epoch: 6, step: 47, training loss: 0.00109
Epoch: 6, step: 48, training loss: 0.00073
Epoch: 6, step: 49, training loss: 0.00076
Epoch: 6, step: 50, training loss: 0.00120
Epoch: 6, step: 51, training loss: 0.00095
Epoch: 6, step: 52, training loss: 0.00103
Epoch: 6, step: 53, training loss: 0.00114
Epoch: 6, step: 54, training loss: 0.00073
Epoch: 6, step: 55, training loss: 0.00114
Epoch: 6, step: 56, training loss: 0.00078
Epoch: 6, step: 57, training loss: 0.00097
Epoch: 6, step: 58, training loss: 0.00108
Epoch: 6, step: 59, training loss: 0.00122
Epoch: 6, step: 60, training loss: 0.00079
Epoch: 6, step: 61, training loss: 0.00099
Epoch: 6, step: 62, training loss: 0.00095
Epoch: 6, step: 63, training loss: 0.00123
Epoch: 6, step: 64, training loss: 0.00075
Epoch: 6, step: 65, training loss: 0.00085
Epoch: 6, step: 66, training loss: 0.00051
Epoch: 6, step: 67, training loss: 0.00142
Epoch: 6, step: 68, training loss: 0.00097
Epoch: 6, step: 69, training loss: 0.00047
Epoch: 6, step: 70, training loss: 0.00066
Epoch: 6, step: 71, training loss: 0.00086
Epoch: 6, step: 72, training loss: 0.00055
Epoch: 6, step: 73, training loss: 0.00068
Epoch: 6, step: 74, training loss: 0.00103
Epoch: 6, step: 75, training loss: 0.00105
Epoch: 6, step: 76, training loss: 0.00057
Epoch: 6, step: 77, training loss: 0.00097
Epoch: 6, step: 78, training loss: 0.00054
Epoch: 6, step: 79, training loss: 0.00127
Epoch: 6, step: 80, training loss: 0.00080
Epoch: 6, step: 81, training loss: 0.00048
Epoch: 6, step: 82, training loss: 0.00057
Epoch: 6, step: 83, training loss: 0.00086
Epoch: 6, step: 84, training loss: 0.00046
Epoch: 6, step: 85, training loss: 0.00126
Epoch: 6, step: 86, training loss: 0.00082
Epoch: 6, step: 87, training loss: 0.00079
Epoch: 6, step: 88, training loss: 0.00119
Epoch: 6, step: 89, training loss: 0.00089
Epoch: 6, step: 90, training loss: 0.00053
Epoch: 6, step: 91, training loss: 0.00074
Epoch: 6, step: 92, training loss: 0.00081
Epoch: 6, step: 93, training loss: 0.00055
Epoch: 6, step: 94, training loss: 0.00048
Epoch: 6, step: 95, training loss: 0.00072
Epoch: 6, step: 96, training loss: 0.00047
Epoch: 6, step: 97, training loss: 0.00063
Epoch: 6, step: 98, training loss: 0.00121
Epoch: 6, step: 99, training loss: 0.00068
Epoch: 6, step: 100, training loss: 0.00090
Epoch: 6, step: 101, training loss: 0.00092
Epoch: 6, step: 102, training loss: 0.00074
Epoch: 6, step: 103, training loss: 0.00081
Epoch: 6, step: 104, training loss: 0.00071
Epoch: 6, step: 105, training loss: 0.00061
Epoch: 6, step: 106, training loss: 0.00051
Epoch: 6, step: 107, training loss: 0.00049
Epoch: 6, step: 108, training loss: 0.00070
Epoch: 6, step: 109, training loss: 0.00075
Epoch: 6, step: 110, training loss: 0.00055
Epoch: 6, step: 111, training loss: 0.00092
Epoch: 6, step: 112, training loss: 0.00067
Epoch: 6, step: 113, training loss: 0.00094
Epoch: 6, step: 114, training loss: 0.00198
Epoch: 6, step: 115, training loss: 0.00100
Epoch: 6, step: 116, training loss: 0.00075
Epoch: 6, step: 117, training loss: 0.00121
Epoch: 6, step: 118, training loss: 0.00054
Epoch: 6, step: 119, training loss: 0.00079
Epoch: 6, step: 120, training loss: 0.00098
Epoch: 6, step: 121, training loss: 0.00070
Epoch: 6, step: 122, training loss: 0.00046
Epoch: 6, step: 123, training loss: 0.00124
Epoch: 6, step: 124, training loss: 0.00099
Epoch: 6, step: 125, training loss: 0.00103
Epoch: 6, step: 126, training loss: 0.00105
Epoch: 6, step: 127, training loss: 0.00043
Epoch: 6, step: 128, training loss: 0.00077
Epoch: 6, step: 129, training loss: 0.00068
Epoch: 6, step: 130, training loss: 0.00065
Epoch: 6, step: 131, training loss: 0.00162
Epoch: 6, step: 132, training loss: 0.00085
Epoch: 6, step: 133, training loss: 0.00081
Epoch: 6, step: 134, training loss: 0.00062
Epoch: 6, step: 135, training loss: 0.00093
Epoch: 6, step: 136, training loss: 0.00080
Epoch: 6, step: 137, training loss: 0.00096
Epoch: 6, step: 138, training loss: 0.00058
Epoch: 6, step: 139, training loss: 0.00121
Epoch: 6, step: 140, training loss: 0.00039
Epoch: 6, step: 141, training loss: 0.00104
Epoch: 6, step: 142, training loss: 0.00067
Epoch: 6, step: 143, training loss: 0.00072
Epoch: 6, step: 144, training loss: 0.00071
Epoch: 6, step: 145, training loss: 0.00040
Epoch: 6, step: 146, training loss: 0.00088
Epoch: 6, step: 147, training loss: 0.00087
Epoch: 6, step: 148, training loss: 0.00054
Epoch: 6, step: 149, training loss: 0.00059
Epoch: 6, step: 150, training loss: 0.00102
Epoch: 6, step: 151, training loss: 0.00062
Epoch: 6, step: 152, training loss: 0.00085
Epoch: 6, step: 153, training loss: 0.00034
Epoch: 6, step: 154, training loss: 0.00098
Epoch: 6, step: 155, training loss: 0.00104
Epoch: 6, step: 156, training loss: 0.00038
Epoch: 6, step: 157, training loss: 0.00058
Epoch: 6, step: 158, training loss: 0.00076
Epoch: 6, step: 159, training loss: 0.00077
Epoch: 6, step: 160, training loss: 0.00138
Epoch: 6, step: 161, training loss: 0.00082
Epoch: 6, step: 162, training loss: 0.00090
Epoch: 6, step: 163, training loss: 0.00065
Epoch: 6, step: 164, training loss: 0.00062
Epoch: 6, step: 165, training loss: 0.00114
Epoch: 6, step: 166, training loss: 0.00056
Epoch: 6, step: 167, training loss: 0.00057
Epoch: 6, step: 168, training loss: 0.00082
Epoch: 6, step: 169, training loss: 0.00060
Epoch: 6, step: 170, training loss: 0.00090
Epoch: 6, step: 171, training loss: 0.00050
Epoch: 6, step: 172, training loss: 0.00146
Epoch: 6, step: 173, training loss: 0.00056
Epoch: 6, step: 174, training loss: 0.00053
Epoch: 6, step: 175, training loss: 0.00069
Epoch: 6, step: 176, training loss: 0.00094
Epoch: 6, step: 177, training loss: 0.00065
Epoch: 6, step: 178, training loss: 0.00038
Epoch: 6, step: 179, training loss: 0.00057
Epoch: 6, step: 180, training loss: 0.00067
Epoch: 6, step: 181, training loss: 0.00067
Epoch: 6, step: 182, training loss: 0.00120
Epoch: 6, step: 183, training loss: 0.00060
Epoch: 6, step: 184, training loss: 0.00090
Epoch: 6, step: 185, training loss: 0.00125
Epoch: 6, step: 186, training loss: 0.00089
Epoch: 6, step: 187, training loss: 0.00078
Epoch: 6, step: 188, training loss: 0.00101
Epoch: 6, step: 189, training loss: 0.00109
Epoch: 6, step: 190, training loss: 0.00092
Epoch: 6, step: 191, training loss: 0.00026
Epoch: 6, step: 192, training loss: 0.00068
Epoch: 6, step: 193, training loss: 0.00087
Epoch: 6, step: 194, training loss: 0.00113
Epoch: 6, step: 195, training loss: 0.00071
Epoch: 6, step: 196, training loss: 0.00095
Epoch: 6, step: 197, training loss: 0.00103
Epoch: 6, step: 198, training loss: 0.00083
Epoch: 6, step: 199, training loss: 0.00078
Epoch: 6, step: 200, training loss: 0.00101
Epoch: 6, step: 201, training loss: 0.00078
Epoch: 6, step: 202, training loss: 0.00044
Epoch: 6, step: 203, training loss: 0.00113
Epoch: 6, step: 204, training loss: 0.00066
Epoch: 6, step: 205, training loss: 0.00044
Epoch: 6, step: 206, training loss: 0.00088
Epoch: 6, step: 207, training loss: 0.00050
Epoch: 6, step: 208, training loss: 0.00089
Epoch: 6, step: 209, training loss: 0.00089
Epoch: 6, step: 210, training loss: 0.00069
Epoch: 6, step: 211, training loss: 0.00070
Epoch: 6, step: 212, training loss: 0.00100
Epoch: 6, step: 213, training loss: 0.00037
Epoch: 6, step: 214, training loss: 0.00070
Epoch: 6, step: 215, training loss: 0.00107
Epoch: 6, step: 216, training loss: 0.00126
Epoch: 6, step: 217, training loss: 0.00066
Epoch: 6, step: 218, training loss: 0.00066
Epoch: 6, step: 219, training loss: 0.00077
Epoch: 6, step: 220, training loss: 0.00069
Epoch: 6, step: 221, training loss: 0.00083
Epoch: 6, step: 222, training loss: 0.00085
Epoch: 6, step: 223, training loss: 0.00085
Epoch: 6, step: 224, training loss: 0.00054
Epoch: 6, step: 225, training loss: 0.00108
Epoch: 6, step: 226, training loss: 0.00085
Epoch: 6, step: 227, training loss: 0.00081
Epoch: 6, step: 228, training loss: 0.00088
Epoch: 6, step: 229, training loss: 0.00043
Epoch: 6, step: 230, training loss: 0.00072
Epoch: 6, step: 231, training loss: 0.00086
Epoch: 6, step: 232, training loss: 0.00113
Epoch: 6, step: 233, training loss: 0.00065
Epoch: 6, step: 234, training loss: 0.00062
Epoch: 6, step: 235, training loss: 0.00082
Epoch: 6, step: 236, training loss: 0.00075
Epoch: 6, step: 237, training loss: 0.00044
Epoch: 6, step: 238, training loss: 0.00050
Epoch: 6, average training loss: 0.00081
Epoch: 6, F1: 83.65262, average dev loss: 0.00285
Epoch: 7, step: 0, training loss: 0.00071
Epoch: 7, step: 1, training loss: 0.00035
Epoch: 7, step: 2, training loss: 0.00062
Epoch: 7, step: 3, training loss: 0.00069
Epoch: 7, step: 4, training loss: 0.00080
Epoch: 7, step: 5, training loss: 0.00035
Epoch: 7, step: 6, training loss: 0.00028
Epoch: 7, step: 7, training loss: 0.00053
Epoch: 7, step: 8, training loss: 0.00070
Epoch: 7, step: 9, training loss: 0.00066
Epoch: 7, step: 10, training loss: 0.00066
Epoch: 7, step: 11, training loss: 0.00061
Epoch: 7, step: 12, training loss: 0.00076
Epoch: 7, step: 13, training loss: 0.00070
Epoch: 7, step: 14, training loss: 0.00052
Epoch: 7, step: 15, training loss: 0.00113
Epoch: 7, step: 16, training loss: 0.00104
Epoch: 7, step: 17, training loss: 0.00059
Epoch: 7, step: 18, training loss: 0.00036
Epoch: 7, step: 19, training loss: 0.00066
Epoch: 7, step: 20, training loss: 0.00046
Epoch: 7, step: 21, training loss: 0.00051
Epoch: 7, step: 22, training loss: 0.00046
Epoch: 7, step: 23, training loss: 0.00031
Epoch: 7, step: 24, training loss: 0.00067
Epoch: 7, step: 25, training loss: 0.00070
Epoch: 7, step: 26, training loss: 0.00040
Epoch: 7, step: 27, training loss: 0.00043
Epoch: 7, step: 28, training loss: 0.00046
Epoch: 7, step: 29, training loss: 0.00064
Epoch: 7, step: 30, training loss: 0.00051
Epoch: 7, step: 31, training loss: 0.00075
Epoch: 7, step: 32, training loss: 0.00068
Epoch: 7, step: 33, training loss: 0.00088
Epoch: 7, step: 34, training loss: 0.00053
Epoch: 7, step: 35, training loss: 0.00045
Epoch: 7, step: 36, training loss: 0.00053
Epoch: 7, step: 37, training loss: 0.00062
Epoch: 7, step: 38, training loss: 0.00055
Epoch: 7, step: 39, training loss: 0.00050
Epoch: 7, step: 40, training loss: 0.00052
Epoch: 7, step: 41, training loss: 0.00042
Epoch: 7, step: 42, training loss: 0.00066
Epoch: 7, step: 43, training loss: 0.00059
Epoch: 7, step: 44, training loss: 0.00054
Epoch: 7, step: 45, training loss: 0.00086
Epoch: 7, step: 46, training loss: 0.00040
Epoch: 7, step: 47, training loss: 0.00063
Epoch: 7, step: 48, training loss: 0.00063
Epoch: 7, step: 49, training loss: 0.00053
Epoch: 7, step: 50, training loss: 0.00087
Epoch: 7, step: 51, training loss: 0.00065
Epoch: 7, step: 52, training loss: 0.00044
Epoch: 7, step: 53, training loss: 0.00069
Epoch: 7, step: 54, training loss: 0.00039
Epoch: 7, step: 55, training loss: 0.00046
Epoch: 7, step: 56, training loss: 0.00114
Epoch: 7, step: 57, training loss: 0.00091
Epoch: 7, step: 58, training loss: 0.00033
Epoch: 7, step: 59, training loss: 0.00049
Epoch: 7, step: 60, training loss: 0.00069
Epoch: 7, step: 61, training loss: 0.00077
Epoch: 7, step: 62, training loss: 0.00047
Epoch: 7, step: 63, training loss: 0.00031
Epoch: 7, step: 64, training loss: 0.00086
Epoch: 7, step: 65, training loss: 0.00038
Epoch: 7, step: 66, training loss: 0.00060
Epoch: 7, step: 67, training loss: 0.00023
Epoch: 7, step: 68, training loss: 0.00035
Epoch: 7, step: 69, training loss: 0.00042
Epoch: 7, step: 70, training loss: 0.00079
Epoch: 7, step: 71, training loss: 0.00052
Epoch: 7, step: 72, training loss: 0.00056
Epoch: 7, step: 73, training loss: 0.00046
Epoch: 7, step: 74, training loss: 0.00067
Epoch: 7, step: 75, training loss: 0.00045
Epoch: 7, step: 76, training loss: 0.00061
Epoch: 7, step: 77, training loss: 0.00049
Epoch: 7, step: 78, training loss: 0.00039
Epoch: 7, step: 79, training loss: 0.00056
Epoch: 7, step: 80, training loss: 0.00066
Epoch: 7, step: 81, training loss: 0.00068
Epoch: 7, step: 82, training loss: 0.00039
Epoch: 7, step: 83, training loss: 0.00068
Epoch: 7, step: 84, training loss: 0.00062
Epoch: 7, step: 85, training loss: 0.00101
Epoch: 7, step: 86, training loss: 0.00063
Epoch: 7, step: 87, training loss: 0.00069
Epoch: 7, step: 88, training loss: 0.00054
Epoch: 7, step: 89, training loss: 0.00051
Epoch: 7, step: 90, training loss: 0.00060
Epoch: 7, step: 91, training loss: 0.00063
Epoch: 7, step: 92, training loss: 0.00062
Epoch: 7, step: 93, training loss: 0.00079
Epoch: 7, step: 94, training loss: 0.00080
Epoch: 7, step: 95, training loss: 0.00046
Epoch: 7, step: 96, training loss: 0.00049
Epoch: 7, step: 97, training loss: 0.00063
Epoch: 7, step: 98, training loss: 0.00053
Epoch: 7, step: 99, training loss: 0.00089
Epoch: 7, step: 100, training loss: 0.00059
Epoch: 7, step: 101, training loss: 0.00049
Epoch: 7, step: 102, training loss: 0.00065
Epoch: 7, step: 103, training loss: 0.00073
Epoch: 7, step: 104, training loss: 0.00047
Epoch: 7, step: 105, training loss: 0.00046
Epoch: 7, step: 106, training loss: 0.00089
Epoch: 7, step: 107, training loss: 0.00074
Epoch: 7, step: 108, training loss: 0.00136
Epoch: 7, step: 109, training loss: 0.00052
Epoch: 7, step: 110, training loss: 0.00035
Epoch: 7, step: 111, training loss: 0.00079
Epoch: 7, step: 112, training loss: 0.00054
Epoch: 7, step: 113, training loss: 0.00039
Epoch: 7, step: 114, training loss: 0.00053
Epoch: 7, step: 115, training loss: 0.00043
Epoch: 7, step: 116, training loss: 0.00071
Epoch: 7, step: 117, training loss: 0.00061
Epoch: 7, step: 118, training loss: 0.00074
Epoch: 7, step: 119, training loss: 0.00058
Epoch: 7, step: 120, training loss: 0.00045
Epoch: 7, step: 121, training loss: 0.00059
Epoch: 7, step: 122, training loss: 0.00052
Epoch: 7, step: 123, training loss: 0.00064
Epoch: 7, step: 124, training loss: 0.00030
Epoch: 7, step: 125, training loss: 0.00046
Epoch: 7, step: 126, training loss: 0.00058
Epoch: 7, step: 127, training loss: 0.00041
Epoch: 7, step: 128, training loss: 0.00073
Epoch: 7, step: 129, training loss: 0.00067
Epoch: 7, step: 130, training loss: 0.00040
Epoch: 7, step: 131, training loss: 0.00062
Epoch: 7, step: 132, training loss: 0.00038
Epoch: 7, step: 133, training loss: 0.00037
Epoch: 7, step: 134, training loss: 0.00049
Epoch: 7, step: 135, training loss: 0.00087
Epoch: 7, step: 136, training loss: 0.00071
Epoch: 7, step: 137, training loss: 0.00055
Epoch: 7, step: 138, training loss: 0.00094
Epoch: 7, step: 139, training loss: 0.00061
Epoch: 7, step: 140, training loss: 0.00033
Epoch: 7, step: 141, training loss: 0.00095
Epoch: 7, step: 142, training loss: 0.00061
Epoch: 7, step: 143, training loss: 0.00059
Epoch: 7, step: 144, training loss: 0.00048
Epoch: 7, step: 145, training loss: 0.00051
Epoch: 7, step: 146, training loss: 0.00059
Epoch: 7, step: 147, training loss: 0.00046
Epoch: 7, step: 148, training loss: 0.00049
Epoch: 7, step: 149, training loss: 0.00112
Epoch: 7, step: 150, training loss: 0.00041
Epoch: 7, step: 151, training loss: 0.00068
Epoch: 7, step: 152, training loss: 0.00055
Epoch: 7, step: 153, training loss: 0.00056
Epoch: 7, step: 154, training loss: 0.00107
Epoch: 7, step: 155, training loss: 0.00065
Epoch: 7, step: 156, training loss: 0.00051
Epoch: 7, step: 157, training loss: 0.00067
Epoch: 7, step: 158, training loss: 0.00025
Epoch: 7, step: 159, training loss: 0.00055
Epoch: 7, step: 160, training loss: 0.00073
Epoch: 7, step: 161, training loss: 0.00100
Epoch: 7, step: 162, training loss: 0.00092
Epoch: 7, step: 163, training loss: 0.00049
Epoch: 7, step: 164, training loss: 0.00075
Epoch: 7, step: 165, training loss: 0.00070
Epoch: 7, step: 166, training loss: 0.00058
Epoch: 7, step: 167, training loss: 0.00068
Epoch: 7, step: 168, training loss: 0.00034
Epoch: 7, step: 169, training loss: 0.00031
Epoch: 7, step: 170, training loss: 0.00044
Epoch: 7, step: 171, training loss: 0.00043
Epoch: 7, step: 172, training loss: 0.00050
Epoch: 7, step: 173, training loss: 0.00061
Epoch: 7, step: 174, training loss: 0.00063
Epoch: 7, step: 175, training loss: 0.00056
Epoch: 7, step: 176, training loss: 0.00071
Epoch: 7, step: 177, training loss: 0.00029
Epoch: 7, step: 178, training loss: 0.00060
Epoch: 7, step: 179, training loss: 0.00103
Epoch: 7, step: 180, training loss: 0.00064
Epoch: 7, step: 181, training loss: 0.00057
Epoch: 7, step: 182, training loss: 0.00091
Epoch: 7, step: 183, training loss: 0.00037
Epoch: 7, step: 184, training loss: 0.00125
Epoch: 7, step: 185, training loss: 0.00045
Epoch: 7, step: 186, training loss: 0.00060
Epoch: 7, step: 187, training loss: 0.00046
Epoch: 7, step: 188, training loss: 0.00043
Epoch: 7, step: 189, training loss: 0.00091
Epoch: 7, step: 190, training loss: 0.00086
Epoch: 7, step: 191, training loss: 0.00068
Epoch: 7, step: 192, training loss: 0.00033
Epoch: 7, step: 193, training loss: 0.00055
Epoch: 7, step: 194, training loss: 0.00085
Epoch: 7, step: 195, training loss: 0.00065
Epoch: 7, step: 196, training loss: 0.00033
Epoch: 7, step: 197, training loss: 0.00034
Epoch: 7, step: 198, training loss: 0.00055
Epoch: 7, step: 199, training loss: 0.00036
Epoch: 7, step: 200, training loss: 0.00044
Epoch: 7, step: 201, training loss: 0.00050
Epoch: 7, step: 202, training loss: 0.00083
Epoch: 7, step: 203, training loss: 0.00072
Epoch: 7, step: 204, training loss: 0.00043
Epoch: 7, step: 205, training loss: 0.00051
Epoch: 7, step: 206, training loss: 0.00071
Epoch: 7, step: 207, training loss: 0.00057
Epoch: 7, step: 208, training loss: 0.00069
Epoch: 7, step: 209, training loss: 0.00062
Epoch: 7, step: 210, training loss: 0.00049
Epoch: 7, step: 211, training loss: 0.00060
Epoch: 7, step: 212, training loss: 0.00154
Epoch: 7, step: 213, training loss: 0.00047
Epoch: 7, step: 214, training loss: 0.00074
Epoch: 7, step: 215, training loss: 0.00083
Epoch: 7, step: 216, training loss: 0.00081
Epoch: 7, step: 217, training loss: 0.00073
Epoch: 7, step: 218, training loss: 0.00066
Epoch: 7, step: 219, training loss: 0.00075
Epoch: 7, step: 220, training loss: 0.00062
Epoch: 7, step: 221, training loss: 0.00067
Epoch: 7, step: 222, training loss: 0.00073
Epoch: 7, step: 223, training loss: 0.00034
Epoch: 7, step: 224, training loss: 0.00062
Epoch: 7, step: 225, training loss: 0.00041
Epoch: 7, step: 226, training loss: 0.00055
Epoch: 7, step: 227, training loss: 0.00050
Epoch: 7, step: 228, training loss: 0.00061
Epoch: 7, step: 229, training loss: 0.00117
Epoch: 7, step: 230, training loss: 0.00044
Epoch: 7, step: 231, training loss: 0.00044
Epoch: 7, step: 232, training loss: 0.00037
Epoch: 7, step: 233, training loss: 0.00055
Epoch: 7, step: 234, training loss: 0.00033
Epoch: 7, step: 235, training loss: 0.00058
Epoch: 7, step: 236, training loss: 0.00032
Epoch: 7, step: 237, training loss: 0.00049
Epoch: 7, step: 238, training loss: 0.00030
Epoch: 7, average training loss: 0.00060
Epoch: 7, F1: 84.34658, average dev loss: 0.00282
Epoch: 8, step: 0, training loss: 0.00055
Epoch: 8, step: 1, training loss: 0.00049
Epoch: 8, step: 2, training loss: 0.00070
Epoch: 8, step: 3, training loss: 0.00069
Epoch: 8, step: 4, training loss: 0.00042
Epoch: 8, step: 5, training loss: 0.00054
Epoch: 8, step: 6, training loss: 0.00031
Epoch: 8, step: 7, training loss: 0.00039
Epoch: 8, step: 8, training loss: 0.00052
Epoch: 8, step: 9, training loss: 0.00048
Epoch: 8, step: 10, training loss: 0.00041
Epoch: 8, step: 11, training loss: 0.00067
Epoch: 8, step: 12, training loss: 0.00057
Epoch: 8, step: 13, training loss: 0.00030
Epoch: 8, step: 14, training loss: 0.00048
Epoch: 8, step: 15, training loss: 0.00031
Epoch: 8, step: 16, training loss: 0.00027
Epoch: 8, step: 17, training loss: 0.00058
Epoch: 8, step: 18, training loss: 0.00020
Epoch: 8, step: 19, training loss: 0.00056
Epoch: 8, step: 20, training loss: 0.00099
Epoch: 8, step: 21, training loss: 0.00036
Epoch: 8, step: 22, training loss: 0.00098
Epoch: 8, step: 23, training loss: 0.00029
Epoch: 8, step: 24, training loss: 0.00036
Epoch: 8, step: 25, training loss: 0.00039
Epoch: 8, step: 26, training loss: 0.00080
Epoch: 8, step: 27, training loss: 0.00055
Epoch: 8, step: 28, training loss: 0.00035
Epoch: 8, step: 29, training loss: 0.00053
Epoch: 8, step: 30, training loss: 0.00049
Epoch: 8, step: 31, training loss: 0.00070
Epoch: 8, step: 32, training loss: 0.00045
Epoch: 8, step: 33, training loss: 0.00077
Epoch: 8, step: 34, training loss: 0.00023
Epoch: 8, step: 35, training loss: 0.00039
Epoch: 8, step: 36, training loss: 0.00043
Epoch: 8, step: 37, training loss: 0.00087
Epoch: 8, step: 38, training loss: 0.00031
Epoch: 8, step: 39, training loss: 0.00046
Epoch: 8, step: 40, training loss: 0.00046
Epoch: 8, step: 41, training loss: 0.00055
Epoch: 8, step: 42, training loss: 0.00033
Epoch: 8, step: 43, training loss: 0.00045
Epoch: 8, step: 44, training loss: 0.00048
Epoch: 8, step: 45, training loss: 0.00059
Epoch: 8, step: 46, training loss: 0.00028
Epoch: 8, step: 47, training loss: 0.00023
Epoch: 8, step: 48, training loss: 0.00028
Epoch: 8, step: 49, training loss: 0.00040
Epoch: 8, step: 50, training loss: 0.00063
Epoch: 8, step: 51, training loss: 0.00037
Epoch: 8, step: 52, training loss: 0.00071
Epoch: 8, step: 53, training loss: 0.00056
Epoch: 8, step: 54, training loss: 0.00070
Epoch: 8, step: 55, training loss: 0.00047
Epoch: 8, step: 56, training loss: 0.00042
Epoch: 8, step: 57, training loss: 0.00086
Epoch: 8, step: 58, training loss: 0.00046
Epoch: 8, step: 59, training loss: 0.00013
Epoch: 8, step: 60, training loss: 0.00049
Epoch: 8, step: 61, training loss: 0.00031
Epoch: 8, step: 62, training loss: 0.00051
Epoch: 8, step: 63, training loss: 0.00056
Epoch: 8, step: 64, training loss: 0.00031
Epoch: 8, step: 65, training loss: 0.00032
Epoch: 8, step: 66, training loss: 0.00079
Epoch: 8, step: 67, training loss: 0.00035
Epoch: 8, step: 68, training loss: 0.00050
Epoch: 8, step: 69, training loss: 0.00041
Epoch: 8, step: 70, training loss: 0.00064
Epoch: 8, step: 71, training loss: 0.00069
Epoch: 8, step: 72, training loss: 0.00046
Epoch: 8, step: 73, training loss: 0.00041
Epoch: 8, step: 74, training loss: 0.00038
Epoch: 8, step: 75, training loss: 0.00045
Epoch: 8, step: 76, training loss: 0.00064
Epoch: 8, step: 77, training loss: 0.00062
Epoch: 8, step: 78, training loss: 0.00051
Epoch: 8, step: 79, training loss: 0.00035
Epoch: 8, step: 80, training loss: 0.00060
Epoch: 8, step: 81, training loss: 0.00053
Epoch: 8, step: 82, training loss: 0.00049
Epoch: 8, step: 83, training loss: 0.00033
Epoch: 8, step: 84, training loss: 0.00030
Epoch: 8, step: 85, training loss: 0.00053
Epoch: 8, step: 86, training loss: 0.00041
Epoch: 8, step: 87, training loss: 0.00052
Epoch: 8, step: 88, training loss: 0.00038
Epoch: 8, step: 89, training loss: 0.00060
Epoch: 8, step: 90, training loss: 0.00078
Epoch: 8, step: 91, training loss: 0.00057
Epoch: 8, step: 92, training loss: 0.00074
Epoch: 8, step: 93, training loss: 0.00078
Epoch: 8, step: 94, training loss: 0.00069
Epoch: 8, step: 95, training loss: 0.00085
Epoch: 8, step: 96, training loss: 0.00050
Epoch: 8, step: 97, training loss: 0.00065
Epoch: 8, step: 98, training loss: 0.00059
Epoch: 8, step: 99, training loss: 0.00034
Epoch: 8, step: 100, training loss: 0.00068
Epoch: 8, step: 101, training loss: 0.00049
Epoch: 8, step: 102, training loss: 0.00068
Epoch: 8, step: 103, training loss: 0.00041
Epoch: 8, step: 104, training loss: 0.00031
Epoch: 8, step: 105, training loss: 0.00028
Epoch: 8, step: 106, training loss: 0.00038
Epoch: 8, step: 107, training loss: 0.00047
Epoch: 8, step: 108, training loss: 0.00094
Epoch: 8, step: 109, training loss: 0.00045
Epoch: 8, step: 110, training loss: 0.00021
Epoch: 8, step: 111, training loss: 0.00055
Epoch: 8, step: 112, training loss: 0.00079
Epoch: 8, step: 113, training loss: 0.00045
Epoch: 8, step: 114, training loss: 0.00040
Epoch: 8, step: 115, training loss: 0.00048
Epoch: 8, step: 116, training loss: 0.00056
Epoch: 8, step: 117, training loss: 0.00033
Epoch: 8, step: 118, training loss: 0.00053
Epoch: 8, step: 119, training loss: 0.00036
Epoch: 8, step: 120, training loss: 0.00078
Epoch: 8, step: 121, training loss: 0.00068
Epoch: 8, step: 122, training loss: 0.00053
Epoch: 8, step: 123, training loss: 0.00041
Epoch: 8, step: 124, training loss: 0.00035
Epoch: 8, step: 125, training loss: 0.00036
Epoch: 8, step: 126, training loss: 0.00030
Epoch: 8, step: 127, training loss: 0.00033
Epoch: 8, step: 128, training loss: 0.00043
Epoch: 8, step: 129, training loss: 0.00051
Epoch: 8, step: 130, training loss: 0.00035
Epoch: 8, step: 131, training loss: 0.00034
Epoch: 8, step: 132, training loss: 0.00035
Epoch: 8, step: 133, training loss: 0.00049
Epoch: 8, step: 134, training loss: 0.00086
Epoch: 8, step: 135, training loss: 0.00036
Epoch: 8, step: 136, training loss: 0.00058
Epoch: 8, step: 137, training loss: 0.00039
Epoch: 8, step: 138, training loss: 0.00035
Epoch: 8, step: 139, training loss: 0.00030
Epoch: 8, step: 140, training loss: 0.00048
Epoch: 8, step: 141, training loss: 0.00054
Epoch: 8, step: 142, training loss: 0.00069
Epoch: 8, step: 143, training loss: 0.00067
Epoch: 8, step: 144, training loss: 0.00045
Epoch: 8, step: 145, training loss: 0.00049
Epoch: 8, step: 146, training loss: 0.00071
Epoch: 8, step: 147, training loss: 0.00053
Epoch: 8, step: 148, training loss: 0.00038
Epoch: 8, step: 149, training loss: 0.00086
Epoch: 8, step: 150, training loss: 0.00024
Epoch: 8, step: 151, training loss: 0.00047
Epoch: 8, step: 152, training loss: 0.00049
Epoch: 8, step: 153, training loss: 0.00052
Epoch: 8, step: 154, training loss: 0.00059
Epoch: 8, step: 155, training loss: 0.00050
Epoch: 8, step: 156, training loss: 0.00039
Epoch: 8, step: 157, training loss: 0.00027
Epoch: 8, step: 158, training loss: 0.00035
Epoch: 8, step: 159, training loss: 0.00039
Epoch: 8, step: 160, training loss: 0.00051
Epoch: 8, step: 161, training loss: 0.00055
Epoch: 8, step: 162, training loss: 0.00061
Epoch: 8, step: 163, training loss: 0.00089
Epoch: 8, step: 164, training loss: 0.00039
Epoch: 8, step: 165, training loss: 0.00055
Epoch: 8, step: 166, training loss: 0.00032
Epoch: 8, step: 167, training loss: 0.00054
Epoch: 8, step: 168, training loss: 0.00038
Epoch: 8, step: 169, training loss: 0.00066
Epoch: 8, step: 170, training loss: 0.00046
Epoch: 8, step: 171, training loss: 0.00027
Epoch: 8, step: 172, training loss: 0.00047
Epoch: 8, step: 173, training loss: 0.00043
Epoch: 8, step: 174, training loss: 0.00029
Epoch: 8, step: 175, training loss: 0.00066
Epoch: 8, step: 176, training loss: 0.00068
Epoch: 8, step: 177, training loss: 0.00064
Epoch: 8, step: 178, training loss: 0.00072
Epoch: 8, step: 179, training loss: 0.00022
Epoch: 8, step: 180, training loss: 0.00063
Epoch: 8, step: 181, training loss: 0.00030
Epoch: 8, step: 182, training loss: 0.00043
Epoch: 8, step: 183, training loss: 0.00056
Epoch: 8, step: 184, training loss: 0.00048
Epoch: 8, step: 185, training loss: 0.00071
Epoch: 8, step: 186, training loss: 0.00059
Epoch: 8, step: 187, training loss: 0.00050
Epoch: 8, step: 188, training loss: 0.00036
Epoch: 8, step: 189, training loss: 0.00051
Epoch: 8, step: 190, training loss: 0.00039
Epoch: 8, step: 191, training loss: 0.00025
Epoch: 8, step: 192, training loss: 0.00033
Epoch: 8, step: 193, training loss: 0.00055
Epoch: 8, step: 194, training loss: 0.00041
Epoch: 8, step: 195, training loss: 0.00074
Epoch: 8, step: 196, training loss: 0.00037
Epoch: 8, step: 197, training loss: 0.00078
Epoch: 8, step: 198, training loss: 0.00030
Epoch: 8, step: 199, training loss: 0.00048
Epoch: 8, step: 200, training loss: 0.00095
Epoch: 8, step: 201, training loss: 0.00032
Epoch: 8, step: 202, training loss: 0.00036
Epoch: 8, step: 203, training loss: 0.00073
Epoch: 8, step: 204, training loss: 0.00041
Epoch: 8, step: 205, training loss: 0.00029
Epoch: 8, step: 206, training loss: 0.00055
Epoch: 8, step: 207, training loss: 0.00044
Epoch: 8, step: 208, training loss: 0.00032
Epoch: 8, step: 209, training loss: 0.00038
Epoch: 8, step: 210, training loss: 0.00033
Epoch: 8, step: 211, training loss: 0.00020
Epoch: 8, step: 212, training loss: 0.00040
Epoch: 8, step: 213, training loss: 0.00052
Epoch: 8, step: 214, training loss: 0.00054
Epoch: 8, step: 215, training loss: 0.00062
Epoch: 8, step: 216, training loss: 0.00042
Epoch: 8, step: 217, training loss: 0.00063
Epoch: 8, step: 218, training loss: 0.00064
Epoch: 8, step: 219, training loss: 0.00110
Epoch: 8, step: 220, training loss: 0.00042
Epoch: 8, step: 221, training loss: 0.00029
Epoch: 8, step: 222, training loss: 0.00046
Epoch: 8, step: 223, training loss: 0.00060
Epoch: 8, step: 224, training loss: 0.00064
Epoch: 8, step: 225, training loss: 0.00041
Epoch: 8, step: 226, training loss: 0.00044
Epoch: 8, step: 227, training loss: 0.00031
Epoch: 8, step: 228, training loss: 0.00046
Epoch: 8, step: 229, training loss: 0.00043
Epoch: 8, step: 230, training loss: 0.00031
Epoch: 8, step: 231, training loss: 0.00041
Epoch: 8, step: 232, training loss: 0.00052
Epoch: 8, step: 233, training loss: 0.00054
Epoch: 8, step: 234, training loss: 0.00067
Epoch: 8, step: 235, training loss: 0.00054
Epoch: 8, step: 236, training loss: 0.00040
Epoch: 8, step: 237, training loss: 0.00044
Epoch: 8, step: 238, training loss: 0.00029
Epoch: 8, average training loss: 0.00049
Epoch: 8, F1: 83.94517, average dev loss: 0.00286
Epoch: 9, step: 0, training loss: 0.00057
Epoch: 9, step: 1, training loss: 0.00060
Epoch: 9, step: 2, training loss: 0.00041
Epoch: 9, step: 3, training loss: 0.00026
Epoch: 9, step: 4, training loss: 0.00049
Epoch: 9, step: 5, training loss: 0.00042
Epoch: 9, step: 6, training loss: 0.00060
Epoch: 9, step: 7, training loss: 0.00055
Epoch: 9, step: 8, training loss: 0.00044
Epoch: 9, step: 9, training loss: 0.00044
Epoch: 9, step: 10, training loss: 0.00039
Epoch: 9, step: 11, training loss: 0.00048
Epoch: 9, step: 12, training loss: 0.00064
Epoch: 9, step: 13, training loss: 0.00048
Epoch: 9, step: 14, training loss: 0.00044
Epoch: 9, step: 15, training loss: 0.00042
Epoch: 9, step: 16, training loss: 0.00040
Epoch: 9, step: 17, training loss: 0.00068
Epoch: 9, step: 18, training loss: 0.00041
Epoch: 9, step: 19, training loss: 0.00035
Epoch: 9, step: 20, training loss: 0.00040
Epoch: 9, step: 21, training loss: 0.00054
Epoch: 9, step: 22, training loss: 0.00030
Epoch: 9, step: 23, training loss: 0.00051
Epoch: 9, step: 24, training loss: 0.00048
Epoch: 9, step: 25, training loss: 0.00039
Epoch: 9, step: 26, training loss: 0.00045
Epoch: 9, step: 27, training loss: 0.00053
Epoch: 9, step: 28, training loss: 0.00023
Epoch: 9, step: 29, training loss: 0.00038
Epoch: 9, step: 30, training loss: 0.00043
Epoch: 9, step: 31, training loss: 0.00045
Epoch: 9, step: 32, training loss: 0.00042
Epoch: 9, step: 33, training loss: 0.00046
Epoch: 9, step: 34, training loss: 0.00059
Epoch: 9, step: 35, training loss: 0.00049
Epoch: 9, step: 36, training loss: 0.00066
Epoch: 9, step: 37, training loss: 0.00066
Epoch: 9, step: 38, training loss: 0.00033
Epoch: 9, step: 39, training loss: 0.00024
Epoch: 9, step: 40, training loss: 0.00059
Epoch: 9, step: 41, training loss: 0.00033
Epoch: 9, step: 42, training loss: 0.00090
Epoch: 9, step: 43, training loss: 0.00086
Epoch: 9, step: 44, training loss: 0.00032
Epoch: 9, step: 45, training loss: 0.00018
Epoch: 9, step: 46, training loss: 0.00079
Epoch: 9, step: 47, training loss: 0.00047
Epoch: 9, step: 48, training loss: 0.00036
Epoch: 9, step: 49, training loss: 0.00046
Epoch: 9, step: 50, training loss: 0.00055
Epoch: 9, step: 51, training loss: 0.00037
Epoch: 9, step: 52, training loss: 0.00064
Epoch: 9, step: 53, training loss: 0.00063
Epoch: 9, step: 54, training loss: 0.00029
Epoch: 9, step: 55, training loss: 0.00038
Epoch: 9, step: 56, training loss: 0.00022
Epoch: 9, step: 57, training loss: 0.00055
Epoch: 9, step: 58, training loss: 0.00025
Epoch: 9, step: 59, training loss: 0.00058
Epoch: 9, step: 60, training loss: 0.00060
Epoch: 9, step: 61, training loss: 0.00063
Epoch: 9, step: 62, training loss: 0.00061
Epoch: 9, step: 63, training loss: 0.00039
Epoch: 9, step: 64, training loss: 0.00028
Epoch: 9, step: 65, training loss: 0.00038
Epoch: 9, step: 66, training loss: 0.00042
Epoch: 9, step: 67, training loss: 0.00055
Epoch: 9, step: 68, training loss: 0.00063
Epoch: 9, step: 69, training loss: 0.00034
Epoch: 9, step: 70, training loss: 0.00051
Epoch: 9, step: 71, training loss: 0.00058
Epoch: 9, step: 72, training loss: 0.00042
Epoch: 9, step: 73, training loss: 0.00046
Epoch: 9, step: 74, training loss: 0.00034
Epoch: 9, step: 75, training loss: 0.00048
Epoch: 9, step: 76, training loss: 0.00031
Epoch: 9, step: 77, training loss: 0.00030
Epoch: 9, step: 78, training loss: 0.00033
Epoch: 9, step: 79, training loss: 0.00028
Epoch: 9, step: 80, training loss: 0.00045
Epoch: 9, step: 81, training loss: 0.00036
Epoch: 9, step: 82, training loss: 0.00043
Epoch: 9, step: 83, training loss: 0.00066
Epoch: 9, step: 84, training loss: 0.00057
Epoch: 9, step: 85, training loss: 0.00037
Epoch: 9, step: 86, training loss: 0.00041
Epoch: 9, step: 87, training loss: 0.00031
Epoch: 9, step: 88, training loss: 0.00050
Epoch: 9, step: 89, training loss: 0.00027
Epoch: 9, step: 90, training loss: 0.00052
Epoch: 9, step: 91, training loss: 0.00047
Epoch: 9, step: 92, training loss: 0.00045
Epoch: 9, step: 93, training loss: 0.00031
Epoch: 9, step: 94, training loss: 0.00041
Epoch: 9, step: 95, training loss: 0.00089
Epoch: 9, step: 96, training loss: 0.00053
Epoch: 9, step: 97, training loss: 0.00055
Epoch: 9, step: 98, training loss: 0.00044
Epoch: 9, step: 99, training loss: 0.00047
Epoch: 9, step: 100, training loss: 0.00030
Epoch: 9, step: 101, training loss: 0.00024
Epoch: 9, step: 102, training loss: 0.00047
Epoch: 9, step: 103, training loss: 0.00029
Epoch: 9, step: 104, training loss: 0.00049
Epoch: 9, step: 105, training loss: 0.00036
Epoch: 9, step: 106, training loss: 0.00028
Epoch: 9, step: 107, training loss: 0.00054
Epoch: 9, step: 108, training loss: 0.00038
Epoch: 9, step: 109, training loss: 0.00055
Epoch: 9, step: 110, training loss: 0.00032
Epoch: 9, step: 111, training loss: 0.00042
Epoch: 9, step: 112, training loss: 0.00088
Epoch: 9, step: 113, training loss: 0.00024
Epoch: 9, step: 114, training loss: 0.00031
Epoch: 9, step: 115, training loss: 0.00037
Epoch: 9, step: 116, training loss: 0.00055
Epoch: 9, step: 117, training loss: 0.00041
Epoch: 9, step: 118, training loss: 0.00031
Epoch: 9, step: 119, training loss: 0.00050
Epoch: 9, step: 120, training loss: 0.00024
Epoch: 9, step: 121, training loss: 0.00061
Epoch: 9, step: 122, training loss: 0.00042
Epoch: 9, step: 123, training loss: 0.00063
Epoch: 9, step: 124, training loss: 0.00049
Epoch: 9, step: 125, training loss: 0.00033
Epoch: 9, step: 126, training loss: 0.00033
Epoch: 9, step: 127, training loss: 0.00027
Epoch: 9, step: 128, training loss: 0.00042
Epoch: 9, step: 129, training loss: 0.00037
Epoch: 9, step: 130, training loss: 0.00058
Epoch: 9, step: 131, training loss: 0.00039
Epoch: 9, step: 132, training loss: 0.00034
Epoch: 9, step: 133, training loss: 0.00029
Epoch: 9, step: 134, training loss: 0.00050
Epoch: 9, step: 135, training loss: 0.00021
Epoch: 9, step: 136, training loss: 0.00043
Epoch: 9, step: 137, training loss: 0.00043
Epoch: 9, step: 138, training loss: 0.00054
Epoch: 9, step: 139, training loss: 0.00022
Epoch: 9, step: 140, training loss: 0.00022
Epoch: 9, step: 141, training loss: 0.00045
Epoch: 9, step: 142, training loss: 0.00059
Epoch: 9, step: 143, training loss: 0.00052
Epoch: 9, step: 144, training loss: 0.00060
Epoch: 9, step: 145, training loss: 0.00040
Epoch: 9, step: 146, training loss: 0.00047
Epoch: 9, step: 147, training loss: 0.00040
Epoch: 9, step: 148, training loss: 0.00034
Epoch: 9, step: 149, training loss: 0.00059
Epoch: 9, step: 150, training loss: 0.00024
Epoch: 9, step: 151, training loss: 0.00027
Epoch: 9, step: 152, training loss: 0.00022
Epoch: 9, step: 153, training loss: 0.00050
Epoch: 9, step: 154, training loss: 0.00078
Epoch: 9, step: 155, training loss: 0.00044
Epoch: 9, step: 156, training loss: 0.00065
Epoch: 9, step: 157, training loss: 0.00037
Epoch: 9, step: 158, training loss: 0.00039
Epoch: 9, step: 159, training loss: 0.00043
Epoch: 9, step: 160, training loss: 0.00046
Epoch: 9, step: 161, training loss: 0.00019
Epoch: 9, step: 162, training loss: 0.00038
Epoch: 9, step: 163, training loss: 0.00060
Epoch: 9, step: 164, training loss: 0.00048
Epoch: 9, step: 165, training loss: 0.00020
Epoch: 9, step: 166, training loss: 0.00026
Epoch: 9, step: 167, training loss: 0.00030
Epoch: 9, step: 168, training loss: 0.00024
Epoch: 9, step: 169, training loss: 0.00061
Epoch: 9, step: 170, training loss: 0.00058
Epoch: 9, step: 171, training loss: 0.00046
Epoch: 9, step: 172, training loss: 0.00057
Epoch: 9, step: 173, training loss: 0.00038
Epoch: 9, step: 174, training loss: 0.00035
Epoch: 9, step: 175, training loss: 0.00034
Epoch: 9, step: 176, training loss: 0.00042
Epoch: 9, step: 177, training loss: 0.00013
Epoch: 9, step: 178, training loss: 0.00053
Epoch: 9, step: 179, training loss: 0.00039
Epoch: 9, step: 180, training loss: 0.00051
Epoch: 9, step: 181, training loss: 0.00041
Epoch: 9, step: 182, training loss: 0.00044
Epoch: 9, step: 183, training loss: 0.00045
Epoch: 9, step: 184, training loss: 0.00020
Epoch: 9, step: 185, training loss: 0.00046
Epoch: 9, step: 186, training loss: 0.00042
Epoch: 9, step: 187, training loss: 0.00039
Epoch: 9, step: 188, training loss: 0.00027
Epoch: 9, step: 189, training loss: 0.00052
Epoch: 9, step: 190, training loss: 0.00039
Epoch: 9, step: 191, training loss: 0.00032
Epoch: 9, step: 192, training loss: 0.00039
Epoch: 9, step: 193, training loss: 0.00068
Epoch: 9, step: 194, training loss: 0.00060
Epoch: 9, step: 195, training loss: 0.00036
Epoch: 9, step: 196, training loss: 0.00039
Epoch: 9, step: 197, training loss: 0.00059
Epoch: 9, step: 198, training loss: 0.00023
Epoch: 9, step: 199, training loss: 0.00025
Epoch: 9, step: 200, training loss: 0.00037
Epoch: 9, step: 201, training loss: 0.00024
Epoch: 9, step: 202, training loss: 0.00068
Epoch: 9, step: 203, training loss: 0.00058
Epoch: 9, step: 204, training loss: 0.00034
Epoch: 9, step: 205, training loss: 0.00050
Epoch: 9, step: 206, training loss: 0.00037
Epoch: 9, step: 207, training loss: 0.00033
Epoch: 9, step: 208, training loss: 0.00031
Epoch: 9, step: 209, training loss: 0.00053
Epoch: 9, step: 210, training loss: 0.00037
Epoch: 9, step: 211, training loss: 0.00024
Epoch: 9, step: 212, training loss: 0.00021
Epoch: 9, step: 213, training loss: 0.00031
Epoch: 9, step: 214, training loss: 0.00041
Epoch: 9, step: 215, training loss: 0.00056
Epoch: 9, step: 216, training loss: 0.00034
Epoch: 9, step: 217, training loss: 0.00030
Epoch: 9, step: 218, training loss: 0.00025
Epoch: 9, step: 219, training loss: 0.00064
Epoch: 9, step: 220, training loss: 0.00036
Epoch: 9, step: 221, training loss: 0.00050
Epoch: 9, step: 222, training loss: 0.00033
Epoch: 9, step: 223, training loss: 0.00064
Epoch: 9, step: 224, training loss: 0.00049
Epoch: 9, step: 225, training loss: 0.00035
Epoch: 9, step: 226, training loss: 0.00030
Epoch: 9, step: 227, training loss: 0.00023
Epoch: 9, step: 228, training loss: 0.00026
Epoch: 9, step: 229, training loss: 0.00082
Epoch: 9, step: 230, training loss: 0.00018
Epoch: 9, step: 231, training loss: 0.00033
Epoch: 9, step: 232, training loss: 0.00039
Epoch: 9, step: 233, training loss: 0.00047
Epoch: 9, step: 234, training loss: 0.00025
Epoch: 9, step: 235, training loss: 0.00048
Epoch: 9, step: 236, training loss: 0.00032
Epoch: 9, step: 237, training loss: 0.00056
Epoch: 9, step: 238, training loss: 0.00089
Epoch: 9, average training loss: 0.00043
Epoch: 9, F1: 84.19054, average dev loss: 0.00287
Loading best check point...
Evaluating on dev set...

DEV F1: 84.34658, avg loss: 0.00282
Evaluating on test set...

Test F1: 48.15978, avg loss: 0.02705
