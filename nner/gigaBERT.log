F1 ================== EXP =====================
F1 Target language: ar
batchsize: 8
learning rate: 0.0000200
max epochs: 7
max_seq_length: 128
num_depulicate: 64
warmup proportion: 0.40000
model ckpt will be saved at: gigabert.ckpt
model config will be saved at: gigabert.cfg
Reading file:  en/train.txt
Data size:  7634
Loading training data...

Reading file:  en/train.txt
Data size:  7634
Dataset set: train, orig size: 7634
Loading development data...

Reading file:  en/dev.txt
Data size:  1005
Dataset set: dev, orig size: 1005
Loading testing data...

Reading file:  ar/test.txt
Data size:  238
Dataset set: test, orig size: 238
Training started...
Epoch: 0, step: 0, training loss: 0.10726
Epoch: 0, step: 1, training loss: 0.09366
Epoch: 0, step: 2, training loss: 0.11298
Epoch: 0, step: 3, training loss: 0.06519
Epoch: 0, step: 4, training loss: 0.12989
Epoch: 0, step: 5, training loss: 0.04709
Epoch: 0, step: 6, training loss: 0.08030
Epoch: 0, step: 7, training loss: 0.07823
Epoch: 0, step: 8, training loss: 0.13454
Epoch: 0, step: 9, training loss: 0.11599
Epoch: 0, step: 10, training loss: 0.15783
Epoch: 0, step: 11, training loss: 0.10718
Epoch: 0, step: 12, training loss: 0.09003
Epoch: 0, step: 13, training loss: 0.06911
Epoch: 0, step: 14, training loss: 0.13012
Epoch: 0, step: 15, training loss: 0.07293
Epoch: 0, step: 16, training loss: 0.11878
Epoch: 0, step: 17, training loss: 0.07210
Epoch: 0, step: 18, training loss: 0.13495
Epoch: 0, step: 19, training loss: 0.12015
Epoch: 0, step: 20, training loss: 0.06590
Epoch: 0, step: 21, training loss: 0.08457
Epoch: 0, step: 22, training loss: 0.08716
Epoch: 0, step: 23, training loss: 0.13632
Epoch: 0, step: 24, training loss: 0.13075
Epoch: 0, step: 25, training loss: 0.12647
Epoch: 0, step: 26, training loss: 0.09734
Epoch: 0, step: 27, training loss: 0.12026
Epoch: 0, step: 28, training loss: 0.12918
Epoch: 0, step: 29, training loss: 0.10496
Epoch: 0, step: 30, training loss: 0.11138
Epoch: 0, step: 31, training loss: 0.12207
Epoch: 0, step: 32, training loss: 0.08366
Epoch: 0, step: 33, training loss: 0.09163
Epoch: 0, step: 34, training loss: 0.05539
Epoch: 0, step: 35, training loss: 0.09126
Epoch: 0, step: 36, training loss: 0.08764
Epoch: 0, step: 37, training loss: 0.10713
Epoch: 0, step: 38, training loss: 0.12800
Epoch: 0, step: 39, training loss: 0.10161
Epoch: 0, step: 40, training loss: 0.14302
Epoch: 0, step: 41, training loss: 0.09828
Epoch: 0, step: 42, training loss: 0.11365
Epoch: 0, step: 43, training loss: 0.09600
Epoch: 0, step: 44, training loss: 0.11565
Epoch: 0, step: 45, training loss: 0.13919
Epoch: 0, step: 46, training loss: 0.11158
Epoch: 0, step: 47, training loss: 0.07841
Epoch: 0, step: 48, training loss: 0.11977
Epoch: 0, step: 49, training loss: 0.09144
Epoch: 0, step: 50, training loss: 0.10160
Epoch: 0, step: 51, training loss: 0.12976
Epoch: 0, step: 52, training loss: 0.10776
Epoch: 0, step: 53, training loss: 0.17976
Epoch: 0, step: 54, training loss: 0.09402
Epoch: 0, step: 55, training loss: 0.13962
Epoch: 0, step: 56, training loss: 0.08816
Epoch: 0, step: 57, training loss: 0.11020
Epoch: 0, step: 58, training loss: 0.08375
Epoch: 0, step: 59, training loss: 0.14671
Epoch: 0, step: 60, training loss: 0.14339
Epoch: 0, step: 61, training loss: 0.12779
Epoch: 0, step: 62, training loss: 0.09708
Epoch: 0, step: 63, training loss: 0.08302
Epoch: 0, step: 64, training loss: 0.07322
Epoch: 0, step: 65, training loss: 0.09408
Epoch: 0, step: 66, training loss: 0.09481
Epoch: 0, step: 67, training loss: 0.09789
Epoch: 0, step: 68, training loss: 0.07645
Epoch: 0, step: 69, training loss: 0.11346
Epoch: 0, step: 70, training loss: 0.09250
Epoch: 0, step: 71, training loss: 0.10228
Epoch: 0, step: 72, training loss: 0.09815
Epoch: 0, step: 73, training loss: 0.13021
Epoch: 0, step: 74, training loss: 0.07556
Epoch: 0, step: 75, training loss: 0.06671
Epoch: 0, step: 76, training loss: 0.05422
Epoch: 0, step: 77, training loss: 0.07546
Epoch: 0, step: 78, training loss: 0.09299
Epoch: 0, step: 79, training loss: 0.12876
Epoch: 0, step: 80, training loss: 0.08196
Epoch: 0, step: 81, training loss: 0.05498
Epoch: 0, step: 82, training loss: 0.07514
Epoch: 0, step: 83, training loss: 0.11056
Epoch: 0, step: 84, training loss: 0.07512
Epoch: 0, step: 85, training loss: 0.08468
Epoch: 0, step: 86, training loss: 0.12463
Epoch: 0, step: 87, training loss: 0.08006
Epoch: 0, step: 88, training loss: 0.07417
Epoch: 0, step: 89, training loss: 0.11230
Epoch: 0, step: 90, training loss: 0.11468
Epoch: 0, step: 91, training loss: 0.09628
Epoch: 0, step: 92, training loss: 0.09160
Epoch: 0, step: 93, training loss: 0.08008
Epoch: 0, step: 94, training loss: 0.05687
Epoch: 0, step: 95, training loss: 0.13080
Epoch: 0, step: 96, training loss: 0.11304
Epoch: 0, step: 97, training loss: 0.10563
Epoch: 0, step: 98, training loss: 0.09155
Epoch: 0, step: 99, training loss: 0.08629
Epoch: 0, step: 100, training loss: 0.07861
Epoch: 0, step: 101, training loss: 0.12249
Epoch: 0, step: 102, training loss: 0.07874
Epoch: 0, step: 103, training loss: 0.09259
Epoch: 0, step: 104, training loss: 0.11193
Epoch: 0, step: 105, training loss: 0.07249
Epoch: 0, step: 106, training loss: 0.08282
Epoch: 0, step: 107, training loss: 0.09802
Epoch: 0, step: 108, training loss: 0.06090
Epoch: 0, step: 109, training loss: 0.09880
Epoch: 0, step: 110, training loss: 0.08749
Epoch: 0, step: 111, training loss: 0.06617
Epoch: 0, step: 112, training loss: 0.09191
Epoch: 0, step: 113, training loss: 0.08266
Epoch: 0, step: 114, training loss: 0.05403
Epoch: 0, step: 115, training loss: 0.07047
Epoch: 0, step: 116, training loss: 0.09203
Epoch: 0, step: 117, training loss: 0.09121
Epoch: 0, step: 118, training loss: 0.11989
Epoch: 0, step: 119, training loss: 0.08048
Epoch: 0, step: 120, training loss: 0.10107
Epoch: 0, step: 121, training loss: 0.07838
Epoch: 0, step: 122, training loss: 0.10468
Epoch: 0, step: 123, training loss: 0.08625
Epoch: 0, step: 124, training loss: 0.07629
Epoch: 0, step: 125, training loss: 0.09860
Epoch: 0, step: 126, training loss: 0.07882
Epoch: 0, step: 127, training loss: 0.05788
Epoch: 0, step: 128, training loss: 0.05863
Epoch: 0, step: 129, training loss: 0.05908
Epoch: 0, step: 130, training loss: 0.06170
Epoch: 0, step: 131, training loss: 0.11098
Epoch: 0, step: 132, training loss: 0.08565
Epoch: 0, step: 133, training loss: 0.09803
Epoch: 0, step: 134, training loss: 0.07747
Epoch: 0, step: 135, training loss: 0.08898
Epoch: 0, step: 136, training loss: 0.07065
Epoch: 0, step: 137, training loss: 0.04321
Epoch: 0, step: 138, training loss: 0.04621
Epoch: 0, step: 139, training loss: 0.04995
Epoch: 0, step: 140, training loss: 0.06014
Epoch: 0, step: 141, training loss: 0.05893
Epoch: 0, step: 142, training loss: 0.06221
Epoch: 0, step: 143, training loss: 0.05751
Epoch: 0, step: 144, training loss: 0.06237
Epoch: 0, step: 145, training loss: 0.06834
Epoch: 0, step: 146, training loss: 0.05904
Epoch: 0, step: 147, training loss: 0.06444
Epoch: 0, step: 148, training loss: 0.04791
Epoch: 0, step: 149, training loss: 0.06209
Epoch: 0, step: 150, training loss: 0.05793
Epoch: 0, step: 151, training loss: 0.08592
Epoch: 0, step: 152, training loss: 0.04772
Epoch: 0, step: 153, training loss: 0.06804
Epoch: 0, step: 154, training loss: 0.06146
Epoch: 0, step: 155, training loss: 0.06902
Epoch: 0, step: 156, training loss: 0.05915
Epoch: 0, step: 157, training loss: 0.05484
Epoch: 0, step: 158, training loss: 0.06923
Epoch: 0, step: 159, training loss: 0.04493
Epoch: 0, step: 160, training loss: 0.05925
Epoch: 0, step: 161, training loss: 0.07074
Epoch: 0, step: 162, training loss: 0.06630
Epoch: 0, step: 163, training loss: 0.05457
Epoch: 0, step: 164, training loss: 0.04760
Epoch: 0, step: 165, training loss: 0.04411
Epoch: 0, step: 166, training loss: 0.06270
Epoch: 0, step: 167, training loss: 0.04284
Epoch: 0, step: 168, training loss: 0.03548
Epoch: 0, step: 169, training loss: 0.04708
Epoch: 0, step: 170, training loss: 0.03477
Epoch: 0, step: 171, training loss: 0.05890
Epoch: 0, step: 172, training loss: 0.05027
Epoch: 0, step: 173, training loss: 0.04508
Epoch: 0, step: 174, training loss: 0.04814
Epoch: 0, step: 175, training loss: 0.05372
Epoch: 0, step: 176, training loss: 0.03760
Epoch: 0, step: 177, training loss: 0.05518
Epoch: 0, step: 178, training loss: 0.02234
Epoch: 0, step: 179, training loss: 0.05718
Epoch: 0, step: 180, training loss: 0.04423
Epoch: 0, step: 181, training loss: 0.02903
Epoch: 0, step: 182, training loss: 0.04561
Epoch: 0, step: 183, training loss: 0.02374
Epoch: 0, step: 184, training loss: 0.04112
Epoch: 0, step: 185, training loss: 0.03682
Epoch: 0, step: 186, training loss: 0.04413
Epoch: 0, step: 187, training loss: 0.02550
Epoch: 0, step: 188, training loss: 0.02708
Epoch: 0, step: 189, training loss: 0.04874
Epoch: 0, step: 190, training loss: 0.03311
Epoch: 0, step: 191, training loss: 0.02901
Epoch: 0, step: 192, training loss: 0.03428
Epoch: 0, step: 193, training loss: 0.03342
Epoch: 0, step: 194, training loss: 0.03199
Epoch: 0, step: 195, training loss: 0.03554
Epoch: 0, step: 196, training loss: 0.03139
Epoch: 0, step: 197, training loss: 0.02693
Epoch: 0, step: 198, training loss: 0.03018
Epoch: 0, step: 199, training loss: 0.04274
Epoch: 0, step: 200, training loss: 0.02920
Epoch: 0, step: 201, training loss: 0.05036
Epoch: 0, step: 202, training loss: 0.03586
Epoch: 0, step: 203, training loss: 0.04274
Epoch: 0, step: 204, training loss: 0.02420
Epoch: 0, step: 205, training loss: 0.04871
Epoch: 0, step: 206, training loss: 0.03306
Epoch: 0, step: 207, training loss: 0.02921
Epoch: 0, step: 208, training loss: 0.02860
Epoch: 0, step: 209, training loss: 0.03174
Epoch: 0, step: 210, training loss: 0.01673
Epoch: 0, step: 211, training loss: 0.03198
Epoch: 0, step: 212, training loss: 0.03043
Epoch: 0, step: 213, training loss: 0.03027
Epoch: 0, step: 214, training loss: 0.02576
Epoch: 0, step: 215, training loss: 0.04770
Epoch: 0, step: 216, training loss: 0.02020
Epoch: 0, step: 217, training loss: 0.03435
Epoch: 0, step: 218, training loss: 0.03487
Epoch: 0, step: 219, training loss: 0.03720
Epoch: 0, step: 220, training loss: 0.03949
Epoch: 0, step: 221, training loss: 0.02770
Epoch: 0, step: 222, training loss: 0.01682
Epoch: 0, step: 223, training loss: 0.02461
Epoch: 0, step: 224, training loss: 0.02918
Epoch: 0, step: 225, training loss: 0.02478
Epoch: 0, step: 226, training loss: 0.03166
Epoch: 0, step: 227, training loss: 0.03205
Epoch: 0, step: 228, training loss: 0.02570
Epoch: 0, step: 229, training loss: 0.02467
Epoch: 0, step: 230, training loss: 0.01880
Epoch: 0, step: 231, training loss: 0.02685
Epoch: 0, step: 232, training loss: 0.02608
Epoch: 0, step: 233, training loss: 0.01953
Epoch: 0, step: 234, training loss: 0.02755
Epoch: 0, step: 235, training loss: 0.03423
Epoch: 0, step: 236, training loss: 0.03101
Epoch: 0, step: 237, training loss: 0.03149
Epoch: 0, step: 238, training loss: 0.03153
Epoch: 0, step: 239, training loss: 0.02836
Epoch: 0, step: 240, training loss: 0.02616
Epoch: 0, step: 241, training loss: 0.02049
Epoch: 0, step: 242, training loss: 0.03157
Epoch: 0, step: 243, training loss: 0.01771
Epoch: 0, step: 244, training loss: 0.02582
Epoch: 0, step: 245, training loss: 0.03189
Epoch: 0, step: 246, training loss: 0.02271
Epoch: 0, step: 247, training loss: 0.03277
Epoch: 0, step: 248, training loss: 0.03700
Epoch: 0, step: 249, training loss: 0.02184
Epoch: 0, step: 250, training loss: 0.01677
Epoch: 0, step: 251, training loss: 0.02289
Epoch: 0, step: 252, training loss: 0.02823
Epoch: 0, step: 253, training loss: 0.02207
Epoch: 0, step: 254, training loss: 0.01961
Epoch: 0, step: 255, training loss: 0.01677
Epoch: 0, step: 256, training loss: 0.01744
Epoch: 0, step: 257, training loss: 0.02367
Epoch: 0, step: 258, training loss: 0.03219
Epoch: 0, step: 259, training loss: 0.02986
Epoch: 0, step: 260, training loss: 0.02364
Epoch: 0, step: 261, training loss: 0.02382
Epoch: 0, step: 262, training loss: 0.01675
Epoch: 0, step: 263, training loss: 0.02354
Epoch: 0, step: 264, training loss: 0.02134
Epoch: 0, step: 265, training loss: 0.01741
Epoch: 0, step: 266, training loss: 0.02940
Epoch: 0, step: 267, training loss: 0.02505
Epoch: 0, step: 268, training loss: 0.01337
Epoch: 0, step: 269, training loss: 0.02594
Epoch: 0, step: 270, training loss: 0.02247
Epoch: 0, step: 271, training loss: 0.01916
Epoch: 0, step: 272, training loss: 0.02278
Epoch: 0, step: 273, training loss: 0.02421
Epoch: 0, step: 274, training loss: 0.01854
Epoch: 0, step: 275, training loss: 0.02537
Epoch: 0, step: 276, training loss: 0.02810
Epoch: 0, step: 277, training loss: 0.01805
Epoch: 0, step: 278, training loss: 0.02525
Epoch: 0, step: 279, training loss: 0.02119
Epoch: 0, step: 280, training loss: 0.03401
Epoch: 0, step: 281, training loss: 0.02988
Epoch: 0, step: 282, training loss: 0.01984
Epoch: 0, step: 283, training loss: 0.01628
Epoch: 0, step: 284, training loss: 0.02714
Epoch: 0, step: 285, training loss: 0.02795
Epoch: 0, step: 286, training loss: 0.01482
Epoch: 0, step: 287, training loss: 0.01909
Epoch: 0, step: 288, training loss: 0.02619
Epoch: 0, step: 289, training loss: 0.02386
Epoch: 0, step: 290, training loss: 0.02196
Epoch: 0, step: 291, training loss: 0.02869
Epoch: 0, step: 292, training loss: 0.01128
Epoch: 0, step: 293, training loss: 0.01279
Epoch: 0, step: 294, training loss: 0.01830
Epoch: 0, step: 295, training loss: 0.03787
Epoch: 0, step: 296, training loss: 0.03018
Epoch: 0, step: 297, training loss: 0.01510
Epoch: 0, step: 298, training loss: 0.02234
Epoch: 0, step: 299, training loss: 0.02160
Epoch: 0, step: 300, training loss: 0.01836
Epoch: 0, step: 301, training loss: 0.02339
Epoch: 0, step: 302, training loss: 0.01542
Epoch: 0, step: 303, training loss: 0.03279
Epoch: 0, step: 304, training loss: 0.02881
Epoch: 0, step: 305, training loss: 0.02261
Epoch: 0, step: 306, training loss: 0.02240
Epoch: 0, step: 307, training loss: 0.01444
Epoch: 0, step: 308, training loss: 0.01634
Epoch: 0, step: 309, training loss: 0.02553
Epoch: 0, step: 310, training loss: 0.01981
Epoch: 0, step: 311, training loss: 0.02616
Epoch: 0, step: 312, training loss: 0.02959
Epoch: 0, step: 313, training loss: 0.01583
Epoch: 0, step: 314, training loss: 0.01891
Epoch: 0, step: 315, training loss: 0.01678
Epoch: 0, step: 316, training loss: 0.02026
Epoch: 0, step: 317, training loss: 0.02976
Epoch: 0, step: 318, training loss: 0.01578
Epoch: 0, step: 319, training loss: 0.02437
Epoch: 0, step: 320, training loss: 0.01788
Epoch: 0, step: 321, training loss: 0.02585
Epoch: 0, step: 322, training loss: 0.02173
Epoch: 0, step: 323, training loss: 0.01704
Epoch: 0, step: 324, training loss: 0.01696
Epoch: 0, step: 325, training loss: 0.02058
Epoch: 0, step: 326, training loss: 0.02001
Epoch: 0, step: 327, training loss: 0.02257
Epoch: 0, step: 328, training loss: 0.02633
Epoch: 0, step: 329, training loss: 0.01758
Epoch: 0, step: 330, training loss: 0.03312
Epoch: 0, step: 331, training loss: 0.02332
Epoch: 0, step: 332, training loss: 0.03041
Epoch: 0, step: 333, training loss: 0.02332
Epoch: 0, step: 334, training loss: 0.01952
Epoch: 0, step: 335, training loss: 0.01580
Epoch: 0, step: 336, training loss: 0.03148
Epoch: 0, step: 337, training loss: 0.01480
Epoch: 0, step: 338, training loss: 0.01678
Epoch: 0, step: 339, training loss: 0.02607
Epoch: 0, step: 340, training loss: 0.01016
Epoch: 0, step: 341, training loss: 0.01591
Epoch: 0, step: 342, training loss: 0.02326
Epoch: 0, step: 343, training loss: 0.02259
Epoch: 0, step: 344, training loss: 0.02525
Epoch: 0, step: 345, training loss: 0.01814
Epoch: 0, step: 346, training loss: 0.01767
Epoch: 0, step: 347, training loss: 0.02460
Epoch: 0, step: 348, training loss: 0.01734
Epoch: 0, step: 349, training loss: 0.03419
Epoch: 0, step: 350, training loss: 0.02111
Epoch: 0, step: 351, training loss: 0.02715
Epoch: 0, step: 352, training loss: 0.02597
Epoch: 0, step: 353, training loss: 0.02006
Epoch: 0, step: 354, training loss: 0.03559
Epoch: 0, step: 355, training loss: 0.01846
Epoch: 0, step: 356, training loss: 0.02039
Epoch: 0, step: 357, training loss: 0.01248
Epoch: 0, step: 358, training loss: 0.02165
Epoch: 0, step: 359, training loss: 0.01392
Epoch: 0, step: 360, training loss: 0.02193
Epoch: 0, step: 361, training loss: 0.03649
Epoch: 0, step: 362, training loss: 0.02225
Epoch: 0, step: 363, training loss: 0.01913
Epoch: 0, step: 364, training loss: 0.02804
Epoch: 0, step: 365, training loss: 0.02678
Epoch: 0, step: 366, training loss: 0.01984
Epoch: 0, step: 367, training loss: 0.02296
Epoch: 0, step: 368, training loss: 0.02843
Epoch: 0, step: 369, training loss: 0.01522
Epoch: 0, step: 370, training loss: 0.02092
Epoch: 0, step: 371, training loss: 0.01371
Epoch: 0, step: 372, training loss: 0.01084
Epoch: 0, step: 373, training loss: 0.02856
Epoch: 0, step: 374, training loss: 0.01781
Epoch: 0, step: 375, training loss: 0.01873
Epoch: 0, step: 376, training loss: 0.01679
Epoch: 0, step: 377, training loss: 0.02107
Epoch: 0, step: 378, training loss: 0.02720
Epoch: 0, step: 379, training loss: 0.01639
Epoch: 0, step: 380, training loss: 0.03068
Epoch: 0, step: 381, training loss: 0.02464
Epoch: 0, step: 382, training loss: 0.01709
Epoch: 0, step: 383, training loss: 0.01991
Epoch: 0, step: 384, training loss: 0.03456
Epoch: 0, step: 385, training loss: 0.01954
Epoch: 0, step: 386, training loss: 0.02530
Epoch: 0, step: 387, training loss: 0.01813
Epoch: 0, step: 388, training loss: 0.01244
Epoch: 0, step: 389, training loss: 0.01251
Epoch: 0, step: 390, training loss: 0.02358
Epoch: 0, step: 391, training loss: 0.01079
Epoch: 0, step: 392, training loss: 0.01494
Epoch: 0, step: 393, training loss: 0.02153
Epoch: 0, step: 394, training loss: 0.01963
Epoch: 0, step: 395, training loss: 0.01670
Epoch: 0, step: 396, training loss: 0.01882
Epoch: 0, step: 397, training loss: 0.02369
Epoch: 0, step: 398, training loss: 0.01337
Epoch: 0, step: 399, training loss: 0.01503
Epoch: 0, step: 400, training loss: 0.02837
Epoch: 0, step: 401, training loss: 0.01927
Epoch: 0, step: 402, training loss: 0.01621
Epoch: 0, step: 403, training loss: 0.02689
Epoch: 0, step: 404, training loss: 0.02317
Epoch: 0, step: 405, training loss: 0.02097
Epoch: 0, step: 406, training loss: 0.02300
Epoch: 0, step: 407, training loss: 0.01882
Epoch: 0, step: 408, training loss: 0.01772
Epoch: 0, step: 409, training loss: 0.02057
Epoch: 0, step: 410, training loss: 0.02011
Epoch: 0, step: 411, training loss: 0.01659
Epoch: 0, step: 412, training loss: 0.01966
Epoch: 0, step: 413, training loss: 0.02265
Epoch: 0, step: 414, training loss: 0.01592
Epoch: 0, step: 415, training loss: 0.03204
Epoch: 0, step: 416, training loss: 0.01804
Epoch: 0, step: 417, training loss: 0.01505
Epoch: 0, step: 418, training loss: 0.01451
Epoch: 0, step: 419, training loss: 0.01608
Epoch: 0, step: 420, training loss: 0.01720
Epoch: 0, step: 421, training loss: 0.02606
Epoch: 0, step: 422, training loss: 0.02209
Epoch: 0, step: 423, training loss: 0.02208
Epoch: 0, step: 424, training loss: 0.01699
Epoch: 0, step: 425, training loss: 0.02882
Epoch: 0, step: 426, training loss: 0.01673
Epoch: 0, step: 427, training loss: 0.01453
Epoch: 0, step: 428, training loss: 0.01996
Epoch: 0, step: 429, training loss: 0.02585
Epoch: 0, step: 430, training loss: 0.02442
Epoch: 0, step: 431, training loss: 0.01400
Epoch: 0, step: 432, training loss: 0.02808
Epoch: 0, step: 433, training loss: 0.01232
Epoch: 0, step: 434, training loss: 0.01093
Epoch: 0, step: 435, training loss: 0.02029
Epoch: 0, step: 436, training loss: 0.01445
Epoch: 0, step: 437, training loss: 0.02196
Epoch: 0, step: 438, training loss: 0.02009
Epoch: 0, step: 439, training loss: 0.02143
Epoch: 0, step: 440, training loss: 0.01378
Epoch: 0, step: 441, training loss: 0.01973
Epoch: 0, step: 442, training loss: 0.02005
Epoch: 0, step: 443, training loss: 0.01616
Epoch: 0, step: 444, training loss: 0.01311
Epoch: 0, step: 445, training loss: 0.02917
Epoch: 0, step: 446, training loss: 0.01866
Epoch: 0, step: 447, training loss: 0.02228
Epoch: 0, step: 448, training loss: 0.02608
Epoch: 0, step: 449, training loss: 0.02087
Epoch: 0, step: 450, training loss: 0.01749
Epoch: 0, step: 451, training loss: 0.01675
Epoch: 0, step: 452, training loss: 0.02815
Epoch: 0, step: 453, training loss: 0.03695
Epoch: 0, step: 454, training loss: 0.01700
Epoch: 0, step: 455, training loss: 0.01592
Epoch: 0, step: 456, training loss: 0.02035
Epoch: 0, step: 457, training loss: 0.01939
Epoch: 0, step: 458, training loss: 0.01891
Epoch: 0, step: 459, training loss: 0.01593
Epoch: 0, step: 460, training loss: 0.01865
Epoch: 0, step: 461, training loss: 0.02113
Epoch: 0, step: 462, training loss: 0.01416
Epoch: 0, step: 463, training loss: 0.01497
Epoch: 0, step: 464, training loss: 0.01317
Epoch: 0, step: 465, training loss: 0.01823
Epoch: 0, step: 466, training loss: 0.01738
Epoch: 0, step: 467, training loss: 0.01538
Epoch: 0, step: 468, training loss: 0.02782
Epoch: 0, step: 469, training loss: 0.01708
Epoch: 0, step: 470, training loss: 0.01065
Epoch: 0, step: 471, training loss: 0.01648
Epoch: 0, step: 472, training loss: 0.02208
Epoch: 0, step: 473, training loss: 0.01867
Epoch: 0, step: 474, training loss: 0.01345
Epoch: 0, step: 475, training loss: 0.01764
Epoch: 0, step: 476, training loss: 0.01384
Epoch: 0, step: 477, training loss: 0.02186
Epoch: 0, step: 478, training loss: 0.01597
Epoch: 0, step: 479, training loss: 0.02672
Epoch: 0, step: 480, training loss: 0.02367
Epoch: 0, step: 481, training loss: 0.02152
Epoch: 0, step: 482, training loss: 0.02052
Epoch: 0, step: 483, training loss: 0.02994
Epoch: 0, step: 484, training loss: 0.02208
Epoch: 0, step: 485, training loss: 0.01387
Epoch: 0, step: 486, training loss: 0.02227
Epoch: 0, step: 487, training loss: 0.02784
Epoch: 0, step: 488, training loss: 0.01833
Epoch: 0, step: 489, training loss: 0.01879
Epoch: 0, step: 490, training loss: 0.01302
Epoch: 0, step: 491, training loss: 0.01538
Epoch: 0, step: 492, training loss: 0.02245
Epoch: 0, step: 493, training loss: 0.03220
Epoch: 0, step: 494, training loss: 0.01482
Epoch: 0, step: 495, training loss: 0.02282
Epoch: 0, step: 496, training loss: 0.02528
Epoch: 0, step: 497, training loss: 0.01499
Epoch: 0, step: 498, training loss: 0.01686
Epoch: 0, step: 499, training loss: 0.01923
Epoch: 0, step: 500, training loss: 0.02326
Epoch: 0, step: 501, training loss: 0.02144
Epoch: 0, step: 502, training loss: 0.01849
Epoch: 0, step: 503, training loss: 0.02504
Epoch: 0, step: 504, training loss: 0.01553
Epoch: 0, step: 505, training loss: 0.01843
Epoch: 0, step: 506, training loss: 0.02838
Epoch: 0, step: 507, training loss: 0.02727
Epoch: 0, step: 508, training loss: 0.02593
Epoch: 0, step: 509, training loss: 0.01513
Epoch: 0, step: 510, training loss: 0.01722
Epoch: 0, step: 511, training loss: 0.03406
Epoch: 0, step: 512, training loss: 0.02476
Epoch: 0, step: 513, training loss: 0.02279
Epoch: 0, step: 514, training loss: 0.02285
Epoch: 0, step: 515, training loss: 0.02701
Epoch: 0, step: 516, training loss: 0.01738
Epoch: 0, step: 517, training loss: 0.02141
Epoch: 0, step: 518, training loss: 0.02109
Epoch: 0, step: 519, training loss: 0.02051
Epoch: 0, step: 520, training loss: 0.01939
Epoch: 0, step: 521, training loss: 0.01957
Epoch: 0, step: 522, training loss: 0.01485
Epoch: 0, step: 523, training loss: 0.02169
Epoch: 0, step: 524, training loss: 0.02638
Epoch: 0, step: 525, training loss: 0.01754
Epoch: 0, step: 526, training loss: 0.02225
Epoch: 0, step: 527, training loss: 0.01417
Epoch: 0, step: 528, training loss: 0.02152
Epoch: 0, step: 529, training loss: 0.02287
Epoch: 0, step: 530, training loss: 0.00994
Epoch: 0, step: 531, training loss: 0.01208
Epoch: 0, step: 532, training loss: 0.02601
Epoch: 0, step: 533, training loss: 0.01595
Epoch: 0, step: 534, training loss: 0.01919
Epoch: 0, step: 535, training loss: 0.01996
Epoch: 0, step: 536, training loss: 0.01554
Epoch: 0, step: 537, training loss: 0.02700
Epoch: 0, step: 538, training loss: 0.01339
Epoch: 0, step: 539, training loss: 0.01508
Epoch: 0, step: 540, training loss: 0.01749
Epoch: 0, step: 541, training loss: 0.01710
Epoch: 0, step: 542, training loss: 0.01423
Epoch: 0, step: 543, training loss: 0.02405
Epoch: 0, step: 544, training loss: 0.02980
Epoch: 0, step: 545, training loss: 0.02184
Epoch: 0, step: 546, training loss: 0.01274
Epoch: 0, step: 547, training loss: 0.01933
Epoch: 0, step: 548, training loss: 0.01862
Epoch: 0, step: 549, training loss: 0.01404
Epoch: 0, step: 550, training loss: 0.02341
Epoch: 0, step: 551, training loss: 0.01092
Epoch: 0, step: 552, training loss: 0.02158
Epoch: 0, step: 553, training loss: 0.01860
Epoch: 0, step: 554, training loss: 0.01527
Epoch: 0, step: 555, training loss: 0.01962
Epoch: 0, step: 556, training loss: 0.02205
Epoch: 0, step: 557, training loss: 0.01230
Epoch: 0, step: 558, training loss: 0.01273
Epoch: 0, step: 559, training loss: 0.01438
Epoch: 0, step: 560, training loss: 0.01386
Epoch: 0, step: 561, training loss: 0.01703
Epoch: 0, step: 562, training loss: 0.02418
Epoch: 0, step: 563, training loss: 0.01684
Epoch: 0, step: 564, training loss: 0.02088
Epoch: 0, step: 565, training loss: 0.02370
Epoch: 0, step: 566, training loss: 0.02181
Epoch: 0, step: 567, training loss: 0.02430
Epoch: 0, step: 568, training loss: 0.01137
Epoch: 0, step: 569, training loss: 0.01401
Epoch: 0, step: 570, training loss: 0.01285
Epoch: 0, step: 571, training loss: 0.01861
Epoch: 0, step: 572, training loss: 0.01166
Epoch: 0, step: 573, training loss: 0.00979
Epoch: 0, step: 574, training loss: 0.02429
Epoch: 0, step: 575, training loss: 0.01803
Epoch: 0, step: 576, training loss: 0.01643
Epoch: 0, step: 577, training loss: 0.01204
Epoch: 0, step: 578, training loss: 0.02147
Epoch: 0, step: 579, training loss: 0.01891
Epoch: 0, step: 580, training loss: 0.01288
Epoch: 0, step: 581, training loss: 0.01570
Epoch: 0, step: 582, training loss: 0.01885
Epoch: 0, step: 583, training loss: 0.03102
Epoch: 0, step: 584, training loss: 0.00993
Epoch: 0, step: 585, training loss: 0.02091
Epoch: 0, step: 586, training loss: 0.02113
Epoch: 0, step: 587, training loss: 0.01299
Epoch: 0, step: 588, training loss: 0.01549
Epoch: 0, step: 589, training loss: 0.01906
Epoch: 0, step: 590, training loss: 0.02411
Epoch: 0, step: 591, training loss: 0.00915
Epoch: 0, step: 592, training loss: 0.01786
Epoch: 0, step: 593, training loss: 0.01655
Epoch: 0, step: 594, training loss: 0.02324
Epoch: 0, step: 595, training loss: 0.01511
Epoch: 0, step: 596, training loss: 0.01059
Epoch: 0, step: 597, training loss: 0.01732
Epoch: 0, step: 598, training loss: 0.01806
Epoch: 0, step: 599, training loss: 0.02275
Epoch: 0, step: 600, training loss: 0.01168
Epoch: 0, step: 601, training loss: 0.01732
Epoch: 0, step: 602, training loss: 0.01801
Epoch: 0, step: 603, training loss: 0.02418
Epoch: 0, step: 604, training loss: 0.01754
Epoch: 0, step: 605, training loss: 0.02325
Epoch: 0, step: 606, training loss: 0.02218
Epoch: 0, step: 607, training loss: 0.02175
Epoch: 0, step: 608, training loss: 0.01768
Epoch: 0, step: 609, training loss: 0.01775
Epoch: 0, step: 610, training loss: 0.01882
Epoch: 0, step: 611, training loss: 0.01746
Epoch: 0, step: 612, training loss: 0.02258
Epoch: 0, step: 613, training loss: 0.01747
Epoch: 0, step: 614, training loss: 0.01298
Epoch: 0, step: 615, training loss: 0.01746
Epoch: 0, step: 616, training loss: 0.01717
Epoch: 0, step: 617, training loss: 0.02333
Epoch: 0, step: 618, training loss: 0.01536
Epoch: 0, step: 619, training loss: 0.02037
Epoch: 0, step: 620, training loss: 0.01022
Epoch: 0, step: 621, training loss: 0.01658
Epoch: 0, step: 622, training loss: 0.01681
Epoch: 0, step: 623, training loss: 0.01763
Epoch: 0, step: 624, training loss: 0.02347
Epoch: 0, step: 625, training loss: 0.02057
Epoch: 0, step: 626, training loss: 0.01994
Epoch: 0, step: 627, training loss: 0.01571
Epoch: 0, step: 628, training loss: 0.01487
Epoch: 0, step: 629, training loss: 0.01177
Epoch: 0, step: 630, training loss: 0.01746
Epoch: 0, step: 631, training loss: 0.02145
Epoch: 0, step: 632, training loss: 0.01056
Epoch: 0, step: 633, training loss: 0.01164
Epoch: 0, step: 634, training loss: 0.02384
Epoch: 0, step: 635, training loss: 0.02024
Epoch: 0, step: 636, training loss: 0.02023
Epoch: 0, step: 637, training loss: 0.01869
Epoch: 0, step: 638, training loss: 0.02113
Epoch: 0, step: 639, training loss: 0.01690
Epoch: 0, step: 640, training loss: 0.02008
Epoch: 0, step: 641, training loss: 0.01960
Epoch: 0, step: 642, training loss: 0.01759
Epoch: 0, step: 643, training loss: 0.02439
Epoch: 0, step: 644, training loss: 0.00738
Epoch: 0, step: 645, training loss: 0.01552
Epoch: 0, step: 646, training loss: 0.02266
Epoch: 0, step: 647, training loss: 0.01192
Epoch: 0, step: 648, training loss: 0.01554
Epoch: 0, step: 649, training loss: 0.01534
Epoch: 0, step: 650, training loss: 0.01433
Epoch: 0, step: 651, training loss: 0.01529
Epoch: 0, step: 652, training loss: 0.01814
Epoch: 0, step: 653, training loss: 0.02325
Epoch: 0, step: 654, training loss: 0.01627
Epoch: 0, step: 655, training loss: 0.01378
Epoch: 0, step: 656, training loss: 0.01698
Epoch: 0, step: 657, training loss: 0.02909
Epoch: 0, step: 658, training loss: 0.01614
Epoch: 0, step: 659, training loss: 0.02061
Epoch: 0, step: 660, training loss: 0.01516
Epoch: 0, step: 661, training loss: 0.00735
Epoch: 0, step: 662, training loss: 0.01376
Epoch: 0, step: 663, training loss: 0.01853
Epoch: 0, step: 664, training loss: 0.01281
Epoch: 0, step: 665, training loss: 0.01674
Epoch: 0, step: 666, training loss: 0.01280
Epoch: 0, step: 667, training loss: 0.01181
Epoch: 0, step: 668, training loss: 0.01610
Epoch: 0, step: 669, training loss: 0.00751
Epoch: 0, step: 670, training loss: 0.02025
Epoch: 0, step: 671, training loss: 0.00967
Epoch: 0, step: 672, training loss: 0.01081
Epoch: 0, step: 673, training loss: 0.01912
Epoch: 0, step: 674, training loss: 0.01157
Epoch: 0, step: 675, training loss: 0.01682
Epoch: 0, step: 676, training loss: 0.01142
Epoch: 0, step: 677, training loss: 0.01504
Epoch: 0, step: 678, training loss: 0.01249
Epoch: 0, step: 679, training loss: 0.01524
Epoch: 0, step: 680, training loss: 0.01555
Epoch: 0, step: 681, training loss: 0.01272
Epoch: 0, step: 682, training loss: 0.01709
Epoch: 0, step: 683, training loss: 0.01383
Epoch: 0, step: 684, training loss: 0.02008
Epoch: 0, step: 685, training loss: 0.01302
Epoch: 0, step: 686, training loss: 0.01192
Epoch: 0, step: 687, training loss: 0.01816
Epoch: 0, step: 688, training loss: 0.01438
Epoch: 0, step: 689, training loss: 0.01599
Epoch: 0, step: 690, training loss: 0.01456
Epoch: 0, step: 691, training loss: 0.02010
Epoch: 0, step: 692, training loss: 0.02239
Epoch: 0, step: 693, training loss: 0.01661
Epoch: 0, step: 694, training loss: 0.01195
Epoch: 0, step: 695, training loss: 0.01177
Epoch: 0, step: 696, training loss: 0.01174
Epoch: 0, step: 697, training loss: 0.01460
Epoch: 0, step: 698, training loss: 0.01734
Epoch: 0, step: 699, training loss: 0.00794
Epoch: 0, step: 700, training loss: 0.01412
Epoch: 0, step: 701, training loss: 0.01016
Epoch: 0, step: 702, training loss: 0.01731
Epoch: 0, step: 703, training loss: 0.01260
Epoch: 0, step: 704, training loss: 0.01007
Epoch: 0, step: 705, training loss: 0.01202
Epoch: 0, step: 706, training loss: 0.01365
Epoch: 0, step: 707, training loss: 0.01940
Epoch: 0, step: 708, training loss: 0.02957
Epoch: 0, step: 709, training loss: 0.01126
Epoch: 0, step: 710, training loss: 0.01589
Epoch: 0, step: 711, training loss: 0.02017
Epoch: 0, step: 712, training loss: 0.01619
Epoch: 0, step: 713, training loss: 0.01247
Epoch: 0, step: 714, training loss: 0.01291
Epoch: 0, step: 715, training loss: 0.01049
Epoch: 0, step: 716, training loss: 0.01342
Epoch: 0, step: 717, training loss: 0.01669
Epoch: 0, step: 718, training loss: 0.02378
Epoch: 0, step: 719, training loss: 0.01705
Epoch: 0, step: 720, training loss: 0.01369
Epoch: 0, step: 721, training loss: 0.01998
Epoch: 0, step: 722, training loss: 0.01606
Epoch: 0, step: 723, training loss: 0.02299
Epoch: 0, step: 724, training loss: 0.01611
Epoch: 0, step: 725, training loss: 0.01435
Epoch: 0, step: 726, training loss: 0.00880
Epoch: 0, step: 727, training loss: 0.01995
Epoch: 0, step: 728, training loss: 0.01465
Epoch: 0, step: 729, training loss: 0.01063
Epoch: 0, step: 730, training loss: 0.01302
Epoch: 0, step: 731, training loss: 0.01276
Epoch: 0, step: 732, training loss: 0.02035
Epoch: 0, step: 733, training loss: 0.01824
Epoch: 0, step: 734, training loss: 0.01596
Epoch: 0, step: 735, training loss: 0.02274
Epoch: 0, step: 736, training loss: 0.01160
Epoch: 0, step: 737, training loss: 0.01305
Epoch: 0, step: 738, training loss: 0.01199
Epoch: 0, step: 739, training loss: 0.01555
Epoch: 0, step: 740, training loss: 0.01623
Epoch: 0, step: 741, training loss: 0.01502
Epoch: 0, step: 742, training loss: 0.00775
Epoch: 0, step: 743, training loss: 0.01772
Epoch: 0, step: 744, training loss: 0.00943
Epoch: 0, step: 745, training loss: 0.01456
Epoch: 0, step: 746, training loss: 0.01677
Epoch: 0, step: 747, training loss: 0.02265
Epoch: 0, step: 748, training loss: 0.01472
Epoch: 0, step: 749, training loss: 0.01590
Epoch: 0, step: 750, training loss: 0.01464
Epoch: 0, step: 751, training loss: 0.01275
Epoch: 0, step: 752, training loss: 0.01333
Epoch: 0, step: 753, training loss: 0.02572
Epoch: 0, step: 754, training loss: 0.01306
Epoch: 0, step: 755, training loss: 0.01087
Epoch: 0, step: 756, training loss: 0.01932
Epoch: 0, step: 757, training loss: 0.01766
Epoch: 0, step: 758, training loss: 0.01730
Epoch: 0, step: 759, training loss: 0.01820
Epoch: 0, step: 760, training loss: 0.01512
Epoch: 0, step: 761, training loss: 0.01239
Epoch: 0, step: 762, training loss: 0.01236
Epoch: 0, step: 763, training loss: 0.01233
Epoch: 0, step: 764, training loss: 0.01057
Epoch: 0, step: 765, training loss: 0.01248
Epoch: 0, step: 766, training loss: 0.01164
Epoch: 0, step: 767, training loss: 0.01230
Epoch: 0, step: 768, training loss: 0.02007
Epoch: 0, step: 769, training loss: 0.00803
Epoch: 0, step: 770, training loss: 0.01225
Epoch: 0, step: 771, training loss: 0.00986
Epoch: 0, step: 772, training loss: 0.01320
Epoch: 0, step: 773, training loss: 0.01644
Epoch: 0, step: 774, training loss: 0.01810
Epoch: 0, step: 775, training loss: 0.01294
Epoch: 0, step: 776, training loss: 0.01775
Epoch: 0, step: 777, training loss: 0.01374
Epoch: 0, step: 778, training loss: 0.02312
Epoch: 0, step: 779, training loss: 0.00948
Epoch: 0, step: 780, training loss: 0.01378
Epoch: 0, step: 781, training loss: 0.01713
Epoch: 0, step: 782, training loss: 0.01343
Epoch: 0, step: 783, training loss: 0.01795
Epoch: 0, step: 784, training loss: 0.01148
Epoch: 0, step: 785, training loss: 0.01213
Epoch: 0, step: 786, training loss: 0.01667
Epoch: 0, step: 787, training loss: 0.01460
Epoch: 0, step: 788, training loss: 0.00640
Epoch: 0, step: 789, training loss: 0.01833
Epoch: 0, step: 790, training loss: 0.00938
Epoch: 0, step: 791, training loss: 0.01521
Epoch: 0, step: 792, training loss: 0.02284
Epoch: 0, step: 793, training loss: 0.01451
Epoch: 0, step: 794, training loss: 0.01268
Epoch: 0, step: 795, training loss: 0.01860
Epoch: 0, step: 796, training loss: 0.00698
Epoch: 0, step: 797, training loss: 0.01221
Epoch: 0, step: 798, training loss: 0.01643
Epoch: 0, step: 799, training loss: 0.02099
Epoch: 0, step: 800, training loss: 0.01712
Epoch: 0, step: 801, training loss: 0.01465
Epoch: 0, step: 802, training loss: 0.01126
Epoch: 0, step: 803, training loss: 0.01637
Epoch: 0, step: 804, training loss: 0.01294
Epoch: 0, step: 805, training loss: 0.00733
Epoch: 0, step: 806, training loss: 0.02043
Epoch: 0, step: 807, training loss: 0.01505
Epoch: 0, step: 808, training loss: 0.01482
Epoch: 0, step: 809, training loss: 0.01940
Epoch: 0, step: 810, training loss: 0.00796
Epoch: 0, step: 811, training loss: 0.01550
Epoch: 0, step: 812, training loss: 0.01198
Epoch: 0, step: 813, training loss: 0.01747
Epoch: 0, step: 814, training loss: 0.01557
Epoch: 0, step: 815, training loss: 0.01699
Epoch: 0, step: 816, training loss: 0.00937
Epoch: 0, step: 817, training loss: 0.01036
Epoch: 0, step: 818, training loss: 0.01269
Epoch: 0, step: 819, training loss: 0.01360
Epoch: 0, step: 820, training loss: 0.01782
Epoch: 0, step: 821, training loss: 0.01471
Epoch: 0, step: 822, training loss: 0.01108
Epoch: 0, step: 823, training loss: 0.01208
Epoch: 0, step: 824, training loss: 0.01173
Epoch: 0, step: 825, training loss: 0.01649
Epoch: 0, step: 826, training loss: 0.01537
Epoch: 0, step: 827, training loss: 0.00984
Epoch: 0, step: 828, training loss: 0.01044
Epoch: 0, step: 829, training loss: 0.02285
Epoch: 0, step: 830, training loss: 0.01164
Epoch: 0, step: 831, training loss: 0.01307
Epoch: 0, step: 832, training loss: 0.00830
Epoch: 0, step: 833, training loss: 0.01900
Epoch: 0, step: 834, training loss: 0.01242
Epoch: 0, step: 835, training loss: 0.01643
Epoch: 0, step: 836, training loss: 0.01968
Epoch: 0, step: 837, training loss: 0.02683
Epoch: 0, step: 838, training loss: 0.01532
Epoch: 0, step: 839, training loss: 0.01394
Epoch: 0, step: 840, training loss: 0.01385
Epoch: 0, step: 841, training loss: 0.02110
Epoch: 0, step: 842, training loss: 0.00948
Epoch: 0, step: 843, training loss: 0.01639
Epoch: 0, step: 844, training loss: 0.01687
Epoch: 0, step: 845, training loss: 0.01296
Epoch: 0, step: 846, training loss: 0.01079
Epoch: 0, step: 847, training loss: 0.01439
Epoch: 0, step: 848, training loss: 0.01064
Epoch: 0, step: 849, training loss: 0.01847
Epoch: 0, step: 850, training loss: 0.02280
Epoch: 0, step: 851, training loss: 0.01619
Epoch: 0, step: 852, training loss: 0.02089
Epoch: 0, step: 853, training loss: 0.01697
Epoch: 0, step: 854, training loss: 0.01671
Epoch: 0, step: 855, training loss: 0.01244
Epoch: 0, step: 856, training loss: 0.01868
Epoch: 0, step: 857, training loss: 0.01603
Epoch: 0, step: 858, training loss: 0.01192
Epoch: 0, step: 859, training loss: 0.01370
Epoch: 0, step: 860, training loss: 0.01675
Epoch: 0, step: 861, training loss: 0.01507
Epoch: 0, step: 862, training loss: 0.01871
Epoch: 0, step: 863, training loss: 0.01339
Epoch: 0, step: 864, training loss: 0.01616
Epoch: 0, step: 865, training loss: 0.01254
Epoch: 0, step: 866, training loss: 0.01359
Epoch: 0, step: 867, training loss: 0.01897
Epoch: 0, step: 868, training loss: 0.01092
Epoch: 0, step: 869, training loss: 0.01074
Epoch: 0, step: 870, training loss: 0.01375
Epoch: 0, step: 871, training loss: 0.01216
Epoch: 0, step: 872, training loss: 0.00883
Epoch: 0, step: 873, training loss: 0.01239
Epoch: 0, step: 874, training loss: 0.01511
Epoch: 0, step: 875, training loss: 0.01048
Epoch: 0, step: 876, training loss: 0.01343
Epoch: 0, step: 877, training loss: 0.01092
Epoch: 0, step: 878, training loss: 0.01248
Epoch: 0, step: 879, training loss: 0.01175
Epoch: 0, step: 880, training loss: 0.01148
Epoch: 0, step: 881, training loss: 0.01152
Epoch: 0, step: 882, training loss: 0.01528
Epoch: 0, step: 883, training loss: 0.01188
Epoch: 0, step: 884, training loss: 0.01376
Epoch: 0, step: 885, training loss: 0.01559
Epoch: 0, step: 886, training loss: 0.01692
Epoch: 0, step: 887, training loss: 0.01266
Epoch: 0, step: 888, training loss: 0.01588
Epoch: 0, step: 889, training loss: 0.01589
Epoch: 0, step: 890, training loss: 0.01584
Epoch: 0, step: 891, training loss: 0.00882
Epoch: 0, step: 892, training loss: 0.00953
Epoch: 0, step: 893, training loss: 0.00713
Epoch: 0, step: 894, training loss: 0.01710
Epoch: 0, step: 895, training loss: 0.01046
Epoch: 0, step: 896, training loss: 0.01244
Epoch: 0, step: 897, training loss: 0.02288
Epoch: 0, step: 898, training loss: 0.01024
Epoch: 0, step: 899, training loss: 0.01553
Epoch: 0, step: 900, training loss: 0.01209
Epoch: 0, step: 901, training loss: 0.00685
Epoch: 0, step: 902, training loss: 0.01007
Epoch: 0, step: 903, training loss: 0.01796
Epoch: 0, step: 904, training loss: 0.01705
Epoch: 0, step: 905, training loss: 0.01183
Epoch: 0, step: 906, training loss: 0.01346
Epoch: 0, step: 907, training loss: 0.01877
Epoch: 0, step: 908, training loss: 0.00767
Epoch: 0, step: 909, training loss: 0.00753
Epoch: 0, step: 910, training loss: 0.01200
Epoch: 0, step: 911, training loss: 0.01039
Epoch: 0, step: 912, training loss: 0.02381
Epoch: 0, step: 913, training loss: 0.01115
Epoch: 0, step: 914, training loss: 0.01466
Epoch: 0, step: 915, training loss: 0.00751
Epoch: 0, step: 916, training loss: 0.00788
Epoch: 0, step: 917, training loss: 0.00971
Epoch: 0, step: 918, training loss: 0.01941
Epoch: 0, step: 919, training loss: 0.01110
Epoch: 0, step: 920, training loss: 0.01333
Epoch: 0, step: 921, training loss: 0.01083
Epoch: 0, step: 922, training loss: 0.01294
Epoch: 0, step: 923, training loss: 0.01630
Epoch: 0, step: 924, training loss: 0.01296
Epoch: 0, step: 925, training loss: 0.02328
Epoch: 0, step: 926, training loss: 0.01016
Epoch: 0, step: 927, training loss: 0.01959
Epoch: 0, step: 928, training loss: 0.00851
Epoch: 0, step: 929, training loss: 0.01508
Epoch: 0, step: 930, training loss: 0.01545
Epoch: 0, step: 931, training loss: 0.01182
Epoch: 0, step: 932, training loss: 0.01884
Epoch: 0, step: 933, training loss: 0.01245
Epoch: 0, step: 934, training loss: 0.01745
Epoch: 0, step: 935, training loss: 0.00706
Epoch: 0, step: 936, training loss: 0.01162
Epoch: 0, step: 937, training loss: 0.01587
Epoch: 0, step: 938, training loss: 0.01706
Epoch: 0, step: 939, training loss: 0.01296
Epoch: 0, step: 940, training loss: 0.01221
Epoch: 0, step: 941, training loss: 0.01426
Epoch: 0, step: 942, training loss: 0.01107
Epoch: 0, step: 943, training loss: 0.01073
Epoch: 0, step: 944, training loss: 0.01009
Epoch: 0, step: 945, training loss: 0.00963
Epoch: 0, step: 946, training loss: 0.01134
Epoch: 0, step: 947, training loss: 0.01283
Epoch: 0, step: 948, training loss: 0.01278
Epoch: 0, step: 949, training loss: 0.01221
Epoch: 0, step: 950, training loss: 0.01309
Epoch: 0, step: 951, training loss: 0.01007
Epoch: 0, step: 952, training loss: 0.01356
Epoch: 0, step: 953, training loss: 0.01219
Epoch: 0, step: 954, training loss: 0.01133
Epoch: 0, average training loss: 0.03166
Epoch: 0, F1: 0.00000, average dev loss: 0.01208
Epoch: 1, step: 0, training loss: 0.01019
Epoch: 1, step: 1, training loss: 0.01200
Epoch: 1, step: 2, training loss: 0.01030
Epoch: 1, step: 3, training loss: 0.01434
Epoch: 1, step: 4, training loss: 0.01092
Epoch: 1, step: 5, training loss: 0.02192
Epoch: 1, step: 6, training loss: 0.00830
Epoch: 1, step: 7, training loss: 0.00742
Epoch: 1, step: 8, training loss: 0.01113
Epoch: 1, step: 9, training loss: 0.01223
Epoch: 1, step: 10, training loss: 0.00836
Epoch: 1, step: 11, training loss: 0.01087
Epoch: 1, step: 12, training loss: 0.01075
Epoch: 1, step: 13, training loss: 0.01120
Epoch: 1, step: 14, training loss: 0.01072
Epoch: 1, step: 15, training loss: 0.00839
Epoch: 1, step: 16, training loss: 0.01734
Epoch: 1, step: 17, training loss: 0.01268
Epoch: 1, step: 18, training loss: 0.01747
Epoch: 1, step: 19, training loss: 0.01927
Epoch: 1, step: 20, training loss: 0.01143
Epoch: 1, step: 21, training loss: 0.00759
Epoch: 1, step: 22, training loss: 0.01534
Epoch: 1, step: 23, training loss: 0.00714
Epoch: 1, step: 24, training loss: 0.01006
Epoch: 1, step: 25, training loss: 0.01381
Epoch: 1, step: 26, training loss: 0.01240
Epoch: 1, step: 27, training loss: 0.01229
Epoch: 1, step: 28, training loss: 0.01629
Epoch: 1, step: 29, training loss: 0.00733
Epoch: 1, step: 30, training loss: 0.01459
Epoch: 1, step: 31, training loss: 0.01574
Epoch: 1, step: 32, training loss: 0.01022
Epoch: 1, step: 33, training loss: 0.00626
Epoch: 1, step: 34, training loss: 0.01317
Epoch: 1, step: 35, training loss: 0.01442
Epoch: 1, step: 36, training loss: 0.01662
Epoch: 1, step: 37, training loss: 0.01659
Epoch: 1, step: 38, training loss: 0.01103
Epoch: 1, step: 39, training loss: 0.01002
Epoch: 1, step: 40, training loss: 0.01516
Epoch: 1, step: 41, training loss: 0.00946
Epoch: 1, step: 42, training loss: 0.01020
Epoch: 1, step: 43, training loss: 0.01297
Epoch: 1, step: 44, training loss: 0.00927
Epoch: 1, step: 45, training loss: 0.01453
Epoch: 1, step: 46, training loss: 0.00903
Epoch: 1, step: 47, training loss: 0.00773
Epoch: 1, step: 48, training loss: 0.01060
Epoch: 1, step: 49, training loss: 0.01487
Epoch: 1, step: 50, training loss: 0.00522
Epoch: 1, step: 51, training loss: 0.01593
Epoch: 1, step: 52, training loss: 0.01479
Epoch: 1, step: 53, training loss: 0.01094
Epoch: 1, step: 54, training loss: 0.00480
Epoch: 1, step: 55, training loss: 0.00988
Epoch: 1, step: 56, training loss: 0.00697
Epoch: 1, step: 57, training loss: 0.01128
Epoch: 1, step: 58, training loss: 0.01423
Epoch: 1, step: 59, training loss: 0.01486
Epoch: 1, step: 60, training loss: 0.00733
Epoch: 1, step: 61, training loss: 0.01307
Epoch: 1, step: 62, training loss: 0.01543
Epoch: 1, step: 63, training loss: 0.02500
Epoch: 1, step: 64, training loss: 0.01369
Epoch: 1, step: 65, training loss: 0.01512
Epoch: 1, step: 66, training loss: 0.01516
Epoch: 1, step: 67, training loss: 0.00792
Epoch: 1, step: 68, training loss: 0.01925
Epoch: 1, step: 69, training loss: 0.00780
Epoch: 1, step: 70, training loss: 0.00686
Epoch: 1, step: 71, training loss: 0.01215
Epoch: 1, step: 72, training loss: 0.00825
Epoch: 1, step: 73, training loss: 0.01892
Epoch: 1, step: 74, training loss: 0.01051
Epoch: 1, step: 75, training loss: 0.00871
Epoch: 1, step: 76, training loss: 0.00873
Epoch: 1, step: 77, training loss: 0.01311
Epoch: 1, step: 78, training loss: 0.01027
Epoch: 1, step: 79, training loss: 0.00568
Epoch: 1, step: 80, training loss: 0.01161
Epoch: 1, step: 81, training loss: 0.01075
Epoch: 1, step: 82, training loss: 0.01046
Epoch: 1, step: 83, training loss: 0.00954
Epoch: 1, step: 84, training loss: 0.00683
Epoch: 1, step: 85, training loss: 0.00930
Epoch: 1, step: 86, training loss: 0.01413
Epoch: 1, step: 87, training loss: 0.01549
Epoch: 1, step: 88, training loss: 0.01890
Epoch: 1, step: 89, training loss: 0.01166
Epoch: 1, step: 90, training loss: 0.01393
Epoch: 1, step: 91, training loss: 0.01384
Epoch: 1, step: 92, training loss: 0.01182
Epoch: 1, step: 93, training loss: 0.00911
Epoch: 1, step: 94, training loss: 0.01395
Epoch: 1, step: 95, training loss: 0.00924
Epoch: 1, step: 96, training loss: 0.02163
Epoch: 1, step: 97, training loss: 0.02066
Epoch: 1, step: 98, training loss: 0.01332
Epoch: 1, step: 99, training loss: 0.01724
Epoch: 1, step: 100, training loss: 0.01540
Epoch: 1, step: 101, training loss: 0.01255
Epoch: 1, step: 102, training loss: 0.01070
Epoch: 1, step: 103, training loss: 0.01826
Epoch: 1, step: 104, training loss: 0.00718
Epoch: 1, step: 105, training loss: 0.00608
Epoch: 1, step: 106, training loss: 0.01165
Epoch: 1, step: 107, training loss: 0.01158
Epoch: 1, step: 108, training loss: 0.01362
Epoch: 1, step: 109, training loss: 0.00778
Epoch: 1, step: 110, training loss: 0.01094
Epoch: 1, step: 111, training loss: 0.01186
Epoch: 1, step: 112, training loss: 0.00548
Epoch: 1, step: 113, training loss: 0.01039
Epoch: 1, step: 114, training loss: 0.01890
Epoch: 1, step: 115, training loss: 0.00975
Epoch: 1, step: 116, training loss: 0.01122
Epoch: 1, step: 117, training loss: 0.01418
Epoch: 1, step: 118, training loss: 0.01351
Epoch: 1, step: 119, training loss: 0.00942
Epoch: 1, step: 120, training loss: 0.01133
Epoch: 1, step: 121, training loss: 0.00828
Epoch: 1, step: 122, training loss: 0.01000
Epoch: 1, step: 123, training loss: 0.01004
Epoch: 1, step: 124, training loss: 0.01347
Epoch: 1, step: 125, training loss: 0.01330
Epoch: 1, step: 126, training loss: 0.00697
Epoch: 1, step: 127, training loss: 0.01802
Epoch: 1, step: 128, training loss: 0.01105
Epoch: 1, step: 129, training loss: 0.01090
Epoch: 1, step: 130, training loss: 0.00896
Epoch: 1, step: 131, training loss: 0.01172
Epoch: 1, step: 132, training loss: 0.01089
Epoch: 1, step: 133, training loss: 0.00681
Epoch: 1, step: 134, training loss: 0.01729
Epoch: 1, step: 135, training loss: 0.00717
Epoch: 1, step: 136, training loss: 0.01189
Epoch: 1, step: 137, training loss: 0.02285
Epoch: 1, step: 138, training loss: 0.01212
Epoch: 1, step: 139, training loss: 0.00937
Epoch: 1, step: 140, training loss: 0.01324
Epoch: 1, step: 141, training loss: 0.00876
Epoch: 1, step: 142, training loss: 0.01111
Epoch: 1, step: 143, training loss: 0.00859
Epoch: 1, step: 144, training loss: 0.01748
Epoch: 1, step: 145, training loss: 0.01474
Epoch: 1, step: 146, training loss: 0.01130
Epoch: 1, step: 147, training loss: 0.01259
Epoch: 1, step: 148, training loss: 0.01046
Epoch: 1, step: 149, training loss: 0.01339
Epoch: 1, step: 150, training loss: 0.00855
Epoch: 1, step: 151, training loss: 0.01219
Epoch: 1, step: 152, training loss: 0.01275
Epoch: 1, step: 153, training loss: 0.01573
Epoch: 1, step: 154, training loss: 0.01358
Epoch: 1, step: 155, training loss: 0.01355
Epoch: 1, step: 156, training loss: 0.00867
Epoch: 1, step: 157, training loss: 0.00967
Epoch: 1, step: 158, training loss: 0.01312
Epoch: 1, step: 159, training loss: 0.01072
Epoch: 1, step: 160, training loss: 0.00927
Epoch: 1, step: 161, training loss: 0.01319
Epoch: 1, step: 162, training loss: 0.02124
Epoch: 1, step: 163, training loss: 0.00926
Epoch: 1, step: 164, training loss: 0.01233
Epoch: 1, step: 165, training loss: 0.00816
Epoch: 1, step: 166, training loss: 0.00428
Epoch: 1, step: 167, training loss: 0.01347
Epoch: 1, step: 168, training loss: 0.01129
Epoch: 1, step: 169, training loss: 0.01048
Epoch: 1, step: 170, training loss: 0.00854
Epoch: 1, step: 171, training loss: 0.01088
Epoch: 1, step: 172, training loss: 0.00834
Epoch: 1, step: 173, training loss: 0.01372
Epoch: 1, step: 174, training loss: 0.01036
Epoch: 1, step: 175, training loss: 0.01028
Epoch: 1, step: 176, training loss: 0.00726
Epoch: 1, step: 177, training loss: 0.00675
Epoch: 1, step: 178, training loss: 0.00705
Epoch: 1, step: 179, training loss: 0.00945
Epoch: 1, step: 180, training loss: 0.00977
Epoch: 1, step: 181, training loss: 0.00980
Epoch: 1, step: 182, training loss: 0.00964
Epoch: 1, step: 183, training loss: 0.00623
Epoch: 1, step: 184, training loss: 0.01055
Epoch: 1, step: 185, training loss: 0.01877
Epoch: 1, step: 186, training loss: 0.01521
Epoch: 1, step: 187, training loss: 0.01540
Epoch: 1, step: 188, training loss: 0.00966
Epoch: 1, step: 189, training loss: 0.01300
Epoch: 1, step: 190, training loss: 0.01304
Epoch: 1, step: 191, training loss: 0.00865
Epoch: 1, step: 192, training loss: 0.01222
Epoch: 1, step: 193, training loss: 0.01386
Epoch: 1, step: 194, training loss: 0.00733
Epoch: 1, step: 195, training loss: 0.00920
Epoch: 1, step: 196, training loss: 0.00778
Epoch: 1, step: 197, training loss: 0.01316
Epoch: 1, step: 198, training loss: 0.01612
Epoch: 1, step: 199, training loss: 0.01201
Epoch: 1, step: 200, training loss: 0.00838
Epoch: 1, step: 201, training loss: 0.00617
Epoch: 1, step: 202, training loss: 0.00703
Epoch: 1, step: 203, training loss: 0.02142
Epoch: 1, step: 204, training loss: 0.01244
Epoch: 1, step: 205, training loss: 0.01289
Epoch: 1, step: 206, training loss: 0.00951
Epoch: 1, step: 207, training loss: 0.01514
Epoch: 1, step: 208, training loss: 0.00939
Epoch: 1, step: 209, training loss: 0.01383
Epoch: 1, step: 210, training loss: 0.01775
Epoch: 1, step: 211, training loss: 0.00999
Epoch: 1, step: 212, training loss: 0.00846
Epoch: 1, step: 213, training loss: 0.00973
Epoch: 1, step: 214, training loss: 0.01133
Epoch: 1, step: 215, training loss: 0.01299
Epoch: 1, step: 216, training loss: 0.00875
Epoch: 1, step: 217, training loss: 0.00838
Epoch: 1, step: 218, training loss: 0.00443
Epoch: 1, step: 219, training loss: 0.00722
Epoch: 1, step: 220, training loss: 0.02056
Epoch: 1, step: 221, training loss: 0.01673
Epoch: 1, step: 222, training loss: 0.00774
Epoch: 1, step: 223, training loss: 0.01068
Epoch: 1, step: 224, training loss: 0.01810
Epoch: 1, step: 225, training loss: 0.01455
Epoch: 1, step: 226, training loss: 0.00577
Epoch: 1, step: 227, training loss: 0.01411
Epoch: 1, step: 228, training loss: 0.01524
Epoch: 1, step: 229, training loss: 0.00879
Epoch: 1, step: 230, training loss: 0.00819
Epoch: 1, step: 231, training loss: 0.00574
Epoch: 1, step: 232, training loss: 0.00895
Epoch: 1, step: 233, training loss: 0.00748
Epoch: 1, step: 234, training loss: 0.01158
Epoch: 1, step: 235, training loss: 0.01541
Epoch: 1, step: 236, training loss: 0.00895
Epoch: 1, step: 237, training loss: 0.00872
Epoch: 1, step: 238, training loss: 0.01071
Epoch: 1, step: 239, training loss: 0.00597
Epoch: 1, step: 240, training loss: 0.00501
Epoch: 1, step: 241, training loss: 0.01378
Epoch: 1, step: 242, training loss: 0.01217
Epoch: 1, step: 243, training loss: 0.00673
Epoch: 1, step: 244, training loss: 0.00564
Epoch: 1, step: 245, training loss: 0.00976
Epoch: 1, step: 246, training loss: 0.01050
Epoch: 1, step: 247, training loss: 0.01202
Epoch: 1, step: 248, training loss: 0.00735
Epoch: 1, step: 249, training loss: 0.01008
Epoch: 1, step: 250, training loss: 0.01030
Epoch: 1, step: 251, training loss: 0.00733
Epoch: 1, step: 252, training loss: 0.00668
Epoch: 1, step: 253, training loss: 0.00805
Epoch: 1, step: 254, training loss: 0.01553
Epoch: 1, step: 255, training loss: 0.01517
Epoch: 1, step: 256, training loss: 0.00611
Epoch: 1, step: 257, training loss: 0.01261
Epoch: 1, step: 258, training loss: 0.00978
Epoch: 1, step: 259, training loss: 0.00834
Epoch: 1, step: 260, training loss: 0.00548
Epoch: 1, step: 261, training loss: 0.01303
Epoch: 1, step: 262, training loss: 0.00976
Epoch: 1, step: 263, training loss: 0.00884
Epoch: 1, step: 264, training loss: 0.01355
Epoch: 1, step: 265, training loss: 0.00871
Epoch: 1, step: 266, training loss: 0.01037
Epoch: 1, step: 267, training loss: 0.00994
Epoch: 1, step: 268, training loss: 0.00909
Epoch: 1, step: 269, training loss: 0.01408
Epoch: 1, step: 270, training loss: 0.01015
Epoch: 1, step: 271, training loss: 0.00601
Epoch: 1, step: 272, training loss: 0.02001
Epoch: 1, step: 273, training loss: 0.01155
Epoch: 1, step: 274, training loss: 0.00613
Epoch: 1, step: 275, training loss: 0.00620
Epoch: 1, step: 276, training loss: 0.00870
Epoch: 1, step: 277, training loss: 0.01410
Epoch: 1, step: 278, training loss: 0.00826
Epoch: 1, step: 279, training loss: 0.00931
Epoch: 1, step: 280, training loss: 0.00510
Epoch: 1, step: 281, training loss: 0.01258
Epoch: 1, step: 282, training loss: 0.00684
Epoch: 1, step: 283, training loss: 0.01338
Epoch: 1, step: 284, training loss: 0.00973
Epoch: 1, step: 285, training loss: 0.00820
Epoch: 1, step: 286, training loss: 0.01445
Epoch: 1, step: 287, training loss: 0.00701
Epoch: 1, step: 288, training loss: 0.01244
Epoch: 1, step: 289, training loss: 0.01831
Epoch: 1, step: 290, training loss: 0.00402
Epoch: 1, step: 291, training loss: 0.00612
Epoch: 1, step: 292, training loss: 0.00708
Epoch: 1, step: 293, training loss: 0.00560
Epoch: 1, step: 294, training loss: 0.01276
Epoch: 1, step: 295, training loss: 0.01299
Epoch: 1, step: 296, training loss: 0.00671
Epoch: 1, step: 297, training loss: 0.01307
Epoch: 1, step: 298, training loss: 0.00832
Epoch: 1, step: 299, training loss: 0.01034
Epoch: 1, step: 300, training loss: 0.00944
Epoch: 1, step: 301, training loss: 0.01031
Epoch: 1, step: 302, training loss: 0.00728
Epoch: 1, step: 303, training loss: 0.01299
Epoch: 1, step: 304, training loss: 0.00985
Epoch: 1, step: 305, training loss: 0.00986
Epoch: 1, step: 306, training loss: 0.01116
Epoch: 1, step: 307, training loss: 0.00638
Epoch: 1, step: 308, training loss: 0.01475
Epoch: 1, step: 309, training loss: 0.01116
Epoch: 1, step: 310, training loss: 0.00893
Epoch: 1, step: 311, training loss: 0.01268
Epoch: 1, step: 312, training loss: 0.00773
Epoch: 1, step: 313, training loss: 0.00469
Epoch: 1, step: 314, training loss: 0.01055
Epoch: 1, step: 315, training loss: 0.00854
Epoch: 1, step: 316, training loss: 0.01000
Epoch: 1, step: 317, training loss: 0.00788
Epoch: 1, step: 318, training loss: 0.00707
Epoch: 1, step: 319, training loss: 0.01464
Epoch: 1, step: 320, training loss: 0.01392
Epoch: 1, step: 321, training loss: 0.00661
Epoch: 1, step: 322, training loss: 0.00506
Epoch: 1, step: 323, training loss: 0.01607
Epoch: 1, step: 324, training loss: 0.00950
Epoch: 1, step: 325, training loss: 0.00853
Epoch: 1, step: 326, training loss: 0.00827
Epoch: 1, step: 327, training loss: 0.00835
Epoch: 1, step: 328, training loss: 0.00979
Epoch: 1, step: 329, training loss: 0.00632
Epoch: 1, step: 330, training loss: 0.01021
Epoch: 1, step: 331, training loss: 0.01154
Epoch: 1, step: 332, training loss: 0.00670
Epoch: 1, step: 333, training loss: 0.00819
Epoch: 1, step: 334, training loss: 0.00626
Epoch: 1, step: 335, training loss: 0.00825
Epoch: 1, step: 336, training loss: 0.00716
Epoch: 1, step: 337, training loss: 0.00889
Epoch: 1, step: 338, training loss: 0.01045
Epoch: 1, step: 339, training loss: 0.01045
Epoch: 1, step: 340, training loss: 0.00944
Epoch: 1, step: 341, training loss: 0.00731
Epoch: 1, step: 342, training loss: 0.01000
Epoch: 1, step: 343, training loss: 0.00987
Epoch: 1, step: 344, training loss: 0.00858
Epoch: 1, step: 345, training loss: 0.00402
Epoch: 1, step: 346, training loss: 0.00667
Epoch: 1, step: 347, training loss: 0.00451
Epoch: 1, step: 348, training loss: 0.01115
Epoch: 1, step: 349, training loss: 0.00689
Epoch: 1, step: 350, training loss: 0.01031
Epoch: 1, step: 351, training loss: 0.00602
Epoch: 1, step: 352, training loss: 0.00645
Epoch: 1, step: 353, training loss: 0.00862
Epoch: 1, step: 354, training loss: 0.01059
Epoch: 1, step: 355, training loss: 0.01396
Epoch: 1, step: 356, training loss: 0.00622
Epoch: 1, step: 357, training loss: 0.00658
Epoch: 1, step: 358, training loss: 0.01236
Epoch: 1, step: 359, training loss: 0.00954
Epoch: 1, step: 360, training loss: 0.01022
Epoch: 1, step: 361, training loss: 0.00889
Epoch: 1, step: 362, training loss: 0.01179
Epoch: 1, step: 363, training loss: 0.01141
Epoch: 1, step: 364, training loss: 0.01176
Epoch: 1, step: 365, training loss: 0.00932
Epoch: 1, step: 366, training loss: 0.00448
Epoch: 1, step: 367, training loss: 0.00531
Epoch: 1, step: 368, training loss: 0.01056
Epoch: 1, step: 369, training loss: 0.01938
Epoch: 1, step: 370, training loss: 0.01404
Epoch: 1, step: 371, training loss: 0.01091
Epoch: 1, step: 372, training loss: 0.00749
Epoch: 1, step: 373, training loss: 0.01516
Epoch: 1, step: 374, training loss: 0.01223
Epoch: 1, step: 375, training loss: 0.01155
Epoch: 1, step: 376, training loss: 0.01074
Epoch: 1, step: 377, training loss: 0.00519
Epoch: 1, step: 378, training loss: 0.00781
Epoch: 1, step: 379, training loss: 0.00793
Epoch: 1, step: 380, training loss: 0.01199
Epoch: 1, step: 381, training loss: 0.00672
Epoch: 1, step: 382, training loss: 0.01018
Epoch: 1, step: 383, training loss: 0.00400
Epoch: 1, step: 384, training loss: 0.01586
Epoch: 1, step: 385, training loss: 0.01610
Epoch: 1, step: 386, training loss: 0.00721
Epoch: 1, step: 387, training loss: 0.00609
Epoch: 1, step: 388, training loss: 0.00552
Epoch: 1, step: 389, training loss: 0.00792
Epoch: 1, step: 390, training loss: 0.01002
Epoch: 1, step: 391, training loss: 0.00884
Epoch: 1, step: 392, training loss: 0.01092
Epoch: 1, step: 393, training loss: 0.01052
Epoch: 1, step: 394, training loss: 0.00649
Epoch: 1, step: 395, training loss: 0.01027
Epoch: 1, step: 396, training loss: 0.00844
Epoch: 1, step: 397, training loss: 0.00696
Epoch: 1, step: 398, training loss: 0.00596
Epoch: 1, step: 399, training loss: 0.00520
Epoch: 1, step: 400, training loss: 0.01163
Epoch: 1, step: 401, training loss: 0.00714
Epoch: 1, step: 402, training loss: 0.01656
Epoch: 1, step: 403, training loss: 0.01347
Epoch: 1, step: 404, training loss: 0.00983
Epoch: 1, step: 405, training loss: 0.00488
Epoch: 1, step: 406, training loss: 0.01102
Epoch: 1, step: 407, training loss: 0.00975
Epoch: 1, step: 408, training loss: 0.01097
Epoch: 1, step: 409, training loss: 0.01010
Epoch: 1, step: 410, training loss: 0.01022
Epoch: 1, step: 411, training loss: 0.00648
Epoch: 1, step: 412, training loss: 0.01275
Epoch: 1, step: 413, training loss: 0.00369
Epoch: 1, step: 414, training loss: 0.00993
Epoch: 1, step: 415, training loss: 0.00868
Epoch: 1, step: 416, training loss: 0.00563
Epoch: 1, step: 417, training loss: 0.00862
Epoch: 1, step: 418, training loss: 0.01017
Epoch: 1, step: 419, training loss: 0.00555
Epoch: 1, step: 420, training loss: 0.01475
Epoch: 1, step: 421, training loss: 0.00898
Epoch: 1, step: 422, training loss: 0.01075
Epoch: 1, step: 423, training loss: 0.00457
Epoch: 1, step: 424, training loss: 0.01534
Epoch: 1, step: 425, training loss: 0.00509
Epoch: 1, step: 426, training loss: 0.00698
Epoch: 1, step: 427, training loss: 0.01105
Epoch: 1, step: 428, training loss: 0.00875
Epoch: 1, step: 429, training loss: 0.00986
Epoch: 1, step: 430, training loss: 0.01267
Epoch: 1, step: 431, training loss: 0.00929
Epoch: 1, step: 432, training loss: 0.00945
Epoch: 1, step: 433, training loss: 0.01030
Epoch: 1, step: 434, training loss: 0.01385
Epoch: 1, step: 435, training loss: 0.00824
Epoch: 1, step: 436, training loss: 0.00708
Epoch: 1, step: 437, training loss: 0.00289
Epoch: 1, step: 438, training loss: 0.00606
Epoch: 1, step: 439, training loss: 0.00982
Epoch: 1, step: 440, training loss: 0.00790
Epoch: 1, step: 441, training loss: 0.00549
Epoch: 1, step: 442, training loss: 0.01495
Epoch: 1, step: 443, training loss: 0.01313
Epoch: 1, step: 444, training loss: 0.01202
Epoch: 1, step: 445, training loss: 0.00729
Epoch: 1, step: 446, training loss: 0.00866
Epoch: 1, step: 447, training loss: 0.00712
Epoch: 1, step: 448, training loss: 0.00969
Epoch: 1, step: 449, training loss: 0.00958
Epoch: 1, step: 450, training loss: 0.00809
Epoch: 1, step: 451, training loss: 0.00836
Epoch: 1, step: 452, training loss: 0.00731
Epoch: 1, step: 453, training loss: 0.01150
Epoch: 1, step: 454, training loss: 0.00887
Epoch: 1, step: 455, training loss: 0.00746
Epoch: 1, step: 456, training loss: 0.00562
Epoch: 1, step: 457, training loss: 0.00607
Epoch: 1, step: 458, training loss: 0.00788
Epoch: 1, step: 459, training loss: 0.00738
Epoch: 1, step: 460, training loss: 0.00741
Epoch: 1, step: 461, training loss: 0.00827
Epoch: 1, step: 462, training loss: 0.01005
Epoch: 1, step: 463, training loss: 0.00834
Epoch: 1, step: 464, training loss: 0.00790
Epoch: 1, step: 465, training loss: 0.00489
Epoch: 1, step: 466, training loss: 0.00844
Epoch: 1, step: 467, training loss: 0.00847
Epoch: 1, step: 468, training loss: 0.01254
Epoch: 1, step: 469, training loss: 0.00811
Epoch: 1, step: 470, training loss: 0.00884
Epoch: 1, step: 471, training loss: 0.01305
Epoch: 1, step: 472, training loss: 0.00932
Epoch: 1, step: 473, training loss: 0.00808
Epoch: 1, step: 474, training loss: 0.01252
Epoch: 1, step: 475, training loss: 0.00886
Epoch: 1, step: 476, training loss: 0.00613
Epoch: 1, step: 477, training loss: 0.00666
Epoch: 1, step: 478, training loss: 0.00496
Epoch: 1, step: 479, training loss: 0.00719
Epoch: 1, step: 480, training loss: 0.01048
Epoch: 1, step: 481, training loss: 0.00983
Epoch: 1, step: 482, training loss: 0.00701
Epoch: 1, step: 483, training loss: 0.00825
Epoch: 1, step: 484, training loss: 0.00758
Epoch: 1, step: 485, training loss: 0.00665
Epoch: 1, step: 486, training loss: 0.00918
Epoch: 1, step: 487, training loss: 0.00813
Epoch: 1, step: 488, training loss: 0.00809
Epoch: 1, step: 489, training loss: 0.00871
Epoch: 1, step: 490, training loss: 0.00931
Epoch: 1, step: 491, training loss: 0.01269
Epoch: 1, step: 492, training loss: 0.01045
Epoch: 1, step: 493, training loss: 0.00636
Epoch: 1, step: 494, training loss: 0.01504
Epoch: 1, step: 495, training loss: 0.00648
Epoch: 1, step: 496, training loss: 0.01154
Epoch: 1, step: 497, training loss: 0.00857
Epoch: 1, step: 498, training loss: 0.00829
Epoch: 1, step: 499, training loss: 0.01046
Epoch: 1, step: 500, training loss: 0.00339
Epoch: 1, step: 501, training loss: 0.01025
Epoch: 1, step: 502, training loss: 0.00974
Epoch: 1, step: 503, training loss: 0.00576
Epoch: 1, step: 504, training loss: 0.01169
Epoch: 1, step: 505, training loss: 0.01553
Epoch: 1, step: 506, training loss: 0.00903
Epoch: 1, step: 507, training loss: 0.00708
Epoch: 1, step: 508, training loss: 0.00743
Epoch: 1, step: 509, training loss: 0.00561
Epoch: 1, step: 510, training loss: 0.00873
Epoch: 1, step: 511, training loss: 0.01089
Epoch: 1, step: 512, training loss: 0.01139
Epoch: 1, step: 513, training loss: 0.00686
Epoch: 1, step: 514, training loss: 0.00496
Epoch: 1, step: 515, training loss: 0.00468
Epoch: 1, step: 516, training loss: 0.00787
Epoch: 1, step: 517, training loss: 0.01027
Epoch: 1, step: 518, training loss: 0.00795
Epoch: 1, step: 519, training loss: 0.01773
Epoch: 1, step: 520, training loss: 0.00806
Epoch: 1, step: 521, training loss: 0.00551
Epoch: 1, step: 522, training loss: 0.01101
Epoch: 1, step: 523, training loss: 0.01357
Epoch: 1, step: 524, training loss: 0.00564
Epoch: 1, step: 525, training loss: 0.00639
Epoch: 1, step: 526, training loss: 0.01366
Epoch: 1, step: 527, training loss: 0.00580
Epoch: 1, step: 528, training loss: 0.00552
Epoch: 1, step: 529, training loss: 0.00516
Epoch: 1, step: 530, training loss: 0.01045
Epoch: 1, step: 531, training loss: 0.00436
Epoch: 1, step: 532, training loss: 0.01250
Epoch: 1, step: 533, training loss: 0.00381
Epoch: 1, step: 534, training loss: 0.01077
Epoch: 1, step: 535, training loss: 0.00530
Epoch: 1, step: 536, training loss: 0.00726
Epoch: 1, step: 537, training loss: 0.00900
Epoch: 1, step: 538, training loss: 0.00840
Epoch: 1, step: 539, training loss: 0.00735
Epoch: 1, step: 540, training loss: 0.01183
Epoch: 1, step: 541, training loss: 0.00469
Epoch: 1, step: 542, training loss: 0.01595
Epoch: 1, step: 543, training loss: 0.00805
Epoch: 1, step: 544, training loss: 0.00896
Epoch: 1, step: 545, training loss: 0.01137
Epoch: 1, step: 546, training loss: 0.01040
Epoch: 1, step: 547, training loss: 0.01539
Epoch: 1, step: 548, training loss: 0.00757
Epoch: 1, step: 549, training loss: 0.00447
Epoch: 1, step: 550, training loss: 0.00620
Epoch: 1, step: 551, training loss: 0.00507
Epoch: 1, step: 552, training loss: 0.01067
Epoch: 1, step: 553, training loss: 0.00580
Epoch: 1, step: 554, training loss: 0.00631
Epoch: 1, step: 555, training loss: 0.00591
Epoch: 1, step: 556, training loss: 0.00544
Epoch: 1, step: 557, training loss: 0.00349
Epoch: 1, step: 558, training loss: 0.00655
Epoch: 1, step: 559, training loss: 0.00707
Epoch: 1, step: 560, training loss: 0.00400
Epoch: 1, step: 561, training loss: 0.00730
Epoch: 1, step: 562, training loss: 0.00899
Epoch: 1, step: 563, training loss: 0.00509
Epoch: 1, step: 564, training loss: 0.00694
Epoch: 1, step: 565, training loss: 0.00491
Epoch: 1, step: 566, training loss: 0.00307
Epoch: 1, step: 567, training loss: 0.00901
Epoch: 1, step: 568, training loss: 0.01532
Epoch: 1, step: 569, training loss: 0.00805
Epoch: 1, step: 570, training loss: 0.00304
Epoch: 1, step: 571, training loss: 0.00877
Epoch: 1, step: 572, training loss: 0.00544
Epoch: 1, step: 573, training loss: 0.00688
Epoch: 1, step: 574, training loss: 0.01020
Epoch: 1, step: 575, training loss: 0.00969
Epoch: 1, step: 576, training loss: 0.00527
Epoch: 1, step: 577, training loss: 0.00613
Epoch: 1, step: 578, training loss: 0.01241
Epoch: 1, step: 579, training loss: 0.00759
Epoch: 1, step: 580, training loss: 0.01546
Epoch: 1, step: 581, training loss: 0.00874
Epoch: 1, step: 582, training loss: 0.01485
Epoch: 1, step: 583, training loss: 0.01236
Epoch: 1, step: 584, training loss: 0.00637
Epoch: 1, step: 585, training loss: 0.00754
Epoch: 1, step: 586, training loss: 0.00724
Epoch: 1, step: 587, training loss: 0.00800
Epoch: 1, step: 588, training loss: 0.00795
Epoch: 1, step: 589, training loss: 0.00902
Epoch: 1, step: 590, training loss: 0.00438
Epoch: 1, step: 591, training loss: 0.00878
Epoch: 1, step: 592, training loss: 0.00655
Epoch: 1, step: 593, training loss: 0.00699
Epoch: 1, step: 594, training loss: 0.01049
Epoch: 1, step: 595, training loss: 0.01107
Epoch: 1, step: 596, training loss: 0.00839
Epoch: 1, step: 597, training loss: 0.00885
Epoch: 1, step: 598, training loss: 0.00678
Epoch: 1, step: 599, training loss: 0.00733
Epoch: 1, step: 600, training loss: 0.00588
Epoch: 1, step: 601, training loss: 0.01050
Epoch: 1, step: 602, training loss: 0.00783
Epoch: 1, step: 603, training loss: 0.00726
Epoch: 1, step: 604, training loss: 0.00479
Epoch: 1, step: 605, training loss: 0.00788
Epoch: 1, step: 606, training loss: 0.00736
Epoch: 1, step: 607, training loss: 0.00370
Epoch: 1, step: 608, training loss: 0.00919
Epoch: 1, step: 609, training loss: 0.00780
Epoch: 1, step: 610, training loss: 0.00794
Epoch: 1, step: 611, training loss: 0.00587
Epoch: 1, step: 612, training loss: 0.01217
Epoch: 1, step: 613, training loss: 0.01186
Epoch: 1, step: 614, training loss: 0.00445
Epoch: 1, step: 615, training loss: 0.00531
Epoch: 1, step: 616, training loss: 0.00545
Epoch: 1, step: 617, training loss: 0.00867
Epoch: 1, step: 618, training loss: 0.00710
Epoch: 1, step: 619, training loss: 0.00468
Epoch: 1, step: 620, training loss: 0.01473
Epoch: 1, step: 621, training loss: 0.00791
Epoch: 1, step: 622, training loss: 0.00610
Epoch: 1, step: 623, training loss: 0.00989
Epoch: 1, step: 624, training loss: 0.00580
Epoch: 1, step: 625, training loss: 0.00538
Epoch: 1, step: 626, training loss: 0.00786
Epoch: 1, step: 627, training loss: 0.00681
Epoch: 1, step: 628, training loss: 0.00994
Epoch: 1, step: 629, training loss: 0.01915
Epoch: 1, step: 630, training loss: 0.00675
Epoch: 1, step: 631, training loss: 0.00530
Epoch: 1, step: 632, training loss: 0.00857
Epoch: 1, step: 633, training loss: 0.01259
Epoch: 1, step: 634, training loss: 0.00547
Epoch: 1, step: 635, training loss: 0.00848
Epoch: 1, step: 636, training loss: 0.00532
Epoch: 1, step: 637, training loss: 0.00631
Epoch: 1, step: 638, training loss: 0.00739
Epoch: 1, step: 639, training loss: 0.00514
Epoch: 1, step: 640, training loss: 0.01318
Epoch: 1, step: 641, training loss: 0.00527
Epoch: 1, step: 642, training loss: 0.00909
Epoch: 1, step: 643, training loss: 0.00534
Epoch: 1, step: 644, training loss: 0.00597
Epoch: 1, step: 645, training loss: 0.00535
Epoch: 1, step: 646, training loss: 0.00468
Epoch: 1, step: 647, training loss: 0.01171
Epoch: 1, step: 648, training loss: 0.00718
Epoch: 1, step: 649, training loss: 0.00511
Epoch: 1, step: 650, training loss: 0.00606
Epoch: 1, step: 651, training loss: 0.01415
Epoch: 1, step: 652, training loss: 0.01451
Epoch: 1, step: 653, training loss: 0.00829
Epoch: 1, step: 654, training loss: 0.00854
Epoch: 1, step: 655, training loss: 0.00813
Epoch: 1, step: 656, training loss: 0.00516
Epoch: 1, step: 657, training loss: 0.00747
Epoch: 1, step: 658, training loss: 0.00714
Epoch: 1, step: 659, training loss: 0.00404
Epoch: 1, step: 660, training loss: 0.00654
Epoch: 1, step: 661, training loss: 0.00474
Epoch: 1, step: 662, training loss: 0.00620
Epoch: 1, step: 663, training loss: 0.00610
Epoch: 1, step: 664, training loss: 0.01262
Epoch: 1, step: 665, training loss: 0.00659
Epoch: 1, step: 666, training loss: 0.00838
Epoch: 1, step: 667, training loss: 0.00699
Epoch: 1, step: 668, training loss: 0.00593
Epoch: 1, step: 669, training loss: 0.00683
Epoch: 1, step: 670, training loss: 0.01038
Epoch: 1, step: 671, training loss: 0.00375
Epoch: 1, step: 672, training loss: 0.00365
Epoch: 1, step: 673, training loss: 0.01179
Epoch: 1, step: 674, training loss: 0.01085
Epoch: 1, step: 675, training loss: 0.00741
Epoch: 1, step: 676, training loss: 0.00497
Epoch: 1, step: 677, training loss: 0.00490
Epoch: 1, step: 678, training loss: 0.00603
Epoch: 1, step: 679, training loss: 0.00937
Epoch: 1, step: 680, training loss: 0.01126
Epoch: 1, step: 681, training loss: 0.00460
Epoch: 1, step: 682, training loss: 0.00419
Epoch: 1, step: 683, training loss: 0.00570
Epoch: 1, step: 684, training loss: 0.00482
Epoch: 1, step: 685, training loss: 0.00549
Epoch: 1, step: 686, training loss: 0.00689
Epoch: 1, step: 687, training loss: 0.00389
Epoch: 1, step: 688, training loss: 0.00813
Epoch: 1, step: 689, training loss: 0.00899
Epoch: 1, step: 690, training loss: 0.00720
Epoch: 1, step: 691, training loss: 0.00772
Epoch: 1, step: 692, training loss: 0.00457
Epoch: 1, step: 693, training loss: 0.01379
Epoch: 1, step: 694, training loss: 0.00306
Epoch: 1, step: 695, training loss: 0.00716
Epoch: 1, step: 696, training loss: 0.00959
Epoch: 1, step: 697, training loss: 0.00526
Epoch: 1, step: 698, training loss: 0.00343
Epoch: 1, step: 699, training loss: 0.00389
Epoch: 1, step: 700, training loss: 0.01328
Epoch: 1, step: 701, training loss: 0.00419
Epoch: 1, step: 702, training loss: 0.00620
Epoch: 1, step: 703, training loss: 0.01013
Epoch: 1, step: 704, training loss: 0.00451
Epoch: 1, step: 705, training loss: 0.01095
Epoch: 1, step: 706, training loss: 0.00529
Epoch: 1, step: 707, training loss: 0.00283
Epoch: 1, step: 708, training loss: 0.00312
Epoch: 1, step: 709, training loss: 0.00430
Epoch: 1, step: 710, training loss: 0.01050
Epoch: 1, step: 711, training loss: 0.00352
Epoch: 1, step: 712, training loss: 0.00254
Epoch: 1, step: 713, training loss: 0.00935
Epoch: 1, step: 714, training loss: 0.00522
Epoch: 1, step: 715, training loss: 0.00641
Epoch: 1, step: 716, training loss: 0.00868
Epoch: 1, step: 717, training loss: 0.01028
Epoch: 1, step: 718, training loss: 0.00443
Epoch: 1, step: 719, training loss: 0.00701
Epoch: 1, step: 720, training loss: 0.00776
Epoch: 1, step: 721, training loss: 0.00523
Epoch: 1, step: 722, training loss: 0.00402
Epoch: 1, step: 723, training loss: 0.00463
Epoch: 1, step: 724, training loss: 0.00699
Epoch: 1, step: 725, training loss: 0.00537
Epoch: 1, step: 726, training loss: 0.00382
Epoch: 1, step: 727, training loss: 0.00408
Epoch: 1, step: 728, training loss: 0.00978
Epoch: 1, step: 729, training loss: 0.00884
Epoch: 1, step: 730, training loss: 0.00575
Epoch: 1, step: 731, training loss: 0.01003
Epoch: 1, step: 732, training loss: 0.00762
Epoch: 1, step: 733, training loss: 0.00643
Epoch: 1, step: 734, training loss: 0.01171
Epoch: 1, step: 735, training loss: 0.00897
Epoch: 1, step: 736, training loss: 0.00375
Epoch: 1, step: 737, training loss: 0.00651
Epoch: 1, step: 738, training loss: 0.00731
Epoch: 1, step: 739, training loss: 0.00782
Epoch: 1, step: 740, training loss: 0.00734
Epoch: 1, step: 741, training loss: 0.00965
Epoch: 1, step: 742, training loss: 0.00560
Epoch: 1, step: 743, training loss: 0.00380
Epoch: 1, step: 744, training loss: 0.00533
Epoch: 1, step: 745, training loss: 0.00425
Epoch: 1, step: 746, training loss: 0.00419
Epoch: 1, step: 747, training loss: 0.00946
Epoch: 1, step: 748, training loss: 0.00986
Epoch: 1, step: 749, training loss: 0.00448
Epoch: 1, step: 750, training loss: 0.00388
Epoch: 1, step: 751, training loss: 0.00637
Epoch: 1, step: 752, training loss: 0.00884
Epoch: 1, step: 753, training loss: 0.00535
Epoch: 1, step: 754, training loss: 0.00285
Epoch: 1, step: 755, training loss: 0.00530
Epoch: 1, step: 756, training loss: 0.00863
Epoch: 1, step: 757, training loss: 0.00523
Epoch: 1, step: 758, training loss: 0.01528
Epoch: 1, step: 759, training loss: 0.00976
Epoch: 1, step: 760, training loss: 0.00383
Epoch: 1, step: 761, training loss: 0.01080
Epoch: 1, step: 762, training loss: 0.00685
Epoch: 1, step: 763, training loss: 0.00570
Epoch: 1, step: 764, training loss: 0.00605
Epoch: 1, step: 765, training loss: 0.00551
Epoch: 1, step: 766, training loss: 0.00682
Epoch: 1, step: 767, training loss: 0.00647
Epoch: 1, step: 768, training loss: 0.01275
Epoch: 1, step: 769, training loss: 0.00284
Epoch: 1, step: 770, training loss: 0.00429
Epoch: 1, step: 771, training loss: 0.01029
Epoch: 1, step: 772, training loss: 0.00560
Epoch: 1, step: 773, training loss: 0.00614
Epoch: 1, step: 774, training loss: 0.00790
Epoch: 1, step: 775, training loss: 0.00629
Epoch: 1, step: 776, training loss: 0.00503
Epoch: 1, step: 777, training loss: 0.00833
Epoch: 1, step: 778, training loss: 0.00666
Epoch: 1, step: 779, training loss: 0.00477
Epoch: 1, step: 780, training loss: 0.00377
Epoch: 1, step: 781, training loss: 0.00568
Epoch: 1, step: 782, training loss: 0.00627
Epoch: 1, step: 783, training loss: 0.00624
Epoch: 1, step: 784, training loss: 0.01110
Epoch: 1, step: 785, training loss: 0.00425
Epoch: 1, step: 786, training loss: 0.00568
Epoch: 1, step: 787, training loss: 0.00331
Epoch: 1, step: 788, training loss: 0.00669
Epoch: 1, step: 789, training loss: 0.00424
Epoch: 1, step: 790, training loss: 0.00899
Epoch: 1, step: 791, training loss: 0.00846
Epoch: 1, step: 792, training loss: 0.00854
Epoch: 1, step: 793, training loss: 0.00620
Epoch: 1, step: 794, training loss: 0.00465
Epoch: 1, step: 795, training loss: 0.00743
Epoch: 1, step: 796, training loss: 0.00261
Epoch: 1, step: 797, training loss: 0.00475
Epoch: 1, step: 798, training loss: 0.00581
Epoch: 1, step: 799, training loss: 0.00621
Epoch: 1, step: 800, training loss: 0.00639
Epoch: 1, step: 801, training loss: 0.00404
Epoch: 1, step: 802, training loss: 0.00890
Epoch: 1, step: 803, training loss: 0.00583
Epoch: 1, step: 804, training loss: 0.00378
Epoch: 1, step: 805, training loss: 0.00259
Epoch: 1, step: 806, training loss: 0.00382
Epoch: 1, step: 807, training loss: 0.01138
Epoch: 1, step: 808, training loss: 0.01148
Epoch: 1, step: 809, training loss: 0.00480
Epoch: 1, step: 810, training loss: 0.00368
Epoch: 1, step: 811, training loss: 0.00488
Epoch: 1, step: 812, training loss: 0.00257
Epoch: 1, step: 813, training loss: 0.00625
Epoch: 1, step: 814, training loss: 0.00188
Epoch: 1, step: 815, training loss: 0.00860
Epoch: 1, step: 816, training loss: 0.00576
Epoch: 1, step: 817, training loss: 0.00795
Epoch: 1, step: 818, training loss: 0.00313
Epoch: 1, step: 819, training loss: 0.00879
Epoch: 1, step: 820, training loss: 0.00792
Epoch: 1, step: 821, training loss: 0.01174
Epoch: 1, step: 822, training loss: 0.01619
Epoch: 1, step: 823, training loss: 0.00279
Epoch: 1, step: 824, training loss: 0.00793
Epoch: 1, step: 825, training loss: 0.00811
Epoch: 1, step: 826, training loss: 0.00479
Epoch: 1, step: 827, training loss: 0.00884
Epoch: 1, step: 828, training loss: 0.00732
Epoch: 1, step: 829, training loss: 0.00744
Epoch: 1, step: 830, training loss: 0.00315
Epoch: 1, step: 831, training loss: 0.00676
Epoch: 1, step: 832, training loss: 0.00674
Epoch: 1, step: 833, training loss: 0.00682
Epoch: 1, step: 834, training loss: 0.00527
Epoch: 1, step: 835, training loss: 0.00863
Epoch: 1, step: 836, training loss: 0.00511
Epoch: 1, step: 837, training loss: 0.00476
Epoch: 1, step: 838, training loss: 0.00529
Epoch: 1, step: 839, training loss: 0.00788
Epoch: 1, step: 840, training loss: 0.00308
Epoch: 1, step: 841, training loss: 0.00642
Epoch: 1, step: 842, training loss: 0.00907
Epoch: 1, step: 843, training loss: 0.00557
Epoch: 1, step: 844, training loss: 0.00827
Epoch: 1, step: 845, training loss: 0.00428
Epoch: 1, step: 846, training loss: 0.00367
Epoch: 1, step: 847, training loss: 0.00514
Epoch: 1, step: 848, training loss: 0.00606
Epoch: 1, step: 849, training loss: 0.00553
Epoch: 1, step: 850, training loss: 0.00449
Epoch: 1, step: 851, training loss: 0.00566
Epoch: 1, step: 852, training loss: 0.00856
Epoch: 1, step: 853, training loss: 0.00749
Epoch: 1, step: 854, training loss: 0.00652
Epoch: 1, step: 855, training loss: 0.00721
Epoch: 1, step: 856, training loss: 0.00209
Epoch: 1, step: 857, training loss: 0.00506
Epoch: 1, step: 858, training loss: 0.00602
Epoch: 1, step: 859, training loss: 0.00516
Epoch: 1, step: 860, training loss: 0.00646
Epoch: 1, step: 861, training loss: 0.00494
Epoch: 1, step: 862, training loss: 0.00546
Epoch: 1, step: 863, training loss: 0.00710
Epoch: 1, step: 864, training loss: 0.00613
Epoch: 1, step: 865, training loss: 0.00554
Epoch: 1, step: 866, training loss: 0.00936
Epoch: 1, step: 867, training loss: 0.00237
Epoch: 1, step: 868, training loss: 0.00481
Epoch: 1, step: 869, training loss: 0.00768
Epoch: 1, step: 870, training loss: 0.00377
Epoch: 1, step: 871, training loss: 0.00950
Epoch: 1, step: 872, training loss: 0.00694
Epoch: 1, step: 873, training loss: 0.00410
Epoch: 1, step: 874, training loss: 0.01252
Epoch: 1, step: 875, training loss: 0.00843
Epoch: 1, step: 876, training loss: 0.00397
Epoch: 1, step: 877, training loss: 0.00638
Epoch: 1, step: 878, training loss: 0.00491
Epoch: 1, step: 879, training loss: 0.00495
Epoch: 1, step: 880, training loss: 0.00863
Epoch: 1, step: 881, training loss: 0.00820
Epoch: 1, step: 882, training loss: 0.00617
Epoch: 1, step: 883, training loss: 0.00567
Epoch: 1, step: 884, training loss: 0.00298
Epoch: 1, step: 885, training loss: 0.00769
Epoch: 1, step: 886, training loss: 0.01239
Epoch: 1, step: 887, training loss: 0.00432
Epoch: 1, step: 888, training loss: 0.00989
Epoch: 1, step: 889, training loss: 0.00805
Epoch: 1, step: 890, training loss: 0.01032
Epoch: 1, step: 891, training loss: 0.00346
Epoch: 1, step: 892, training loss: 0.00629
Epoch: 1, step: 893, training loss: 0.00374
Epoch: 1, step: 894, training loss: 0.00555
Epoch: 1, step: 895, training loss: 0.00408
Epoch: 1, step: 896, training loss: 0.00283
Epoch: 1, step: 897, training loss: 0.00970
Epoch: 1, step: 898, training loss: 0.00341
Epoch: 1, step: 899, training loss: 0.00277
Epoch: 1, step: 900, training loss: 0.01192
Epoch: 1, step: 901, training loss: 0.00490
Epoch: 1, step: 902, training loss: 0.00741
Epoch: 1, step: 903, training loss: 0.00858
Epoch: 1, step: 904, training loss: 0.00574
Epoch: 1, step: 905, training loss: 0.00902
Epoch: 1, step: 906, training loss: 0.00453
Epoch: 1, step: 907, training loss: 0.00465
Epoch: 1, step: 908, training loss: 0.00625
Epoch: 1, step: 909, training loss: 0.00519
Epoch: 1, step: 910, training loss: 0.00794
Epoch: 1, step: 911, training loss: 0.00491
Epoch: 1, step: 912, training loss: 0.00288
Epoch: 1, step: 913, training loss: 0.00613
Epoch: 1, step: 914, training loss: 0.00710
Epoch: 1, step: 915, training loss: 0.00512
Epoch: 1, step: 916, training loss: 0.00552
Epoch: 1, step: 917, training loss: 0.00233
Epoch: 1, step: 918, training loss: 0.00860
Epoch: 1, step: 919, training loss: 0.00110
Epoch: 1, step: 920, training loss: 0.00640
Epoch: 1, step: 921, training loss: 0.00457
Epoch: 1, step: 922, training loss: 0.00845
Epoch: 1, step: 923, training loss: 0.00812
Epoch: 1, step: 924, training loss: 0.00386
Epoch: 1, step: 925, training loss: 0.00511
Epoch: 1, step: 926, training loss: 0.00583
Epoch: 1, step: 927, training loss: 0.00517
Epoch: 1, step: 928, training loss: 0.00175
Epoch: 1, step: 929, training loss: 0.00965
Epoch: 1, step: 930, training loss: 0.00596
Epoch: 1, step: 931, training loss: 0.00322
Epoch: 1, step: 932, training loss: 0.00610
Epoch: 1, step: 933, training loss: 0.00643
Epoch: 1, step: 934, training loss: 0.00579
Epoch: 1, step: 935, training loss: 0.00877
Epoch: 1, step: 936, training loss: 0.00821
Epoch: 1, step: 937, training loss: 0.00380
Epoch: 1, step: 938, training loss: 0.00650
Epoch: 1, step: 939, training loss: 0.00556
Epoch: 1, step: 940, training loss: 0.00604
Epoch: 1, step: 941, training loss: 0.00485
Epoch: 1, step: 942, training loss: 0.00569
Epoch: 1, step: 943, training loss: 0.00688
Epoch: 1, step: 944, training loss: 0.00279
Epoch: 1, step: 945, training loss: 0.00515
Epoch: 1, step: 946, training loss: 0.00791
Epoch: 1, step: 947, training loss: 0.00859
Epoch: 1, step: 948, training loss: 0.00790
Epoch: 1, step: 949, training loss: 0.00639
Epoch: 1, step: 950, training loss: 0.00584
Epoch: 1, step: 951, training loss: 0.01227
Epoch: 1, step: 952, training loss: 0.01126
Epoch: 1, step: 953, training loss: 0.00856
Epoch: 1, step: 954, training loss: 0.00597
Epoch: 1, average training loss: 0.00880
Epoch: 1, F1: 60.54571, average dev loss: 0.00550
Epoch: 2, step: 0, training loss: 0.00389
Epoch: 2, step: 1, training loss: 0.00965
Epoch: 2, step: 2, training loss: 0.00394
Epoch: 2, step: 3, training loss: 0.00819
Epoch: 2, step: 4, training loss: 0.00602
Epoch: 2, step: 5, training loss: 0.00472
Epoch: 2, step: 6, training loss: 0.00357
Epoch: 2, step: 7, training loss: 0.00867
Epoch: 2, step: 8, training loss: 0.00774
Epoch: 2, step: 9, training loss: 0.00886
Epoch: 2, step: 10, training loss: 0.00502
Epoch: 2, step: 11, training loss: 0.00528
Epoch: 2, step: 12, training loss: 0.00693
Epoch: 2, step: 13, training loss: 0.00398
Epoch: 2, step: 14, training loss: 0.00188
Epoch: 2, step: 15, training loss: 0.00375
Epoch: 2, step: 16, training loss: 0.00593
Epoch: 2, step: 17, training loss: 0.00177
Epoch: 2, step: 18, training loss: 0.00528
Epoch: 2, step: 19, training loss: 0.00433
Epoch: 2, step: 20, training loss: 0.00326
Epoch: 2, step: 21, training loss: 0.00376
Epoch: 2, step: 22, training loss: 0.00503
Epoch: 2, step: 23, training loss: 0.00568
Epoch: 2, step: 24, training loss: 0.00311
Epoch: 2, step: 25, training loss: 0.00443
Epoch: 2, step: 26, training loss: 0.00684
Epoch: 2, step: 27, training loss: 0.00768
Epoch: 2, step: 28, training loss: 0.00571
Epoch: 2, step: 29, training loss: 0.00265
Epoch: 2, step: 30, training loss: 0.00351
Epoch: 2, step: 31, training loss: 0.00904
Epoch: 2, step: 32, training loss: 0.00395
Epoch: 2, step: 33, training loss: 0.00373
Epoch: 2, step: 34, training loss: 0.01129
Epoch: 2, step: 35, training loss: 0.00641
Epoch: 2, step: 36, training loss: 0.00374
Epoch: 2, step: 37, training loss: 0.00487
Epoch: 2, step: 38, training loss: 0.00472
Epoch: 2, step: 39, training loss: 0.00369
Epoch: 2, step: 40, training loss: 0.00766
Epoch: 2, step: 41, training loss: 0.00420
Epoch: 2, step: 42, training loss: 0.00328
Epoch: 2, step: 43, training loss: 0.00248
Epoch: 2, step: 44, training loss: 0.00496
Epoch: 2, step: 45, training loss: 0.00715
Epoch: 2, step: 46, training loss: 0.00550
Epoch: 2, step: 47, training loss: 0.00756
Epoch: 2, step: 48, training loss: 0.00694
Epoch: 2, step: 49, training loss: 0.00294
Epoch: 2, step: 50, training loss: 0.00330
Epoch: 2, step: 51, training loss: 0.00343
Epoch: 2, step: 52, training loss: 0.00693
Epoch: 2, step: 53, training loss: 0.00315
Epoch: 2, step: 54, training loss: 0.00841
Epoch: 2, step: 55, training loss: 0.00833
Epoch: 2, step: 56, training loss: 0.00545
Epoch: 2, step: 57, training loss: 0.00764
Epoch: 2, step: 58, training loss: 0.00698
Epoch: 2, step: 59, training loss: 0.00842
Epoch: 2, step: 60, training loss: 0.00531
Epoch: 2, step: 61, training loss: 0.00361
Epoch: 2, step: 62, training loss: 0.00501
Epoch: 2, step: 63, training loss: 0.00450
Epoch: 2, step: 64, training loss: 0.00173
Epoch: 2, step: 65, training loss: 0.00215
Epoch: 2, step: 66, training loss: 0.00659
Epoch: 2, step: 67, training loss: 0.00764
Epoch: 2, step: 68, training loss: 0.00730
Epoch: 2, step: 69, training loss: 0.00341
Epoch: 2, step: 70, training loss: 0.00540
Epoch: 2, step: 71, training loss: 0.00229
Epoch: 2, step: 72, training loss: 0.00320
Epoch: 2, step: 73, training loss: 0.00614
Epoch: 2, step: 74, training loss: 0.00894
Epoch: 2, step: 75, training loss: 0.00686
Epoch: 2, step: 76, training loss: 0.01149
Epoch: 2, step: 77, training loss: 0.00668
Epoch: 2, step: 78, training loss: 0.00455
Epoch: 2, step: 79, training loss: 0.00660
Epoch: 2, step: 80, training loss: 0.00564
Epoch: 2, step: 81, training loss: 0.00376
Epoch: 2, step: 82, training loss: 0.00743
Epoch: 2, step: 83, training loss: 0.00269
Epoch: 2, step: 84, training loss: 0.00447
Epoch: 2, step: 85, training loss: 0.00213
Epoch: 2, step: 86, training loss: 0.00845
Epoch: 2, step: 87, training loss: 0.00723
Epoch: 2, step: 88, training loss: 0.00379
Epoch: 2, step: 89, training loss: 0.00649
Epoch: 2, step: 90, training loss: 0.00697
Epoch: 2, step: 91, training loss: 0.00459
Epoch: 2, step: 92, training loss: 0.00430
Epoch: 2, step: 93, training loss: 0.00395
Epoch: 2, step: 94, training loss: 0.00542
Epoch: 2, step: 95, training loss: 0.00412
Epoch: 2, step: 96, training loss: 0.00638
Epoch: 2, step: 97, training loss: 0.00582
Epoch: 2, step: 98, training loss: 0.00607
Epoch: 2, step: 99, training loss: 0.00914
Epoch: 2, step: 100, training loss: 0.00608
Epoch: 2, step: 101, training loss: 0.00480
Epoch: 2, step: 102, training loss: 0.00643
Epoch: 2, step: 103, training loss: 0.00296
Epoch: 2, step: 104, training loss: 0.00410
Epoch: 2, step: 105, training loss: 0.00390
Epoch: 2, step: 106, training loss: 0.00335
Epoch: 2, step: 107, training loss: 0.00383
Epoch: 2, step: 108, training loss: 0.00417
Epoch: 2, step: 109, training loss: 0.00702
Epoch: 2, step: 110, training loss: 0.00479
Epoch: 2, step: 111, training loss: 0.00727
Epoch: 2, step: 112, training loss: 0.00749
Epoch: 2, step: 113, training loss: 0.00718
Epoch: 2, step: 114, training loss: 0.00441
Epoch: 2, step: 115, training loss: 0.00820
Epoch: 2, step: 116, training loss: 0.00339
Epoch: 2, step: 117, training loss: 0.00311
Epoch: 2, step: 118, training loss: 0.00508
Epoch: 2, step: 119, training loss: 0.00441
Epoch: 2, step: 120, training loss: 0.00180
Epoch: 2, step: 121, training loss: 0.00733
Epoch: 2, step: 122, training loss: 0.00189
Epoch: 2, step: 123, training loss: 0.00471
Epoch: 2, step: 124, training loss: 0.00527
Epoch: 2, step: 125, training loss: 0.00415
Epoch: 2, step: 126, training loss: 0.00370
Epoch: 2, step: 127, training loss: 0.00619
Epoch: 2, step: 128, training loss: 0.00252
Epoch: 2, step: 129, training loss: 0.00708
Epoch: 2, step: 130, training loss: 0.00685
Epoch: 2, step: 131, training loss: 0.00248
Epoch: 2, step: 132, training loss: 0.00561
Epoch: 2, step: 133, training loss: 0.00364
Epoch: 2, step: 134, training loss: 0.00417
Epoch: 2, step: 135, training loss: 0.00351
Epoch: 2, step: 136, training loss: 0.00578
Epoch: 2, step: 137, training loss: 0.00392
Epoch: 2, step: 138, training loss: 0.01038
Epoch: 2, step: 139, training loss: 0.00690
Epoch: 2, step: 140, training loss: 0.00329
Epoch: 2, step: 141, training loss: 0.00583
Epoch: 2, step: 142, training loss: 0.00359
Epoch: 2, step: 143, training loss: 0.00348
Epoch: 2, step: 144, training loss: 0.00470
Epoch: 2, step: 145, training loss: 0.00649
Epoch: 2, step: 146, training loss: 0.00314
Epoch: 2, step: 147, training loss: 0.00356
Epoch: 2, step: 148, training loss: 0.00512
Epoch: 2, step: 149, training loss: 0.00532
Epoch: 2, step: 150, training loss: 0.01011
Epoch: 2, step: 151, training loss: 0.00645
Epoch: 2, step: 152, training loss: 0.00310
Epoch: 2, step: 153, training loss: 0.00603
Epoch: 2, step: 154, training loss: 0.00296
Epoch: 2, step: 155, training loss: 0.00548
Epoch: 2, step: 156, training loss: 0.00474
Epoch: 2, step: 157, training loss: 0.00245
Epoch: 2, step: 158, training loss: 0.00723
Epoch: 2, step: 159, training loss: 0.00552
Epoch: 2, step: 160, training loss: 0.00703
Epoch: 2, step: 161, training loss: 0.00435
Epoch: 2, step: 162, training loss: 0.00188
Epoch: 2, step: 163, training loss: 0.00189
Epoch: 2, step: 164, training loss: 0.00625
Epoch: 2, step: 165, training loss: 0.00546
Epoch: 2, step: 166, training loss: 0.00444
Epoch: 2, step: 167, training loss: 0.00758
Epoch: 2, step: 168, training loss: 0.00321
Epoch: 2, step: 169, training loss: 0.00797
Epoch: 2, step: 170, training loss: 0.00275
Epoch: 2, step: 171, training loss: 0.00387
Epoch: 2, step: 172, training loss: 0.00378
Epoch: 2, step: 173, training loss: 0.00209
Epoch: 2, step: 174, training loss: 0.00743
Epoch: 2, step: 175, training loss: 0.00526
Epoch: 2, step: 176, training loss: 0.00154
Epoch: 2, step: 177, training loss: 0.00310
Epoch: 2, step: 178, training loss: 0.00366
Epoch: 2, step: 179, training loss: 0.01130
Epoch: 2, step: 180, training loss: 0.00376
Epoch: 2, step: 181, training loss: 0.00431
Epoch: 2, step: 182, training loss: 0.00722
Epoch: 2, step: 183, training loss: 0.00305
Epoch: 2, step: 184, training loss: 0.00700
Epoch: 2, step: 185, training loss: 0.00440
Epoch: 2, step: 186, training loss: 0.00383
Epoch: 2, step: 187, training loss: 0.00436
Epoch: 2, step: 188, training loss: 0.00974
Epoch: 2, step: 189, training loss: 0.00321
Epoch: 2, step: 190, training loss: 0.00828
Epoch: 2, step: 191, training loss: 0.00721
Epoch: 2, step: 192, training loss: 0.00434
Epoch: 2, step: 193, training loss: 0.00511
Epoch: 2, step: 194, training loss: 0.00557
Epoch: 2, step: 195, training loss: 0.00257
Epoch: 2, step: 196, training loss: 0.00736
Epoch: 2, step: 197, training loss: 0.00535
Epoch: 2, step: 198, training loss: 0.00753
Epoch: 2, step: 199, training loss: 0.00379
Epoch: 2, step: 200, training loss: 0.00307
Epoch: 2, step: 201, training loss: 0.00279
Epoch: 2, step: 202, training loss: 0.00756
Epoch: 2, step: 203, training loss: 0.00582
Epoch: 2, step: 204, training loss: 0.00348
Epoch: 2, step: 205, training loss: 0.00653
Epoch: 2, step: 206, training loss: 0.00841
Epoch: 2, step: 207, training loss: 0.01002
Epoch: 2, step: 208, training loss: 0.00570
Epoch: 2, step: 209, training loss: 0.00340
Epoch: 2, step: 210, training loss: 0.00502
Epoch: 2, step: 211, training loss: 0.00866
Epoch: 2, step: 212, training loss: 0.00590
Epoch: 2, step: 213, training loss: 0.00661
Epoch: 2, step: 214, training loss: 0.00741
Epoch: 2, step: 215, training loss: 0.00635
Epoch: 2, step: 216, training loss: 0.00410
Epoch: 2, step: 217, training loss: 0.00297
Epoch: 2, step: 218, training loss: 0.00300
Epoch: 2, step: 219, training loss: 0.00758
Epoch: 2, step: 220, training loss: 0.00311
Epoch: 2, step: 221, training loss: 0.00465
Epoch: 2, step: 222, training loss: 0.00360
Epoch: 2, step: 223, training loss: 0.00546
Epoch: 2, step: 224, training loss: 0.00563
Epoch: 2, step: 225, training loss: 0.00240
Epoch: 2, step: 226, training loss: 0.00353
Epoch: 2, step: 227, training loss: 0.00514
Epoch: 2, step: 228, training loss: 0.00249
Epoch: 2, step: 229, training loss: 0.01077
Epoch: 2, step: 230, training loss: 0.00463
Epoch: 2, step: 231, training loss: 0.00358
Epoch: 2, step: 232, training loss: 0.00360
Epoch: 2, step: 233, training loss: 0.00289
Epoch: 2, step: 234, training loss: 0.00820
Epoch: 2, step: 235, training loss: 0.00547
Epoch: 2, step: 236, training loss: 0.00464
Epoch: 2, step: 237, training loss: 0.00520
Epoch: 2, step: 238, training loss: 0.00527
Epoch: 2, step: 239, training loss: 0.00647
Epoch: 2, step: 240, training loss: 0.00331
Epoch: 2, step: 241, training loss: 0.00857
Epoch: 2, step: 242, training loss: 0.00669
Epoch: 2, step: 243, training loss: 0.00435
Epoch: 2, step: 244, training loss: 0.00756
Epoch: 2, step: 245, training loss: 0.00613
Epoch: 2, step: 246, training loss: 0.00295
Epoch: 2, step: 247, training loss: 0.00375
Epoch: 2, step: 248, training loss: 0.00197
Epoch: 2, step: 249, training loss: 0.00406
Epoch: 2, step: 250, training loss: 0.00519
Epoch: 2, step: 251, training loss: 0.00977
Epoch: 2, step: 252, training loss: 0.00245
Epoch: 2, step: 253, training loss: 0.00495
Epoch: 2, step: 254, training loss: 0.00330
Epoch: 2, step: 255, training loss: 0.00329
Epoch: 2, step: 256, training loss: 0.00778
Epoch: 2, step: 257, training loss: 0.00413
Epoch: 2, step: 258, training loss: 0.00685
Epoch: 2, step: 259, training loss: 0.00890
Epoch: 2, step: 260, training loss: 0.00501
Epoch: 2, step: 261, training loss: 0.00615
Epoch: 2, step: 262, training loss: 0.00144
Epoch: 2, step: 263, training loss: 0.00524
Epoch: 2, step: 264, training loss: 0.00668
Epoch: 2, step: 265, training loss: 0.00206
Epoch: 2, step: 266, training loss: 0.00668
Epoch: 2, step: 267, training loss: 0.00469
Epoch: 2, step: 268, training loss: 0.00440
Epoch: 2, step: 269, training loss: 0.00791
Epoch: 2, step: 270, training loss: 0.00164
Epoch: 2, step: 271, training loss: 0.00526
Epoch: 2, step: 272, training loss: 0.00275
Epoch: 2, step: 273, training loss: 0.00531
Epoch: 2, step: 274, training loss: 0.00424
Epoch: 2, step: 275, training loss: 0.00730
Epoch: 2, step: 276, training loss: 0.00745
Epoch: 2, step: 277, training loss: 0.00565
Epoch: 2, step: 278, training loss: 0.00271
Epoch: 2, step: 279, training loss: 0.00456
Epoch: 2, step: 280, training loss: 0.00194
Epoch: 2, step: 281, training loss: 0.00330
Epoch: 2, step: 282, training loss: 0.00402
Epoch: 2, step: 283, training loss: 0.00295
Epoch: 2, step: 284, training loss: 0.00548
Epoch: 2, step: 285, training loss: 0.00470
Epoch: 2, step: 286, training loss: 0.00337
Epoch: 2, step: 287, training loss: 0.00201
Epoch: 2, step: 288, training loss: 0.00340
Epoch: 2, step: 289, training loss: 0.00370
Epoch: 2, step: 290, training loss: 0.00320
Epoch: 2, step: 291, training loss: 0.00750
Epoch: 2, step: 292, training loss: 0.00446
Epoch: 2, step: 293, training loss: 0.00570
Epoch: 2, step: 294, training loss: 0.00302
Epoch: 2, step: 295, training loss: 0.00331
Epoch: 2, step: 296, training loss: 0.00330
Epoch: 2, step: 297, training loss: 0.00599
Epoch: 2, step: 298, training loss: 0.00495
Epoch: 2, step: 299, training loss: 0.00337
Epoch: 2, step: 300, training loss: 0.00468
Epoch: 2, step: 301, training loss: 0.00517
Epoch: 2, step: 302, training loss: 0.00751
Epoch: 2, step: 303, training loss: 0.00605
Epoch: 2, step: 304, training loss: 0.00317
Epoch: 2, step: 305, training loss: 0.00287
Epoch: 2, step: 306, training loss: 0.00644
Epoch: 2, step: 307, training loss: 0.00299
Epoch: 2, step: 308, training loss: 0.00603
Epoch: 2, step: 309, training loss: 0.00566
Epoch: 2, step: 310, training loss: 0.00466
Epoch: 2, step: 311, training loss: 0.00562
Epoch: 2, step: 312, training loss: 0.00603
Epoch: 2, step: 313, training loss: 0.00369
Epoch: 2, step: 314, training loss: 0.00730
Epoch: 2, step: 315, training loss: 0.00604
Epoch: 2, step: 316, training loss: 0.00621
Epoch: 2, step: 317, training loss: 0.00621
Epoch: 2, step: 318, training loss: 0.00477
Epoch: 2, step: 319, training loss: 0.00341
Epoch: 2, step: 320, training loss: 0.00381
Epoch: 2, step: 321, training loss: 0.00541
Epoch: 2, step: 322, training loss: 0.00317
Epoch: 2, step: 323, training loss: 0.00407
Epoch: 2, step: 324, training loss: 0.00926
Epoch: 2, step: 325, training loss: 0.00328
Epoch: 2, step: 326, training loss: 0.00965
Epoch: 2, step: 327, training loss: 0.00611
Epoch: 2, step: 328, training loss: 0.00546
Epoch: 2, step: 329, training loss: 0.00547
Epoch: 2, step: 330, training loss: 0.00581
Epoch: 2, step: 331, training loss: 0.00938
Epoch: 2, step: 332, training loss: 0.00280
Epoch: 2, step: 333, training loss: 0.00179
Epoch: 2, step: 334, training loss: 0.00558
Epoch: 2, step: 335, training loss: 0.00232
Epoch: 2, step: 336, training loss: 0.00332
Epoch: 2, step: 337, training loss: 0.00641
Epoch: 2, step: 338, training loss: 0.00221
Epoch: 2, step: 339, training loss: 0.00554
Epoch: 2, step: 340, training loss: 0.00520
Epoch: 2, step: 341, training loss: 0.00625
Epoch: 2, step: 342, training loss: 0.00396
Epoch: 2, step: 343, training loss: 0.00596
Epoch: 2, step: 344, training loss: 0.00427
Epoch: 2, step: 345, training loss: 0.00190
Epoch: 2, step: 346, training loss: 0.00674
Epoch: 2, step: 347, training loss: 0.00347
Epoch: 2, step: 348, training loss: 0.00263
Epoch: 2, step: 349, training loss: 0.00490
Epoch: 2, step: 350, training loss: 0.00569
Epoch: 2, step: 351, training loss: 0.00339
Epoch: 2, step: 352, training loss: 0.00265
Epoch: 2, step: 353, training loss: 0.00888
Epoch: 2, step: 354, training loss: 0.00316
Epoch: 2, step: 355, training loss: 0.00429
Epoch: 2, step: 356, training loss: 0.00371
Epoch: 2, step: 357, training loss: 0.00384
Epoch: 2, step: 358, training loss: 0.00344
Epoch: 2, step: 359, training loss: 0.00263
Epoch: 2, step: 360, training loss: 0.00435
Epoch: 2, step: 361, training loss: 0.00507
Epoch: 2, step: 362, training loss: 0.00487
Epoch: 2, step: 363, training loss: 0.00404
Epoch: 2, step: 364, training loss: 0.00135
Epoch: 2, step: 365, training loss: 0.00933
Epoch: 2, step: 366, training loss: 0.00408
Epoch: 2, step: 367, training loss: 0.00316
Epoch: 2, step: 368, training loss: 0.00561
Epoch: 2, step: 369, training loss: 0.00765
Epoch: 2, step: 370, training loss: 0.00702
Epoch: 2, step: 371, training loss: 0.00380
Epoch: 2, step: 372, training loss: 0.00674
Epoch: 2, step: 373, training loss: 0.00298
Epoch: 2, step: 374, training loss: 0.00416
Epoch: 2, step: 375, training loss: 0.00459
Epoch: 2, step: 376, training loss: 0.00562
Epoch: 2, step: 377, training loss: 0.00498
Epoch: 2, step: 378, training loss: 0.00452
Epoch: 2, step: 379, training loss: 0.00194
Epoch: 2, step: 380, training loss: 0.00637
Epoch: 2, step: 381, training loss: 0.00457
Epoch: 2, step: 382, training loss: 0.00644
Epoch: 2, step: 383, training loss: 0.00386
Epoch: 2, step: 384, training loss: 0.00611
Epoch: 2, step: 385, training loss: 0.00395
Epoch: 2, step: 386, training loss: 0.00530
Epoch: 2, step: 387, training loss: 0.00599
Epoch: 2, step: 388, training loss: 0.00586
Epoch: 2, step: 389, training loss: 0.00736
Epoch: 2, step: 390, training loss: 0.00356
Epoch: 2, step: 391, training loss: 0.00228
Epoch: 2, step: 392, training loss: 0.00488
Epoch: 2, step: 393, training loss: 0.00491
Epoch: 2, step: 394, training loss: 0.00275
Epoch: 2, step: 395, training loss: 0.00469
Epoch: 2, step: 396, training loss: 0.00383
Epoch: 2, step: 397, training loss: 0.00626
Epoch: 2, step: 398, training loss: 0.00688
Epoch: 2, step: 399, training loss: 0.00507
Epoch: 2, step: 400, training loss: 0.00552
Epoch: 2, step: 401, training loss: 0.00301
Epoch: 2, step: 402, training loss: 0.00370
Epoch: 2, step: 403, training loss: 0.00411
Epoch: 2, step: 404, training loss: 0.00503
Epoch: 2, step: 405, training loss: 0.00073
Epoch: 2, step: 406, training loss: 0.00538
Epoch: 2, step: 407, training loss: 0.00458
Epoch: 2, step: 408, training loss: 0.00404
Epoch: 2, step: 409, training loss: 0.00628
Epoch: 2, step: 410, training loss: 0.00269
Epoch: 2, step: 411, training loss: 0.00559
Epoch: 2, step: 412, training loss: 0.00638
Epoch: 2, step: 413, training loss: 0.00173
Epoch: 2, step: 414, training loss: 0.00429
Epoch: 2, step: 415, training loss: 0.00457
Epoch: 2, step: 416, training loss: 0.00471
Epoch: 2, step: 417, training loss: 0.00627
Epoch: 2, step: 418, training loss: 0.00730
Epoch: 2, step: 419, training loss: 0.00285
Epoch: 2, step: 420, training loss: 0.00843
Epoch: 2, step: 421, training loss: 0.00418
Epoch: 2, step: 422, training loss: 0.00375
Epoch: 2, step: 423, training loss: 0.00300
Epoch: 2, step: 424, training loss: 0.00503
Epoch: 2, step: 425, training loss: 0.00408
Epoch: 2, step: 426, training loss: 0.00532
Epoch: 2, step: 427, training loss: 0.00882
Epoch: 2, step: 428, training loss: 0.00271
Epoch: 2, step: 429, training loss: 0.00634
Epoch: 2, step: 430, training loss: 0.00560
Epoch: 2, step: 431, training loss: 0.00357
Epoch: 2, step: 432, training loss: 0.00507
Epoch: 2, step: 433, training loss: 0.00369
Epoch: 2, step: 434, training loss: 0.00188
Epoch: 2, step: 435, training loss: 0.00433
Epoch: 2, step: 436, training loss: 0.00516
Epoch: 2, step: 437, training loss: 0.00692
Epoch: 2, step: 438, training loss: 0.00239
Epoch: 2, step: 439, training loss: 0.00345
Epoch: 2, step: 440, training loss: 0.00325
Epoch: 2, step: 441, training loss: 0.00481
Epoch: 2, step: 442, training loss: 0.00474
Epoch: 2, step: 443, training loss: 0.00344
Epoch: 2, step: 444, training loss: 0.00627
Epoch: 2, step: 445, training loss: 0.00498
Epoch: 2, step: 446, training loss: 0.00428
Epoch: 2, step: 447, training loss: 0.00880
Epoch: 2, step: 448, training loss: 0.00414
Epoch: 2, step: 449, training loss: 0.00363
Epoch: 2, step: 450, training loss: 0.00418
Epoch: 2, step: 451, training loss: 0.00512
Epoch: 2, step: 452, training loss: 0.00381
Epoch: 2, step: 453, training loss: 0.00631
Epoch: 2, step: 454, training loss: 0.00406
Epoch: 2, step: 455, training loss: 0.00384
Epoch: 2, step: 456, training loss: 0.00408
Epoch: 2, step: 457, training loss: 0.00249
Epoch: 2, step: 458, training loss: 0.00459
Epoch: 2, step: 459, training loss: 0.00478
Epoch: 2, step: 460, training loss: 0.00425
Epoch: 2, step: 461, training loss: 0.00413
Epoch: 2, step: 462, training loss: 0.00337
Epoch: 2, step: 463, training loss: 0.00388
Epoch: 2, step: 464, training loss: 0.00489
Epoch: 2, step: 465, training loss: 0.00371
Epoch: 2, step: 466, training loss: 0.00363
Epoch: 2, step: 467, training loss: 0.00667
Epoch: 2, step: 468, training loss: 0.00372
Epoch: 2, step: 469, training loss: 0.00332
Epoch: 2, step: 470, training loss: 0.00226
Epoch: 2, step: 471, training loss: 0.00784
Epoch: 2, step: 472, training loss: 0.00321
Epoch: 2, step: 473, training loss: 0.00353
Epoch: 2, step: 474, training loss: 0.00444
Epoch: 2, step: 475, training loss: 0.00664
Epoch: 2, step: 476, training loss: 0.00229
Epoch: 2, step: 477, training loss: 0.00351
Epoch: 2, step: 478, training loss: 0.00802
Epoch: 2, step: 479, training loss: 0.00440
Epoch: 2, step: 480, training loss: 0.00323
Epoch: 2, step: 481, training loss: 0.00280
Epoch: 2, step: 482, training loss: 0.00364
Epoch: 2, step: 483, training loss: 0.00513
Epoch: 2, step: 484, training loss: 0.00585
Epoch: 2, step: 485, training loss: 0.00249
Epoch: 2, step: 486, training loss: 0.00451
Epoch: 2, step: 487, training loss: 0.00780
Epoch: 2, step: 488, training loss: 0.00385
Epoch: 2, step: 489, training loss: 0.00411
Epoch: 2, step: 490, training loss: 0.00428
Epoch: 2, step: 491, training loss: 0.00192
Epoch: 2, step: 492, training loss: 0.00110
Epoch: 2, step: 493, training loss: 0.00964
Epoch: 2, step: 494, training loss: 0.00395
Epoch: 2, step: 495, training loss: 0.00454
Epoch: 2, step: 496, training loss: 0.00249
Epoch: 2, step: 497, training loss: 0.00860
Epoch: 2, step: 498, training loss: 0.00282
Epoch: 2, step: 499, training loss: 0.00218
Epoch: 2, step: 500, training loss: 0.00605
Epoch: 2, step: 501, training loss: 0.00368
Epoch: 2, step: 502, training loss: 0.00386
Epoch: 2, step: 503, training loss: 0.00782
Epoch: 2, step: 504, training loss: 0.00625
Epoch: 2, step: 505, training loss: 0.00518
Epoch: 2, step: 506, training loss: 0.00259
Epoch: 2, step: 507, training loss: 0.00166
Epoch: 2, step: 508, training loss: 0.00309
Epoch: 2, step: 509, training loss: 0.00496
Epoch: 2, step: 510, training loss: 0.00399
Epoch: 2, step: 511, training loss: 0.00535
Epoch: 2, step: 512, training loss: 0.00457
Epoch: 2, step: 513, training loss: 0.00674
Epoch: 2, step: 514, training loss: 0.00451
Epoch: 2, step: 515, training loss: 0.00445
Epoch: 2, step: 516, training loss: 0.00248
Epoch: 2, step: 517, training loss: 0.00311
Epoch: 2, step: 518, training loss: 0.00257
Epoch: 2, step: 519, training loss: 0.00622
Epoch: 2, step: 520, training loss: 0.00679
Epoch: 2, step: 521, training loss: 0.00389
Epoch: 2, step: 522, training loss: 0.00193
Epoch: 2, step: 523, training loss: 0.00255
Epoch: 2, step: 524, training loss: 0.00390
Epoch: 2, step: 525, training loss: 0.00229
Epoch: 2, step: 526, training loss: 0.00686
Epoch: 2, step: 527, training loss: 0.00584
Epoch: 2, step: 528, training loss: 0.00282
Epoch: 2, step: 529, training loss: 0.00445
Epoch: 2, step: 530, training loss: 0.00444
Epoch: 2, step: 531, training loss: 0.00651
Epoch: 2, step: 532, training loss: 0.00390
Epoch: 2, step: 533, training loss: 0.00608
Epoch: 2, step: 534, training loss: 0.00314
Epoch: 2, step: 535, training loss: 0.00763
Epoch: 2, step: 536, training loss: 0.00353
Epoch: 2, step: 537, training loss: 0.00266
Epoch: 2, step: 538, training loss: 0.00278
Epoch: 2, step: 539, training loss: 0.00423
Epoch: 2, step: 540, training loss: 0.00508
Epoch: 2, step: 541, training loss: 0.00364
Epoch: 2, step: 542, training loss: 0.00510
Epoch: 2, step: 543, training loss: 0.00357
Epoch: 2, step: 544, training loss: 0.00330
Epoch: 2, step: 545, training loss: 0.00633
Epoch: 2, step: 546, training loss: 0.00413
Epoch: 2, step: 547, training loss: 0.00560
Epoch: 2, step: 548, training loss: 0.00947
Epoch: 2, step: 549, training loss: 0.00205
Epoch: 2, step: 550, training loss: 0.00656
Epoch: 2, step: 551, training loss: 0.00102
Epoch: 2, step: 552, training loss: 0.00501
Epoch: 2, step: 553, training loss: 0.00443
Epoch: 2, step: 554, training loss: 0.00464
Epoch: 2, step: 555, training loss: 0.00145
Epoch: 2, step: 556, training loss: 0.00403
Epoch: 2, step: 557, training loss: 0.00134
Epoch: 2, step: 558, training loss: 0.00218
Epoch: 2, step: 559, training loss: 0.00344
Epoch: 2, step: 560, training loss: 0.00332
Epoch: 2, step: 561, training loss: 0.00305
Epoch: 2, step: 562, training loss: 0.00608
Epoch: 2, step: 563, training loss: 0.00382
Epoch: 2, step: 564, training loss: 0.00373
Epoch: 2, step: 565, training loss: 0.00419
Epoch: 2, step: 566, training loss: 0.00494
Epoch: 2, step: 567, training loss: 0.00502
Epoch: 2, step: 568, training loss: 0.00431
Epoch: 2, step: 569, training loss: 0.00378
Epoch: 2, step: 570, training loss: 0.00589
Epoch: 2, step: 571, training loss: 0.00330
Epoch: 2, step: 572, training loss: 0.00331
Epoch: 2, step: 573, training loss: 0.00438
Epoch: 2, step: 574, training loss: 0.00689
Epoch: 2, step: 575, training loss: 0.00387
Epoch: 2, step: 576, training loss: 0.00314
Epoch: 2, step: 577, training loss: 0.00206
Epoch: 2, step: 578, training loss: 0.00839
Epoch: 2, step: 579, training loss: 0.00266
Epoch: 2, step: 580, training loss: 0.00656
Epoch: 2, step: 581, training loss: 0.00316
Epoch: 2, step: 582, training loss: 0.00236
Epoch: 2, step: 583, training loss: 0.00904
Epoch: 2, step: 584, training loss: 0.00581
Epoch: 2, step: 585, training loss: 0.00281
Epoch: 2, step: 586, training loss: 0.00688
Epoch: 2, step: 587, training loss: 0.00446
Epoch: 2, step: 588, training loss: 0.00411
Epoch: 2, step: 589, training loss: 0.00398
Epoch: 2, step: 590, training loss: 0.00301
Epoch: 2, step: 591, training loss: 0.00354
Epoch: 2, step: 592, training loss: 0.00331
Epoch: 2, step: 593, training loss: 0.00408
Epoch: 2, step: 594, training loss: 0.00435
Epoch: 2, step: 595, training loss: 0.00672
Epoch: 2, step: 596, training loss: 0.00352
Epoch: 2, step: 597, training loss: 0.00927
Epoch: 2, step: 598, training loss: 0.00509
Epoch: 2, step: 599, training loss: 0.00537
Epoch: 2, step: 600, training loss: 0.00259
Epoch: 2, step: 601, training loss: 0.00293
Epoch: 2, step: 602, training loss: 0.00092
Epoch: 2, step: 603, training loss: 0.00454
Epoch: 2, step: 604, training loss: 0.00313
Epoch: 2, step: 605, training loss: 0.00358
Epoch: 2, step: 606, training loss: 0.00244
Epoch: 2, step: 607, training loss: 0.00221
Epoch: 2, step: 608, training loss: 0.00212
Epoch: 2, step: 609, training loss: 0.00373
Epoch: 2, step: 610, training loss: 0.00479
Epoch: 2, step: 611, training loss: 0.00446
Epoch: 2, step: 612, training loss: 0.00296
Epoch: 2, step: 613, training loss: 0.00227
Epoch: 2, step: 614, training loss: 0.00409
Epoch: 2, step: 615, training loss: 0.00402
Epoch: 2, step: 616, training loss: 0.00347
Epoch: 2, step: 617, training loss: 0.00370
Epoch: 2, step: 618, training loss: 0.00364
Epoch: 2, step: 619, training loss: 0.00466
Epoch: 2, step: 620, training loss: 0.00430
Epoch: 2, step: 621, training loss: 0.00288
Epoch: 2, step: 622, training loss: 0.00426
Epoch: 2, step: 623, training loss: 0.00307
Epoch: 2, step: 624, training loss: 0.00411
Epoch: 2, step: 625, training loss: 0.00437
Epoch: 2, step: 626, training loss: 0.00368
Epoch: 2, step: 627, training loss: 0.00331
Epoch: 2, step: 628, training loss: 0.00645
Epoch: 2, step: 629, training loss: 0.00529
Epoch: 2, step: 630, training loss: 0.00753
Epoch: 2, step: 631, training loss: 0.00560
Epoch: 2, step: 632, training loss: 0.00325
Epoch: 2, step: 633, training loss: 0.00307
Epoch: 2, step: 634, training loss: 0.00362
Epoch: 2, step: 635, training loss: 0.00245
Epoch: 2, step: 636, training loss: 0.00343
Epoch: 2, step: 637, training loss: 0.00344
Epoch: 2, step: 638, training loss: 0.00232
Epoch: 2, step: 639, training loss: 0.00195
Epoch: 2, step: 640, training loss: 0.00131
Epoch: 2, step: 641, training loss: 0.00390
Epoch: 2, step: 642, training loss: 0.00298
Epoch: 2, step: 643, training loss: 0.00326
Epoch: 2, step: 644, training loss: 0.00450
Epoch: 2, step: 645, training loss: 0.00762
Epoch: 2, step: 646, training loss: 0.00372
Epoch: 2, step: 647, training loss: 0.00262
Epoch: 2, step: 648, training loss: 0.00516
Epoch: 2, step: 649, training loss: 0.00236
Epoch: 2, step: 650, training loss: 0.00241
Epoch: 2, step: 651, training loss: 0.00198
Epoch: 2, step: 652, training loss: 0.00286
Epoch: 2, step: 653, training loss: 0.00287
Epoch: 2, step: 654, training loss: 0.00509
Epoch: 2, step: 655, training loss: 0.00408
Epoch: 2, step: 656, training loss: 0.00202
Epoch: 2, step: 657, training loss: 0.00273
Epoch: 2, step: 658, training loss: 0.00324
Epoch: 2, step: 659, training loss: 0.00499
Epoch: 2, step: 660, training loss: 0.00313
Epoch: 2, step: 661, training loss: 0.00670
Epoch: 2, step: 662, training loss: 0.00122
Epoch: 2, step: 663, training loss: 0.00482
Epoch: 2, step: 664, training loss: 0.00231
Epoch: 2, step: 665, training loss: 0.00374
Epoch: 2, step: 666, training loss: 0.00157
Epoch: 2, step: 667, training loss: 0.00777
Epoch: 2, step: 668, training loss: 0.00574
Epoch: 2, step: 669, training loss: 0.00330
Epoch: 2, step: 670, training loss: 0.00236
Epoch: 2, step: 671, training loss: 0.00335
Epoch: 2, step: 672, training loss: 0.00124
Epoch: 2, step: 673, training loss: 0.00310
Epoch: 2, step: 674, training loss: 0.00212
Epoch: 2, step: 675, training loss: 0.00427
Epoch: 2, step: 676, training loss: 0.00214
Epoch: 2, step: 677, training loss: 0.00382
Epoch: 2, step: 678, training loss: 0.00175
Epoch: 2, step: 679, training loss: 0.00499
Epoch: 2, step: 680, training loss: 0.00688
Epoch: 2, step: 681, training loss: 0.00716
Epoch: 2, step: 682, training loss: 0.00270
Epoch: 2, step: 683, training loss: 0.00179
Epoch: 2, step: 684, training loss: 0.00484
Epoch: 2, step: 685, training loss: 0.00356
Epoch: 2, step: 686, training loss: 0.00476
Epoch: 2, step: 687, training loss: 0.00322
Epoch: 2, step: 688, training loss: 0.00630
Epoch: 2, step: 689, training loss: 0.01037
Epoch: 2, step: 690, training loss: 0.00227
Epoch: 2, step: 691, training loss: 0.00293
Epoch: 2, step: 692, training loss: 0.00440
Epoch: 2, step: 693, training loss: 0.00459
Epoch: 2, step: 694, training loss: 0.00383
Epoch: 2, step: 695, training loss: 0.00387
Epoch: 2, step: 696, training loss: 0.00561
Epoch: 2, step: 697, training loss: 0.00587
Epoch: 2, step: 698, training loss: 0.00316
Epoch: 2, step: 699, training loss: 0.00277
Epoch: 2, step: 700, training loss: 0.00241
Epoch: 2, step: 701, training loss: 0.00263
Epoch: 2, step: 702, training loss: 0.00515
Epoch: 2, step: 703, training loss: 0.00603
Epoch: 2, step: 704, training loss: 0.00590
Epoch: 2, step: 705, training loss: 0.00558
Epoch: 2, step: 706, training loss: 0.00256
Epoch: 2, step: 707, training loss: 0.00299
Epoch: 2, step: 708, training loss: 0.00452
Epoch: 2, step: 709, training loss: 0.00424
Epoch: 2, step: 710, training loss: 0.00788
Epoch: 2, step: 711, training loss: 0.00197
Epoch: 2, step: 712, training loss: 0.00568
Epoch: 2, step: 713, training loss: 0.00696
Epoch: 2, step: 714, training loss: 0.00357
Epoch: 2, step: 715, training loss: 0.00663
Epoch: 2, step: 716, training loss: 0.00712
Epoch: 2, step: 717, training loss: 0.00356
Epoch: 2, step: 718, training loss: 0.00378
Epoch: 2, step: 719, training loss: 0.00609
Epoch: 2, step: 720, training loss: 0.00310
Epoch: 2, step: 721, training loss: 0.00343
Epoch: 2, step: 722, training loss: 0.00261
Epoch: 2, step: 723, training loss: 0.00209
Epoch: 2, step: 724, training loss: 0.00368
Epoch: 2, step: 725, training loss: 0.00286
Epoch: 2, step: 726, training loss: 0.00177
Epoch: 2, step: 727, training loss: 0.00376
Epoch: 2, step: 728, training loss: 0.00286
Epoch: 2, step: 729, training loss: 0.00666
Epoch: 2, step: 730, training loss: 0.00413
Epoch: 2, step: 731, training loss: 0.01665
Epoch: 2, step: 732, training loss: 0.00365
Epoch: 2, step: 733, training loss: 0.00271
Epoch: 2, step: 734, training loss: 0.00386
Epoch: 2, step: 735, training loss: 0.00289
Epoch: 2, step: 736, training loss: 0.00311
Epoch: 2, step: 737, training loss: 0.00228
Epoch: 2, step: 738, training loss: 0.00176
Epoch: 2, step: 739, training loss: 0.00974
Epoch: 2, step: 740, training loss: 0.00560
Epoch: 2, step: 741, training loss: 0.00274
Epoch: 2, step: 742, training loss: 0.00574
Epoch: 2, step: 743, training loss: 0.00535
Epoch: 2, step: 744, training loss: 0.00293
Epoch: 2, step: 745, training loss: 0.00282
Epoch: 2, step: 746, training loss: 0.00512
Epoch: 2, step: 747, training loss: 0.00696
Epoch: 2, step: 748, training loss: 0.00845
Epoch: 2, step: 749, training loss: 0.00283
Epoch: 2, step: 750, training loss: 0.00429
Epoch: 2, step: 751, training loss: 0.00379
Epoch: 2, step: 752, training loss: 0.00518
Epoch: 2, step: 753, training loss: 0.00278
Epoch: 2, step: 754, training loss: 0.00633
Epoch: 2, step: 755, training loss: 0.00384
Epoch: 2, step: 756, training loss: 0.00440
Epoch: 2, step: 757, training loss: 0.00435
Epoch: 2, step: 758, training loss: 0.00519
Epoch: 2, step: 759, training loss: 0.00114
Epoch: 2, step: 760, training loss: 0.00363
Epoch: 2, step: 761, training loss: 0.00627
Epoch: 2, step: 762, training loss: 0.00983
Epoch: 2, step: 763, training loss: 0.00664
Epoch: 2, step: 764, training loss: 0.00771
Epoch: 2, step: 765, training loss: 0.00247
Epoch: 2, step: 766, training loss: 0.00329
Epoch: 2, step: 767, training loss: 0.00256
Epoch: 2, step: 768, training loss: 0.00548
Epoch: 2, step: 769, training loss: 0.00748
Epoch: 2, step: 770, training loss: 0.00564
Epoch: 2, step: 771, training loss: 0.00289
Epoch: 2, step: 772, training loss: 0.00416
Epoch: 2, step: 773, training loss: 0.00322
Epoch: 2, step: 774, training loss: 0.00356
Epoch: 2, step: 775, training loss: 0.00283
Epoch: 2, step: 776, training loss: 0.00153
Epoch: 2, step: 777, training loss: 0.00814
Epoch: 2, step: 778, training loss: 0.00385
Epoch: 2, step: 779, training loss: 0.00477
Epoch: 2, step: 780, training loss: 0.00376
Epoch: 2, step: 781, training loss: 0.00479
Epoch: 2, step: 782, training loss: 0.00393
Epoch: 2, step: 783, training loss: 0.00268
Epoch: 2, step: 784, training loss: 0.00240
Epoch: 2, step: 785, training loss: 0.00691
Epoch: 2, step: 786, training loss: 0.00959
Epoch: 2, step: 787, training loss: 0.00469
Epoch: 2, step: 788, training loss: 0.00280
Epoch: 2, step: 789, training loss: 0.00670
Epoch: 2, step: 790, training loss: 0.00222
Epoch: 2, step: 791, training loss: 0.00273
Epoch: 2, step: 792, training loss: 0.00456
Epoch: 2, step: 793, training loss: 0.00175
Epoch: 2, step: 794, training loss: 0.00266
Epoch: 2, step: 795, training loss: 0.01485
Epoch: 2, step: 796, training loss: 0.00450
Epoch: 2, step: 797, training loss: 0.00639
Epoch: 2, step: 798, training loss: 0.00394
Epoch: 2, step: 799, training loss: 0.00264
Epoch: 2, step: 800, training loss: 0.00293
Epoch: 2, step: 801, training loss: 0.00589
Epoch: 2, step: 802, training loss: 0.00191
Epoch: 2, step: 803, training loss: 0.00295
Epoch: 2, step: 804, training loss: 0.00715
Epoch: 2, step: 805, training loss: 0.00217
Epoch: 2, step: 806, training loss: 0.00303
Epoch: 2, step: 807, training loss: 0.00948
Epoch: 2, step: 808, training loss: 0.00264
Epoch: 2, step: 809, training loss: 0.00519
Epoch: 2, step: 810, training loss: 0.00304
Epoch: 2, step: 811, training loss: 0.00431
Epoch: 2, step: 812, training loss: 0.00325
Epoch: 2, step: 813, training loss: 0.00406
Epoch: 2, step: 814, training loss: 0.00425
Epoch: 2, step: 815, training loss: 0.00585
Epoch: 2, step: 816, training loss: 0.00418
Epoch: 2, step: 817, training loss: 0.00259
Epoch: 2, step: 818, training loss: 0.00271
Epoch: 2, step: 819, training loss: 0.00367
Epoch: 2, step: 820, training loss: 0.00469
Epoch: 2, step: 821, training loss: 0.01072
Epoch: 2, step: 822, training loss: 0.00536
Epoch: 2, step: 823, training loss: 0.00266
Epoch: 2, step: 824, training loss: 0.00909
Epoch: 2, step: 825, training loss: 0.00474
Epoch: 2, step: 826, training loss: 0.00280
Epoch: 2, step: 827, training loss: 0.00260
Epoch: 2, step: 828, training loss: 0.00269
Epoch: 2, step: 829, training loss: 0.00335
Epoch: 2, step: 830, training loss: 0.00745
Epoch: 2, step: 831, training loss: 0.00203
Epoch: 2, step: 832, training loss: 0.00371
Epoch: 2, step: 833, training loss: 0.00187
Epoch: 2, step: 834, training loss: 0.00395
Epoch: 2, step: 835, training loss: 0.00782
Epoch: 2, step: 836, training loss: 0.00161
Epoch: 2, step: 837, training loss: 0.00515
Epoch: 2, step: 838, training loss: 0.00471
Epoch: 2, step: 839, training loss: 0.00433
Epoch: 2, step: 840, training loss: 0.00284
Epoch: 2, step: 841, training loss: 0.00335
Epoch: 2, step: 842, training loss: 0.00128
Epoch: 2, step: 843, training loss: 0.00564
Epoch: 2, step: 844, training loss: 0.00381
Epoch: 2, step: 845, training loss: 0.00240
Epoch: 2, step: 846, training loss: 0.00982
Epoch: 2, step: 847, training loss: 0.00207
Epoch: 2, step: 848, training loss: 0.00585
Epoch: 2, step: 849, training loss: 0.00152
Epoch: 2, step: 850, training loss: 0.00454
Epoch: 2, step: 851, training loss: 0.00335
Epoch: 2, step: 852, training loss: 0.00110
Epoch: 2, step: 853, training loss: 0.00175
Epoch: 2, step: 854, training loss: 0.00813
Epoch: 2, step: 855, training loss: 0.00316
Epoch: 2, step: 856, training loss: 0.00270
Epoch: 2, step: 857, training loss: 0.00600
Epoch: 2, step: 858, training loss: 0.00322
Epoch: 2, step: 859, training loss: 0.00301
Epoch: 2, step: 860, training loss: 0.00212
Epoch: 2, step: 861, training loss: 0.00229
Epoch: 2, step: 862, training loss: 0.00244
Epoch: 2, step: 863, training loss: 0.00133
Epoch: 2, step: 864, training loss: 0.00207
Epoch: 2, step: 865, training loss: 0.00481
Epoch: 2, step: 866, training loss: 0.00334
Epoch: 2, step: 867, training loss: 0.00269
Epoch: 2, step: 868, training loss: 0.00385
Epoch: 2, step: 869, training loss: 0.00274
Epoch: 2, step: 870, training loss: 0.00392
Epoch: 2, step: 871, training loss: 0.00300
Epoch: 2, step: 872, training loss: 0.00672
Epoch: 2, step: 873, training loss: 0.00369
Epoch: 2, step: 874, training loss: 0.00449
Epoch: 2, step: 875, training loss: 0.00297
Epoch: 2, step: 876, training loss: 0.00144
Epoch: 2, step: 877, training loss: 0.00567
Epoch: 2, step: 878, training loss: 0.00370
Epoch: 2, step: 879, training loss: 0.00561
Epoch: 2, step: 880, training loss: 0.00914
Epoch: 2, step: 881, training loss: 0.00598
Epoch: 2, step: 882, training loss: 0.00153
Epoch: 2, step: 883, training loss: 0.00602
Epoch: 2, step: 884, training loss: 0.00261
Epoch: 2, step: 885, training loss: 0.00296
Epoch: 2, step: 886, training loss: 0.00301
Epoch: 2, step: 887, training loss: 0.00245
Epoch: 2, step: 888, training loss: 0.00142
Epoch: 2, step: 889, training loss: 0.00513
Epoch: 2, step: 890, training loss: 0.00447
Epoch: 2, step: 891, training loss: 0.00123
Epoch: 2, step: 892, training loss: 0.00494
Epoch: 2, step: 893, training loss: 0.00678
Epoch: 2, step: 894, training loss: 0.00472
Epoch: 2, step: 895, training loss: 0.00249
Epoch: 2, step: 896, training loss: 0.00592
Epoch: 2, step: 897, training loss: 0.00271
Epoch: 2, step: 898, training loss: 0.00308
Epoch: 2, step: 899, training loss: 0.00310
Epoch: 2, step: 900, training loss: 0.00237
Epoch: 2, step: 901, training loss: 0.00389
Epoch: 2, step: 902, training loss: 0.00272
Epoch: 2, step: 903, training loss: 0.00151
Epoch: 2, step: 904, training loss: 0.00283
Epoch: 2, step: 905, training loss: 0.00177
Epoch: 2, step: 906, training loss: 0.00199
Epoch: 2, step: 907, training loss: 0.00233
Epoch: 2, step: 908, training loss: 0.00318
Epoch: 2, step: 909, training loss: 0.00245
Epoch: 2, step: 910, training loss: 0.00161
Epoch: 2, step: 911, training loss: 0.00596
Epoch: 2, step: 912, training loss: 0.00620
Epoch: 2, step: 913, training loss: 0.00349
Epoch: 2, step: 914, training loss: 0.00543
Epoch: 2, step: 915, training loss: 0.00499
Epoch: 2, step: 916, training loss: 0.00256
Epoch: 2, step: 917, training loss: 0.00377
Epoch: 2, step: 918, training loss: 0.00257
Epoch: 2, step: 919, training loss: 0.00539
Epoch: 2, step: 920, training loss: 0.00265
Epoch: 2, step: 921, training loss: 0.00147
Epoch: 2, step: 922, training loss: 0.00117
Epoch: 2, step: 923, training loss: 0.00320
Epoch: 2, step: 924, training loss: 0.00458
Epoch: 2, step: 925, training loss: 0.00223
Epoch: 2, step: 926, training loss: 0.00172
Epoch: 2, step: 927, training loss: 0.00463
Epoch: 2, step: 928, training loss: 0.00533
Epoch: 2, step: 929, training loss: 0.00479
Epoch: 2, step: 930, training loss: 0.00197
Epoch: 2, step: 931, training loss: 0.00169
Epoch: 2, step: 932, training loss: 0.00331
Epoch: 2, step: 933, training loss: 0.00365
Epoch: 2, step: 934, training loss: 0.00130
Epoch: 2, step: 935, training loss: 0.00354
Epoch: 2, step: 936, training loss: 0.00213
Epoch: 2, step: 937, training loss: 0.00514
Epoch: 2, step: 938, training loss: 0.00619
Epoch: 2, step: 939, training loss: 0.00114
Epoch: 2, step: 940, training loss: 0.00722
Epoch: 2, step: 941, training loss: 0.00195
Epoch: 2, step: 942, training loss: 0.00683
Epoch: 2, step: 943, training loss: 0.00219
Epoch: 2, step: 944, training loss: 0.00161
Epoch: 2, step: 945, training loss: 0.00281
Epoch: 2, step: 946, training loss: 0.00606
Epoch: 2, step: 947, training loss: 0.00477
Epoch: 2, step: 948, training loss: 0.00158
Epoch: 2, step: 949, training loss: 0.00459
Epoch: 2, step: 950, training loss: 0.00339
Epoch: 2, step: 951, training loss: 0.00939
Epoch: 2, step: 952, training loss: 0.00524
Epoch: 2, step: 953, training loss: 0.00718
Epoch: 2, step: 954, training loss: 0.00189
Epoch: 2, average training loss: 0.00455
Epoch: 2, F1: 75.64912, average dev loss: 0.00337
Epoch: 3, step: 0, training loss: 0.00454
Epoch: 3, step: 1, training loss: 0.00440
Epoch: 3, step: 2, training loss: 0.00209
Epoch: 3, step: 3, training loss: 0.00266
Epoch: 3, step: 4, training loss: 0.00109
Epoch: 3, step: 5, training loss: 0.00613
Epoch: 3, step: 6, training loss: 0.00348
Epoch: 3, step: 7, training loss: 0.00301
Epoch: 3, step: 8, training loss: 0.00288
Epoch: 3, step: 9, training loss: 0.00456
Epoch: 3, step: 10, training loss: 0.00265
Epoch: 3, step: 11, training loss: 0.00354
Epoch: 3, step: 12, training loss: 0.00378
Epoch: 3, step: 13, training loss: 0.00258
Epoch: 3, step: 14, training loss: 0.00382
Epoch: 3, step: 15, training loss: 0.00296
Epoch: 3, step: 16, training loss: 0.00376
Epoch: 3, step: 17, training loss: 0.00221
Epoch: 3, step: 18, training loss: 0.00295
Epoch: 3, step: 19, training loss: 0.00528
Epoch: 3, step: 20, training loss: 0.00250
Epoch: 3, step: 21, training loss: 0.00114
Epoch: 3, step: 22, training loss: 0.00401
Epoch: 3, step: 23, training loss: 0.00423
Epoch: 3, step: 24, training loss: 0.00211
Epoch: 3, step: 25, training loss: 0.00206
Epoch: 3, step: 26, training loss: 0.00353
Epoch: 3, step: 27, training loss: 0.00244
Epoch: 3, step: 28, training loss: 0.00354
Epoch: 3, step: 29, training loss: 0.00287
Epoch: 3, step: 30, training loss: 0.00326
Epoch: 3, step: 31, training loss: 0.00554
Epoch: 3, step: 32, training loss: 0.00418
Epoch: 3, step: 33, training loss: 0.00086
Epoch: 3, step: 34, training loss: 0.00237
Epoch: 3, step: 35, training loss: 0.00322
Epoch: 3, step: 36, training loss: 0.00242
Epoch: 3, step: 37, training loss: 0.00125
Epoch: 3, step: 38, training loss: 0.00121
Epoch: 3, step: 39, training loss: 0.00268
Epoch: 3, step: 40, training loss: 0.00366
Epoch: 3, step: 41, training loss: 0.00627
Epoch: 3, step: 42, training loss: 0.00571
Epoch: 3, step: 43, training loss: 0.00238
Epoch: 3, step: 44, training loss: 0.00290
Epoch: 3, step: 45, training loss: 0.00142
Epoch: 3, step: 46, training loss: 0.00709
Epoch: 3, step: 47, training loss: 0.00384
Epoch: 3, step: 48, training loss: 0.00641
Epoch: 3, step: 49, training loss: 0.00676
Epoch: 3, step: 50, training loss: 0.00297
Epoch: 3, step: 51, training loss: 0.00342
Epoch: 3, step: 52, training loss: 0.00164
Epoch: 3, step: 53, training loss: 0.00409
Epoch: 3, step: 54, training loss: 0.00556
Epoch: 3, step: 55, training loss: 0.00243
Epoch: 3, step: 56, training loss: 0.00488
Epoch: 3, step: 57, training loss: 0.00267
Epoch: 3, step: 58, training loss: 0.00331
Epoch: 3, step: 59, training loss: 0.00664
Epoch: 3, step: 60, training loss: 0.00268
Epoch: 3, step: 61, training loss: 0.00334
Epoch: 3, step: 62, training loss: 0.00605
Epoch: 3, step: 63, training loss: 0.00353
Epoch: 3, step: 64, training loss: 0.00242
Epoch: 3, step: 65, training loss: 0.00240
Epoch: 3, step: 66, training loss: 0.00249
Epoch: 3, step: 67, training loss: 0.00309
Epoch: 3, step: 68, training loss: 0.00406
Epoch: 3, step: 69, training loss: 0.00409
Epoch: 3, step: 70, training loss: 0.00368
Epoch: 3, step: 71, training loss: 0.00348
Epoch: 3, step: 72, training loss: 0.00376
Epoch: 3, step: 73, training loss: 0.00262
Epoch: 3, step: 74, training loss: 0.00108
Epoch: 3, step: 75, training loss: 0.00235
Epoch: 3, step: 76, training loss: 0.00149
Epoch: 3, step: 77, training loss: 0.00238
Epoch: 3, step: 78, training loss: 0.00249
Epoch: 3, step: 79, training loss: 0.00225
Epoch: 3, step: 80, training loss: 0.00224
Epoch: 3, step: 81, training loss: 0.00190
Epoch: 3, step: 82, training loss: 0.00300
Epoch: 3, step: 83, training loss: 0.00197
Epoch: 3, step: 84, training loss: 0.00122
Epoch: 3, step: 85, training loss: 0.00223
Epoch: 3, step: 86, training loss: 0.00236
Epoch: 3, step: 87, training loss: 0.00378
Epoch: 3, step: 88, training loss: 0.00291
Epoch: 3, step: 89, training loss: 0.00391
Epoch: 3, step: 90, training loss: 0.00218
Epoch: 3, step: 91, training loss: 0.00429
Epoch: 3, step: 92, training loss: 0.00289
Epoch: 3, step: 93, training loss: 0.00177
Epoch: 3, step: 94, training loss: 0.00086
Epoch: 3, step: 95, training loss: 0.00233
Epoch: 3, step: 96, training loss: 0.00617
Epoch: 3, step: 97, training loss: 0.00367
Epoch: 3, step: 98, training loss: 0.00327
Epoch: 3, step: 99, training loss: 0.00314
Epoch: 3, step: 100, training loss: 0.00493
Epoch: 3, step: 101, training loss: 0.00676
Epoch: 3, step: 102, training loss: 0.00254
Epoch: 3, step: 103, training loss: 0.00217
Epoch: 3, step: 104, training loss: 0.00368
Epoch: 3, step: 105, training loss: 0.00251
Epoch: 3, step: 106, training loss: 0.00294
Epoch: 3, step: 107, training loss: 0.00353
Epoch: 3, step: 108, training loss: 0.00440
Epoch: 3, step: 109, training loss: 0.00177
Epoch: 3, step: 110, training loss: 0.00475
Epoch: 3, step: 111, training loss: 0.00305
Epoch: 3, step: 112, training loss: 0.00189
Epoch: 3, step: 113, training loss: 0.00158
Epoch: 3, step: 114, training loss: 0.00465
Epoch: 3, step: 115, training loss: 0.00581
Epoch: 3, step: 116, training loss: 0.00192
Epoch: 3, step: 117, training loss: 0.00244
Epoch: 3, step: 118, training loss: 0.00462
Epoch: 3, step: 119, training loss: 0.00374
Epoch: 3, step: 120, training loss: 0.00352
Epoch: 3, step: 121, training loss: 0.00292
Epoch: 3, step: 122, training loss: 0.00902
Epoch: 3, step: 123, training loss: 0.00225
Epoch: 3, step: 124, training loss: 0.00296
Epoch: 3, step: 125, training loss: 0.00626
Epoch: 3, step: 126, training loss: 0.00330
Epoch: 3, step: 127, training loss: 0.00331
Epoch: 3, step: 128, training loss: 0.00302
Epoch: 3, step: 129, training loss: 0.00262
Epoch: 3, step: 130, training loss: 0.00386
Epoch: 3, step: 131, training loss: 0.00291
Epoch: 3, step: 132, training loss: 0.00175
Epoch: 3, step: 133, training loss: 0.00270
Epoch: 3, step: 134, training loss: 0.00380
Epoch: 3, step: 135, training loss: 0.00227
Epoch: 3, step: 136, training loss: 0.00479
Epoch: 3, step: 137, training loss: 0.00131
Epoch: 3, step: 138, training loss: 0.00442
Epoch: 3, step: 139, training loss: 0.00333
Epoch: 3, step: 140, training loss: 0.00493
Epoch: 3, step: 141, training loss: 0.00096
Epoch: 3, step: 142, training loss: 0.00229
Epoch: 3, step: 143, training loss: 0.00248
Epoch: 3, step: 144, training loss: 0.00219
Epoch: 3, step: 145, training loss: 0.00229
Epoch: 3, step: 146, training loss: 0.00265
Epoch: 3, step: 147, training loss: 0.00370
Epoch: 3, step: 148, training loss: 0.00358
Epoch: 3, step: 149, training loss: 0.00268
Epoch: 3, step: 150, training loss: 0.00176
Epoch: 3, step: 151, training loss: 0.00109
Epoch: 3, step: 152, training loss: 0.00292
Epoch: 3, step: 153, training loss: 0.00421
Epoch: 3, step: 154, training loss: 0.00470
Epoch: 3, step: 155, training loss: 0.00075
Epoch: 3, step: 156, training loss: 0.00316
Epoch: 3, step: 157, training loss: 0.00274
Epoch: 3, step: 158, training loss: 0.00123
Epoch: 3, step: 159, training loss: 0.00186
Epoch: 3, step: 160, training loss: 0.00246
Epoch: 3, step: 161, training loss: 0.00251
Epoch: 3, step: 162, training loss: 0.00259
Epoch: 3, step: 163, training loss: 0.00265
Epoch: 3, step: 164, training loss: 0.00169
Epoch: 3, step: 165, training loss: 0.00365
Epoch: 3, step: 166, training loss: 0.00308
Epoch: 3, step: 167, training loss: 0.00214
Epoch: 3, step: 168, training loss: 0.00269
Epoch: 3, step: 169, training loss: 0.00290
Epoch: 3, step: 170, training loss: 0.00284
Epoch: 3, step: 171, training loss: 0.00237
Epoch: 3, step: 172, training loss: 0.00548
Epoch: 3, step: 173, training loss: 0.00232
Epoch: 3, step: 174, training loss: 0.00750
Epoch: 3, step: 175, training loss: 0.00349
Epoch: 3, step: 176, training loss: 0.00490
Epoch: 3, step: 177, training loss: 0.00155
Epoch: 3, step: 178, training loss: 0.00394
Epoch: 3, step: 179, training loss: 0.00099
Epoch: 3, step: 180, training loss: 0.00418
Epoch: 3, step: 181, training loss: 0.00075
Epoch: 3, step: 182, training loss: 0.00313
Epoch: 3, step: 183, training loss: 0.00150
Epoch: 3, step: 184, training loss: 0.00289
Epoch: 3, step: 185, training loss: 0.00138
Epoch: 3, step: 186, training loss: 0.00276
Epoch: 3, step: 187, training loss: 0.00284
Epoch: 3, step: 188, training loss: 0.00175
Epoch: 3, step: 189, training loss: 0.00187
Epoch: 3, step: 190, training loss: 0.00316
Epoch: 3, step: 191, training loss: 0.00317
Epoch: 3, step: 192, training loss: 0.00506
Epoch: 3, step: 193, training loss: 0.00191
Epoch: 3, step: 194, training loss: 0.00331
Epoch: 3, step: 195, training loss: 0.00141
Epoch: 3, step: 196, training loss: 0.00295
Epoch: 3, step: 197, training loss: 0.00203
Epoch: 3, step: 198, training loss: 0.00439
Epoch: 3, step: 199, training loss: 0.00346
Epoch: 3, step: 200, training loss: 0.00481
Epoch: 3, step: 201, training loss: 0.00367
Epoch: 3, step: 202, training loss: 0.00121
Epoch: 3, step: 203, training loss: 0.01272
Epoch: 3, step: 204, training loss: 0.00186
Epoch: 3, step: 205, training loss: 0.00214
Epoch: 3, step: 206, training loss: 0.00169
Epoch: 3, step: 207, training loss: 0.00259
Epoch: 3, step: 208, training loss: 0.00377
Epoch: 3, step: 209, training loss: 0.00443
Epoch: 3, step: 210, training loss: 0.00264
Epoch: 3, step: 211, training loss: 0.00516
Epoch: 3, step: 212, training loss: 0.00152
Epoch: 3, step: 213, training loss: 0.00235
Epoch: 3, step: 214, training loss: 0.00139
Epoch: 3, step: 215, training loss: 0.00239
Epoch: 3, step: 216, training loss: 0.00368
Epoch: 3, step: 217, training loss: 0.00221
Epoch: 3, step: 218, training loss: 0.00243
Epoch: 3, step: 219, training loss: 0.00405
Epoch: 3, step: 220, training loss: 0.00346
Epoch: 3, step: 221, training loss: 0.00142
Epoch: 3, step: 222, training loss: 0.00328
Epoch: 3, step: 223, training loss: 0.00198
Epoch: 3, step: 224, training loss: 0.00234
Epoch: 3, step: 225, training loss: 0.00379
Epoch: 3, step: 226, training loss: 0.00302
Epoch: 3, step: 227, training loss: 0.00195
Epoch: 3, step: 228, training loss: 0.00297
Epoch: 3, step: 229, training loss: 0.00117
Epoch: 3, step: 230, training loss: 0.00346
Epoch: 3, step: 231, training loss: 0.00460
Epoch: 3, step: 232, training loss: 0.00371
Epoch: 3, step: 233, training loss: 0.00356
Epoch: 3, step: 234, training loss: 0.00238
Epoch: 3, step: 235, training loss: 0.00444
Epoch: 3, step: 236, training loss: 0.00233
Epoch: 3, step: 237, training loss: 0.00408
Epoch: 3, step: 238, training loss: 0.00079
Epoch: 3, step: 239, training loss: 0.00330
Epoch: 3, step: 240, training loss: 0.00167
Epoch: 3, step: 241, training loss: 0.00334
Epoch: 3, step: 242, training loss: 0.00389
Epoch: 3, step: 243, training loss: 0.00349
Epoch: 3, step: 244, training loss: 0.00380
Epoch: 3, step: 245, training loss: 0.00118
Epoch: 3, step: 246, training loss: 0.00248
Epoch: 3, step: 247, training loss: 0.00398
Epoch: 3, step: 248, training loss: 0.00163
Epoch: 3, step: 249, training loss: 0.00548
Epoch: 3, step: 250, training loss: 0.00274
Epoch: 3, step: 251, training loss: 0.00109
Epoch: 3, step: 252, training loss: 0.00133
Epoch: 3, step: 253, training loss: 0.00212
Epoch: 3, step: 254, training loss: 0.00440
Epoch: 3, step: 255, training loss: 0.00395
Epoch: 3, step: 256, training loss: 0.00126
Epoch: 3, step: 257, training loss: 0.00096
Epoch: 3, step: 258, training loss: 0.00266
Epoch: 3, step: 259, training loss: 0.00190
Epoch: 3, step: 260, training loss: 0.00375
Epoch: 3, step: 261, training loss: 0.00871
Epoch: 3, step: 262, training loss: 0.00141
Epoch: 3, step: 263, training loss: 0.00098
Epoch: 3, step: 264, training loss: 0.00360
Epoch: 3, step: 265, training loss: 0.00237
Epoch: 3, step: 266, training loss: 0.00514
Epoch: 3, step: 267, training loss: 0.00231
Epoch: 3, step: 268, training loss: 0.00136
Epoch: 3, step: 269, training loss: 0.00195
Epoch: 3, step: 270, training loss: 0.00222
Epoch: 3, step: 271, training loss: 0.00140
Epoch: 3, step: 272, training loss: 0.00449
Epoch: 3, step: 273, training loss: 0.00394
Epoch: 3, step: 274, training loss: 0.00215
Epoch: 3, step: 275, training loss: 0.00256
Epoch: 3, step: 276, training loss: 0.00305
Epoch: 3, step: 277, training loss: 0.00102
Epoch: 3, step: 278, training loss: 0.00076
Epoch: 3, step: 279, training loss: 0.00284
Epoch: 3, step: 280, training loss: 0.00128
Epoch: 3, step: 281, training loss: 0.00319
Epoch: 3, step: 282, training loss: 0.00256
Epoch: 3, step: 283, training loss: 0.00248
Epoch: 3, step: 284, training loss: 0.00232
Epoch: 3, step: 285, training loss: 0.00214
Epoch: 3, step: 286, training loss: 0.00224
Epoch: 3, step: 287, training loss: 0.00240
Epoch: 3, step: 288, training loss: 0.00296
Epoch: 3, step: 289, training loss: 0.00663
Epoch: 3, step: 290, training loss: 0.00251
Epoch: 3, step: 291, training loss: 0.00227
Epoch: 3, step: 292, training loss: 0.00165
Epoch: 3, step: 293, training loss: 0.00341
Epoch: 3, step: 294, training loss: 0.00133
Epoch: 3, step: 295, training loss: 0.00292
Epoch: 3, step: 296, training loss: 0.00311
Epoch: 3, step: 297, training loss: 0.00159
Epoch: 3, step: 298, training loss: 0.00444
Epoch: 3, step: 299, training loss: 0.00241
Epoch: 3, step: 300, training loss: 0.00257
Epoch: 3, step: 301, training loss: 0.00224
Epoch: 3, step: 302, training loss: 0.00144
Epoch: 3, step: 303, training loss: 0.00395
Epoch: 3, step: 304, training loss: 0.00392
Epoch: 3, step: 305, training loss: 0.00241
Epoch: 3, step: 306, training loss: 0.00304
Epoch: 3, step: 307, training loss: 0.00142
Epoch: 3, step: 308, training loss: 0.00172
Epoch: 3, step: 309, training loss: 0.00180
Epoch: 3, step: 310, training loss: 0.00105
Epoch: 3, step: 311, training loss: 0.00383
Epoch: 3, step: 312, training loss: 0.00254
Epoch: 3, step: 313, training loss: 0.00231
Epoch: 3, step: 314, training loss: 0.00186
Epoch: 3, step: 315, training loss: 0.00844
Epoch: 3, step: 316, training loss: 0.00594
Epoch: 3, step: 317, training loss: 0.00222
Epoch: 3, step: 318, training loss: 0.00343
Epoch: 3, step: 319, training loss: 0.00339
Epoch: 3, step: 320, training loss: 0.00346
Epoch: 3, step: 321, training loss: 0.00208
Epoch: 3, step: 322, training loss: 0.00227
Epoch: 3, step: 323, training loss: 0.00130
Epoch: 3, step: 324, training loss: 0.00394
Epoch: 3, step: 325, training loss: 0.00249
Epoch: 3, step: 326, training loss: 0.00344
Epoch: 3, step: 327, training loss: 0.00221
Epoch: 3, step: 328, training loss: 0.00284
Epoch: 3, step: 329, training loss: 0.00307
Epoch: 3, step: 330, training loss: 0.00335
Epoch: 3, step: 331, training loss: 0.00695
Epoch: 3, step: 332, training loss: 0.00224
Epoch: 3, step: 333, training loss: 0.00500
Epoch: 3, step: 334, training loss: 0.00223
Epoch: 3, step: 335, training loss: 0.00372
Epoch: 3, step: 336, training loss: 0.00568
Epoch: 3, step: 337, training loss: 0.00427
Epoch: 3, step: 338, training loss: 0.00201
Epoch: 3, step: 339, training loss: 0.00542
Epoch: 3, step: 340, training loss: 0.00289
Epoch: 3, step: 341, training loss: 0.00167
Epoch: 3, step: 342, training loss: 0.00112
Epoch: 3, step: 343, training loss: 0.00208
Epoch: 3, step: 344, training loss: 0.00225
Epoch: 3, step: 345, training loss: 0.00405
Epoch: 3, step: 346, training loss: 0.00316
Epoch: 3, step: 347, training loss: 0.00303
Epoch: 3, step: 348, training loss: 0.00297
Epoch: 3, step: 349, training loss: 0.00151
Epoch: 3, step: 350, training loss: 0.00146
Epoch: 3, step: 351, training loss: 0.00503
Epoch: 3, step: 352, training loss: 0.00232
Epoch: 3, step: 353, training loss: 0.00535
Epoch: 3, step: 354, training loss: 0.00260
Epoch: 3, step: 355, training loss: 0.00379
Epoch: 3, step: 356, training loss: 0.00437
Epoch: 3, step: 357, training loss: 0.00226
Epoch: 3, step: 358, training loss: 0.00379
Epoch: 3, step: 359, training loss: 0.00301
Epoch: 3, step: 360, training loss: 0.00190
Epoch: 3, step: 361, training loss: 0.00160
Epoch: 3, step: 362, training loss: 0.00169
Epoch: 3, step: 363, training loss: 0.00459
Epoch: 3, step: 364, training loss: 0.00125
Epoch: 3, step: 365, training loss: 0.00343
Epoch: 3, step: 366, training loss: 0.00098
Epoch: 3, step: 367, training loss: 0.00232
Epoch: 3, step: 368, training loss: 0.00223
Epoch: 3, step: 369, training loss: 0.00243
Epoch: 3, step: 370, training loss: 0.00356
Epoch: 3, step: 371, training loss: 0.00354
Epoch: 3, step: 372, training loss: 0.00179
Epoch: 3, step: 373, training loss: 0.00385
Epoch: 3, step: 374, training loss: 0.00227
Epoch: 3, step: 375, training loss: 0.00147
Epoch: 3, step: 376, training loss: 0.00334
Epoch: 3, step: 377, training loss: 0.00541
Epoch: 3, step: 378, training loss: 0.00263
Epoch: 3, step: 379, training loss: 0.00267
Epoch: 3, step: 380, training loss: 0.00255
Epoch: 3, step: 381, training loss: 0.00336
Epoch: 3, step: 382, training loss: 0.00213
Epoch: 3, step: 383, training loss: 0.00154
Epoch: 3, step: 384, training loss: 0.00180
Epoch: 3, step: 385, training loss: 0.00217
Epoch: 3, step: 386, training loss: 0.00179
Epoch: 3, step: 387, training loss: 0.00099
Epoch: 3, step: 388, training loss: 0.00258
Epoch: 3, step: 389, training loss: 0.00231
Epoch: 3, step: 390, training loss: 0.00220
Epoch: 3, step: 391, training loss: 0.00230
Epoch: 3, step: 392, training loss: 0.00162
Epoch: 3, step: 393, training loss: 0.00401
Epoch: 3, step: 394, training loss: 0.00420
Epoch: 3, step: 395, training loss: 0.00215
Epoch: 3, step: 396, training loss: 0.00151
Epoch: 3, step: 397, training loss: 0.00279
Epoch: 3, step: 398, training loss: 0.00129
Epoch: 3, step: 399, training loss: 0.00300
Epoch: 3, step: 400, training loss: 0.00323
Epoch: 3, step: 401, training loss: 0.00427
Epoch: 3, step: 402, training loss: 0.00393
Epoch: 3, step: 403, training loss: 0.00142
Epoch: 3, step: 404, training loss: 0.00424
Epoch: 3, step: 405, training loss: 0.00212
Epoch: 3, step: 406, training loss: 0.00553
Epoch: 3, step: 407, training loss: 0.00355
Epoch: 3, step: 408, training loss: 0.00309
Epoch: 3, step: 409, training loss: 0.00405
Epoch: 3, step: 410, training loss: 0.00342
Epoch: 3, step: 411, training loss: 0.00293
Epoch: 3, step: 412, training loss: 0.00220
Epoch: 3, step: 413, training loss: 0.00099
Epoch: 3, step: 414, training loss: 0.00438
Epoch: 3, step: 415, training loss: 0.00203
Epoch: 3, step: 416, training loss: 0.00174
Epoch: 3, step: 417, training loss: 0.00152
Epoch: 3, step: 418, training loss: 0.00137
Epoch: 3, step: 419, training loss: 0.00172
Epoch: 3, step: 420, training loss: 0.00103
Epoch: 3, step: 421, training loss: 0.00097
Epoch: 3, step: 422, training loss: 0.00108
Epoch: 3, step: 423, training loss: 0.00250
Epoch: 3, step: 424, training loss: 0.00128
Epoch: 3, step: 425, training loss: 0.00123
Epoch: 3, step: 426, training loss: 0.00461
Epoch: 3, step: 427, training loss: 0.00279
Epoch: 3, step: 428, training loss: 0.00473
Epoch: 3, step: 429, training loss: 0.00164
Epoch: 3, step: 430, training loss: 0.00484
Epoch: 3, step: 431, training loss: 0.00226
Epoch: 3, step: 432, training loss: 0.00201
Epoch: 3, step: 433, training loss: 0.00194
Epoch: 3, step: 434, training loss: 0.00228
Epoch: 3, step: 435, training loss: 0.00298
Epoch: 3, step: 436, training loss: 0.00410
Epoch: 3, step: 437, training loss: 0.00679
Epoch: 3, step: 438, training loss: 0.00488
Epoch: 3, step: 439, training loss: 0.00471
Epoch: 3, step: 440, training loss: 0.00237
Epoch: 3, step: 441, training loss: 0.00370
Epoch: 3, step: 442, training loss: 0.00263
Epoch: 3, step: 443, training loss: 0.00180
Epoch: 3, step: 444, training loss: 0.00238
Epoch: 3, step: 445, training loss: 0.00105
Epoch: 3, step: 446, training loss: 0.00327
Epoch: 3, step: 447, training loss: 0.00393
Epoch: 3, step: 448, training loss: 0.00310
Epoch: 3, step: 449, training loss: 0.00180
Epoch: 3, step: 450, training loss: 0.00462
Epoch: 3, step: 451, training loss: 0.00310
Epoch: 3, step: 452, training loss: 0.00367
Epoch: 3, step: 453, training loss: 0.00104
Epoch: 3, step: 454, training loss: 0.00438
Epoch: 3, step: 455, training loss: 0.00725
Epoch: 3, step: 456, training loss: 0.00228
Epoch: 3, step: 457, training loss: 0.00086
Epoch: 3, step: 458, training loss: 0.00176
Epoch: 3, step: 459, training loss: 0.00151
Epoch: 3, step: 460, training loss: 0.00577
Epoch: 3, step: 461, training loss: 0.00261
Epoch: 3, step: 462, training loss: 0.00400
Epoch: 3, step: 463, training loss: 0.00099
Epoch: 3, step: 464, training loss: 0.00360
Epoch: 3, step: 465, training loss: 0.00238
Epoch: 3, step: 466, training loss: 0.00392
Epoch: 3, step: 467, training loss: 0.00343
Epoch: 3, step: 468, training loss: 0.00110
Epoch: 3, step: 469, training loss: 0.00284
Epoch: 3, step: 470, training loss: 0.00106
Epoch: 3, step: 471, training loss: 0.00290
Epoch: 3, step: 472, training loss: 0.00481
Epoch: 3, step: 473, training loss: 0.00367
Epoch: 3, step: 474, training loss: 0.00416
Epoch: 3, step: 475, training loss: 0.00352
Epoch: 3, step: 476, training loss: 0.00402
Epoch: 3, step: 477, training loss: 0.00292
Epoch: 3, step: 478, training loss: 0.00663
Epoch: 3, step: 479, training loss: 0.00397
Epoch: 3, step: 480, training loss: 0.00307
Epoch: 3, step: 481, training loss: 0.00194
Epoch: 3, step: 482, training loss: 0.00254
Epoch: 3, step: 483, training loss: 0.00403
Epoch: 3, step: 484, training loss: 0.00189
Epoch: 3, step: 485, training loss: 0.00514
Epoch: 3, step: 486, training loss: 0.00104
Epoch: 3, step: 487, training loss: 0.00212
Epoch: 3, step: 488, training loss: 0.00456
Epoch: 3, step: 489, training loss: 0.00658
Epoch: 3, step: 490, training loss: 0.00122
Epoch: 3, step: 491, training loss: 0.00136
Epoch: 3, step: 492, training loss: 0.00075
Epoch: 3, step: 493, training loss: 0.00125
Epoch: 3, step: 494, training loss: 0.00334
Epoch: 3, step: 495, training loss: 0.00197
Epoch: 3, step: 496, training loss: 0.00826
Epoch: 3, step: 497, training loss: 0.00395
Epoch: 3, step: 498, training loss: 0.00210
Epoch: 3, step: 499, training loss: 0.00548
Epoch: 3, step: 500, training loss: 0.00351
Epoch: 3, step: 501, training loss: 0.00318
Epoch: 3, step: 502, training loss: 0.00328
Epoch: 3, step: 503, training loss: 0.00554
Epoch: 3, step: 504, training loss: 0.00241
Epoch: 3, step: 505, training loss: 0.00239
Epoch: 3, step: 506, training loss: 0.00182
Epoch: 3, step: 507, training loss: 0.00339
Epoch: 3, step: 508, training loss: 0.00295
Epoch: 3, step: 509, training loss: 0.00287
Epoch: 3, step: 510, training loss: 0.00393
Epoch: 3, step: 511, training loss: 0.00555
Epoch: 3, step: 512, training loss: 0.00302
Epoch: 3, step: 513, training loss: 0.00201
Epoch: 3, step: 514, training loss: 0.00168
Epoch: 3, step: 515, training loss: 0.00349
Epoch: 3, step: 516, training loss: 0.00324
Epoch: 3, step: 517, training loss: 0.00239
Epoch: 3, step: 518, training loss: 0.00223
Epoch: 3, step: 519, training loss: 0.00083
Epoch: 3, step: 520, training loss: 0.00242
Epoch: 3, step: 521, training loss: 0.00352
Epoch: 3, step: 522, training loss: 0.00311
Epoch: 3, step: 523, training loss: 0.00305
Epoch: 3, step: 524, training loss: 0.00221
Epoch: 3, step: 525, training loss: 0.00308
Epoch: 3, step: 526, training loss: 0.00242
Epoch: 3, step: 527, training loss: 0.00184
Epoch: 3, step: 528, training loss: 0.00225
Epoch: 3, step: 529, training loss: 0.00355
Epoch: 3, step: 530, training loss: 0.00309
Epoch: 3, step: 531, training loss: 0.00182
Epoch: 3, step: 532, training loss: 0.00374
Epoch: 3, step: 533, training loss: 0.00377
Epoch: 3, step: 534, training loss: 0.00263
Epoch: 3, step: 535, training loss: 0.00299
Epoch: 3, step: 536, training loss: 0.00266
Epoch: 3, step: 537, training loss: 0.00207
Epoch: 3, step: 538, training loss: 0.00153
Epoch: 3, step: 539, training loss: 0.00376
Epoch: 3, step: 540, training loss: 0.00272
Epoch: 3, step: 541, training loss: 0.00232
Epoch: 3, step: 542, training loss: 0.00417
Epoch: 3, step: 543, training loss: 0.00081
Epoch: 3, step: 544, training loss: 0.00146
Epoch: 3, step: 545, training loss: 0.00316
Epoch: 3, step: 546, training loss: 0.00150
Epoch: 3, step: 547, training loss: 0.00168
Epoch: 3, step: 548, training loss: 0.00190
Epoch: 3, step: 549, training loss: 0.00197
Epoch: 3, step: 550, training loss: 0.00294
Epoch: 3, step: 551, training loss: 0.00352
Epoch: 3, step: 552, training loss: 0.00289
Epoch: 3, step: 553, training loss: 0.00232
Epoch: 3, step: 554, training loss: 0.00166
Epoch: 3, step: 555, training loss: 0.00378
Epoch: 3, step: 556, training loss: 0.00287
Epoch: 3, step: 557, training loss: 0.00187
Epoch: 3, step: 558, training loss: 0.00267
Epoch: 3, step: 559, training loss: 0.00230
Epoch: 3, step: 560, training loss: 0.00283
Epoch: 3, step: 561, training loss: 0.00282
Epoch: 3, step: 562, training loss: 0.00166
Epoch: 3, step: 563, training loss: 0.00226
Epoch: 3, step: 564, training loss: 0.00177
Epoch: 3, step: 565, training loss: 0.00166
Epoch: 3, step: 566, training loss: 0.00336
Epoch: 3, step: 567, training loss: 0.00140
Epoch: 3, step: 568, training loss: 0.00151
Epoch: 3, step: 569, training loss: 0.00092
Epoch: 3, step: 570, training loss: 0.00164
Epoch: 3, step: 571, training loss: 0.00074
Epoch: 3, step: 572, training loss: 0.00108
Epoch: 3, step: 573, training loss: 0.00233
Epoch: 3, step: 574, training loss: 0.00273
Epoch: 3, step: 575, training loss: 0.00232
Epoch: 3, step: 576, training loss: 0.00618
Epoch: 3, step: 577, training loss: 0.00599
Epoch: 3, step: 578, training loss: 0.00428
Epoch: 3, step: 579, training loss: 0.00216
Epoch: 3, step: 580, training loss: 0.00101
Epoch: 3, step: 581, training loss: 0.00211
Epoch: 3, step: 582, training loss: 0.00297
Epoch: 3, step: 583, training loss: 0.00144
Epoch: 3, step: 584, training loss: 0.00151
Epoch: 3, step: 585, training loss: 0.00355
Epoch: 3, step: 586, training loss: 0.00251
Epoch: 3, step: 587, training loss: 0.00345
Epoch: 3, step: 588, training loss: 0.00381
Epoch: 3, step: 589, training loss: 0.00164
Epoch: 3, step: 590, training loss: 0.00414
Epoch: 3, step: 591, training loss: 0.00512
Epoch: 3, step: 592, training loss: 0.00385
Epoch: 3, step: 593, training loss: 0.00058
Epoch: 3, step: 594, training loss: 0.00143
Epoch: 3, step: 595, training loss: 0.00407
Epoch: 3, step: 596, training loss: 0.00254
Epoch: 3, step: 597, training loss: 0.00349
Epoch: 3, step: 598, training loss: 0.00378
Epoch: 3, step: 599, training loss: 0.00201
Epoch: 3, step: 600, training loss: 0.00139
Epoch: 3, step: 601, training loss: 0.00753
Epoch: 3, step: 602, training loss: 0.00217
Epoch: 3, step: 603, training loss: 0.00340
Epoch: 3, step: 604, training loss: 0.00147
Epoch: 3, step: 605, training loss: 0.00314
Epoch: 3, step: 606, training loss: 0.00186
Epoch: 3, step: 607, training loss: 0.00233
Epoch: 3, step: 608, training loss: 0.00415
Epoch: 3, step: 609, training loss: 0.00094
Epoch: 3, step: 610, training loss: 0.00291
Epoch: 3, step: 611, training loss: 0.00251
Epoch: 3, step: 612, training loss: 0.00051
Epoch: 3, step: 613, training loss: 0.00447
Epoch: 3, step: 614, training loss: 0.00168
Epoch: 3, step: 615, training loss: 0.00095
Epoch: 3, step: 616, training loss: 0.00451
Epoch: 3, step: 617, training loss: 0.00199
Epoch: 3, step: 618, training loss: 0.00067
Epoch: 3, step: 619, training loss: 0.00174
Epoch: 3, step: 620, training loss: 0.00059
Epoch: 3, step: 621, training loss: 0.00095
Epoch: 3, step: 622, training loss: 0.00221
Epoch: 3, step: 623, training loss: 0.00223
Epoch: 3, step: 624, training loss: 0.00192
Epoch: 3, step: 625, training loss: 0.00259
Epoch: 3, step: 626, training loss: 0.00231
Epoch: 3, step: 627, training loss: 0.00131
Epoch: 3, step: 628, training loss: 0.00405
Epoch: 3, step: 629, training loss: 0.00277
Epoch: 3, step: 630, training loss: 0.00090
Epoch: 3, step: 631, training loss: 0.00695
Epoch: 3, step: 632, training loss: 0.00191
Epoch: 3, step: 633, training loss: 0.00242
Epoch: 3, step: 634, training loss: 0.00169
Epoch: 3, step: 635, training loss: 0.00307
Epoch: 3, step: 636, training loss: 0.00309
Epoch: 3, step: 637, training loss: 0.00466
Epoch: 3, step: 638, training loss: 0.00448
Epoch: 3, step: 639, training loss: 0.00289
Epoch: 3, step: 640, training loss: 0.00083
Epoch: 3, step: 641, training loss: 0.00039
Epoch: 3, step: 642, training loss: 0.00312
Epoch: 3, step: 643, training loss: 0.00308
Epoch: 3, step: 644, training loss: 0.00331
Epoch: 3, step: 645, training loss: 0.00166
Epoch: 3, step: 646, training loss: 0.00123
Epoch: 3, step: 647, training loss: 0.00186
Epoch: 3, step: 648, training loss: 0.00493
Epoch: 3, step: 649, training loss: 0.00356
Epoch: 3, step: 650, training loss: 0.00156
Epoch: 3, step: 651, training loss: 0.00300
Epoch: 3, step: 652, training loss: 0.00253
Epoch: 3, step: 653, training loss: 0.00169
Epoch: 3, step: 654, training loss: 0.00231
Epoch: 3, step: 655, training loss: 0.00339
Epoch: 3, step: 656, training loss: 0.00508
Epoch: 3, step: 657, training loss: 0.00142
Epoch: 3, step: 658, training loss: 0.00139
Epoch: 3, step: 659, training loss: 0.00383
Epoch: 3, step: 660, training loss: 0.00238
Epoch: 3, step: 661, training loss: 0.00113
Epoch: 3, step: 662, training loss: 0.00466
Epoch: 3, step: 663, training loss: 0.00367
Epoch: 3, step: 664, training loss: 0.00403
Epoch: 3, step: 665, training loss: 0.00195
Epoch: 3, step: 666, training loss: 0.00215
Epoch: 3, step: 667, training loss: 0.00385
Epoch: 3, step: 668, training loss: 0.00163
Epoch: 3, step: 669, training loss: 0.00193
Epoch: 3, step: 670, training loss: 0.00282
Epoch: 3, step: 671, training loss: 0.00243
Epoch: 3, step: 672, training loss: 0.00187
Epoch: 3, step: 673, training loss: 0.00094
Epoch: 3, step: 674, training loss: 0.00143
Epoch: 3, step: 675, training loss: 0.00063
Epoch: 3, step: 676, training loss: 0.00155
Epoch: 3, step: 677, training loss: 0.00124
Epoch: 3, step: 678, training loss: 0.00437
Epoch: 3, step: 679, training loss: 0.00209
Epoch: 3, step: 680, training loss: 0.00224
Epoch: 3, step: 681, training loss: 0.00274
Epoch: 3, step: 682, training loss: 0.00409
Epoch: 3, step: 683, training loss: 0.00160
Epoch: 3, step: 684, training loss: 0.00278
Epoch: 3, step: 685, training loss: 0.00409
Epoch: 3, step: 686, training loss: 0.00305
Epoch: 3, step: 687, training loss: 0.00415
Epoch: 3, step: 688, training loss: 0.00325
Epoch: 3, step: 689, training loss: 0.00582
Epoch: 3, step: 690, training loss: 0.00236
Epoch: 3, step: 691, training loss: 0.00353
Epoch: 3, step: 692, training loss: 0.00314
Epoch: 3, step: 693, training loss: 0.00176
Epoch: 3, step: 694, training loss: 0.00232
Epoch: 3, step: 695, training loss: 0.00282
Epoch: 3, step: 696, training loss: 0.00302
Epoch: 3, step: 697, training loss: 0.00193
Epoch: 3, step: 698, training loss: 0.00110
Epoch: 3, step: 699, training loss: 0.00293
Epoch: 3, step: 700, training loss: 0.00331
Epoch: 3, step: 701, training loss: 0.00153
Epoch: 3, step: 702, training loss: 0.00124
Epoch: 3, step: 703, training loss: 0.00217
Epoch: 3, step: 704, training loss: 0.00167
Epoch: 3, step: 705, training loss: 0.00520
Epoch: 3, step: 706, training loss: 0.00249
Epoch: 3, step: 707, training loss: 0.00435
Epoch: 3, step: 708, training loss: 0.00214
Epoch: 3, step: 709, training loss: 0.00154
Epoch: 3, step: 710, training loss: 0.00113
Epoch: 3, step: 711, training loss: 0.00165
Epoch: 3, step: 712, training loss: 0.00207
Epoch: 3, step: 713, training loss: 0.00724
Epoch: 3, step: 714, training loss: 0.00445
Epoch: 3, step: 715, training loss: 0.00215
Epoch: 3, step: 716, training loss: 0.00333
Epoch: 3, step: 717, training loss: 0.00362
Epoch: 3, step: 718, training loss: 0.00256
Epoch: 3, step: 719, training loss: 0.00221
Epoch: 3, step: 720, training loss: 0.00099
Epoch: 3, step: 721, training loss: 0.00230
Epoch: 3, step: 722, training loss: 0.00319
Epoch: 3, step: 723, training loss: 0.00237
Epoch: 3, step: 724, training loss: 0.00265
Epoch: 3, step: 725, training loss: 0.00144
Epoch: 3, step: 726, training loss: 0.00382
Epoch: 3, step: 727, training loss: 0.00391
Epoch: 3, step: 728, training loss: 0.00540
Epoch: 3, step: 729, training loss: 0.00272
Epoch: 3, step: 730, training loss: 0.00069
Epoch: 3, step: 731, training loss: 0.00119
Epoch: 3, step: 732, training loss: 0.00443
Epoch: 3, step: 733, training loss: 0.00506
Epoch: 3, step: 734, training loss: 0.00103
Epoch: 3, step: 735, training loss: 0.00217
Epoch: 3, step: 736, training loss: 0.00100
Epoch: 3, step: 737, training loss: 0.00110
Epoch: 3, step: 738, training loss: 0.00222
Epoch: 3, step: 739, training loss: 0.00373
Epoch: 3, step: 740, training loss: 0.00150
Epoch: 3, step: 741, training loss: 0.00314
Epoch: 3, step: 742, training loss: 0.00235
Epoch: 3, step: 743, training loss: 0.00075
Epoch: 3, step: 744, training loss: 0.00152
Epoch: 3, step: 745, training loss: 0.00363
Epoch: 3, step: 746, training loss: 0.00291
Epoch: 3, step: 747, training loss: 0.00247
Epoch: 3, step: 748, training loss: 0.00210
Epoch: 3, step: 749, training loss: 0.00178
Epoch: 3, step: 750, training loss: 0.00235
Epoch: 3, step: 751, training loss: 0.00181
Epoch: 3, step: 752, training loss: 0.00176
Epoch: 3, step: 753, training loss: 0.00154
Epoch: 3, step: 754, training loss: 0.00497
Epoch: 3, step: 755, training loss: 0.00377
Epoch: 3, step: 756, training loss: 0.00154
Epoch: 3, step: 757, training loss: 0.00385
Epoch: 3, step: 758, training loss: 0.00368
Epoch: 3, step: 759, training loss: 0.00116
Epoch: 3, step: 760, training loss: 0.00309
Epoch: 3, step: 761, training loss: 0.00477
Epoch: 3, step: 762, training loss: 0.00267
Epoch: 3, step: 763, training loss: 0.00331
Epoch: 3, step: 764, training loss: 0.00310
Epoch: 3, step: 765, training loss: 0.00367
Epoch: 3, step: 766, training loss: 0.00155
Epoch: 3, step: 767, training loss: 0.00204
Epoch: 3, step: 768, training loss: 0.00243
Epoch: 3, step: 769, training loss: 0.00534
Epoch: 3, step: 770, training loss: 0.00275
Epoch: 3, step: 771, training loss: 0.00135
Epoch: 3, step: 772, training loss: 0.00113
Epoch: 3, step: 773, training loss: 0.00281
Epoch: 3, step: 774, training loss: 0.00204
Epoch: 3, step: 775, training loss: 0.00542
Epoch: 3, step: 776, training loss: 0.00287
Epoch: 3, step: 777, training loss: 0.00176
Epoch: 3, step: 778, training loss: 0.00096
Epoch: 3, step: 779, training loss: 0.00130
Epoch: 3, step: 780, training loss: 0.00239
Epoch: 3, step: 781, training loss: 0.00095
Epoch: 3, step: 782, training loss: 0.00672
Epoch: 3, step: 783, training loss: 0.00295
Epoch: 3, step: 784, training loss: 0.00130
Epoch: 3, step: 785, training loss: 0.00193
Epoch: 3, step: 786, training loss: 0.00401
Epoch: 3, step: 787, training loss: 0.00459
Epoch: 3, step: 788, training loss: 0.00560
Epoch: 3, step: 789, training loss: 0.00274
Epoch: 3, step: 790, training loss: 0.00175
Epoch: 3, step: 791, training loss: 0.00172
Epoch: 3, step: 792, training loss: 0.00279
Epoch: 3, step: 793, training loss: 0.00159
Epoch: 3, step: 794, training loss: 0.00447
Epoch: 3, step: 795, training loss: 0.00146
Epoch: 3, step: 796, training loss: 0.00266
Epoch: 3, step: 797, training loss: 0.00214
Epoch: 3, step: 798, training loss: 0.00338
Epoch: 3, step: 799, training loss: 0.00476
Epoch: 3, step: 800, training loss: 0.00187
Epoch: 3, step: 801, training loss: 0.00361
Epoch: 3, step: 802, training loss: 0.00141
Epoch: 3, step: 803, training loss: 0.00159
Epoch: 3, step: 804, training loss: 0.00175
Epoch: 3, step: 805, training loss: 0.00162
Epoch: 3, step: 806, training loss: 0.00956
Epoch: 3, step: 807, training loss: 0.00229
Epoch: 3, step: 808, training loss: 0.00340
Epoch: 3, step: 809, training loss: 0.00153
Epoch: 3, step: 810, training loss: 0.00250
Epoch: 3, step: 811, training loss: 0.00117
Epoch: 3, step: 812, training loss: 0.00195
Epoch: 3, step: 813, training loss: 0.00525
Epoch: 3, step: 814, training loss: 0.00328
Epoch: 3, step: 815, training loss: 0.00333
Epoch: 3, step: 816, training loss: 0.00167
Epoch: 3, step: 817, training loss: 0.00267
Epoch: 3, step: 818, training loss: 0.00497
Epoch: 3, step: 819, training loss: 0.00085
Epoch: 3, step: 820, training loss: 0.00270
Epoch: 3, step: 821, training loss: 0.00176
Epoch: 3, step: 822, training loss: 0.00252
Epoch: 3, step: 823, training loss: 0.00338
Epoch: 3, step: 824, training loss: 0.00220
Epoch: 3, step: 825, training loss: 0.00362
Epoch: 3, step: 826, training loss: 0.00408
Epoch: 3, step: 827, training loss: 0.00122
Epoch: 3, step: 828, training loss: 0.00334
Epoch: 3, step: 829, training loss: 0.00585
Epoch: 3, step: 830, training loss: 0.00344
Epoch: 3, step: 831, training loss: 0.00297
Epoch: 3, step: 832, training loss: 0.00122
Epoch: 3, step: 833, training loss: 0.00245
Epoch: 3, step: 834, training loss: 0.00503
Epoch: 3, step: 835, training loss: 0.00244
Epoch: 3, step: 836, training loss: 0.00080
Epoch: 3, step: 837, training loss: 0.00151
Epoch: 3, step: 838, training loss: 0.00165
Epoch: 3, step: 839, training loss: 0.00216
Epoch: 3, step: 840, training loss: 0.00215
Epoch: 3, step: 841, training loss: 0.00298
Epoch: 3, step: 842, training loss: 0.00279
Epoch: 3, step: 843, training loss: 0.00215
Epoch: 3, step: 844, training loss: 0.00166
Epoch: 3, step: 845, training loss: 0.00332
Epoch: 3, step: 846, training loss: 0.00714
Epoch: 3, step: 847, training loss: 0.00418
Epoch: 3, step: 848, training loss: 0.00510
Epoch: 3, step: 849, training loss: 0.00135
Epoch: 3, step: 850, training loss: 0.00374
Epoch: 3, step: 851, training loss: 0.00064
Epoch: 3, step: 852, training loss: 0.00278
Epoch: 3, step: 853, training loss: 0.00512
Epoch: 3, step: 854, training loss: 0.00170
Epoch: 3, step: 855, training loss: 0.00034
Epoch: 3, step: 856, training loss: 0.00364
Epoch: 3, step: 857, training loss: 0.00192
Epoch: 3, step: 858, training loss: 0.00232
Epoch: 3, step: 859, training loss: 0.00118
Epoch: 3, step: 860, training loss: 0.00189
Epoch: 3, step: 861, training loss: 0.00386
Epoch: 3, step: 862, training loss: 0.00178
Epoch: 3, step: 863, training loss: 0.00174
Epoch: 3, step: 864, training loss: 0.00422
Epoch: 3, step: 865, training loss: 0.00251
Epoch: 3, step: 866, training loss: 0.00187
Epoch: 3, step: 867, training loss: 0.00073
Epoch: 3, step: 868, training loss: 0.00172
Epoch: 3, step: 869, training loss: 0.00175
Epoch: 3, step: 870, training loss: 0.00198
Epoch: 3, step: 871, training loss: 0.00079
Epoch: 3, step: 872, training loss: 0.00328
Epoch: 3, step: 873, training loss: 0.00108
Epoch: 3, step: 874, training loss: 0.00475
Epoch: 3, step: 875, training loss: 0.00212
Epoch: 3, step: 876, training loss: 0.00273
Epoch: 3, step: 877, training loss: 0.00286
Epoch: 3, step: 878, training loss: 0.00217
Epoch: 3, step: 879, training loss: 0.00150
Epoch: 3, step: 880, training loss: 0.00631
Epoch: 3, step: 881, training loss: 0.00126
Epoch: 3, step: 882, training loss: 0.00338
Epoch: 3, step: 883, training loss: 0.00216
Epoch: 3, step: 884, training loss: 0.00189
Epoch: 3, step: 885, training loss: 0.00331
Epoch: 3, step: 886, training loss: 0.00255
Epoch: 3, step: 887, training loss: 0.00239
Epoch: 3, step: 888, training loss: 0.00183
Epoch: 3, step: 889, training loss: 0.00115
Epoch: 3, step: 890, training loss: 0.00136
Epoch: 3, step: 891, training loss: 0.00100
Epoch: 3, step: 892, training loss: 0.00380
Epoch: 3, step: 893, training loss: 0.00133
Epoch: 3, step: 894, training loss: 0.00056
Epoch: 3, step: 895, training loss: 0.00084
Epoch: 3, step: 896, training loss: 0.00305
Epoch: 3, step: 897, training loss: 0.00207
Epoch: 3, step: 898, training loss: 0.00155
Epoch: 3, step: 899, training loss: 0.00295
Epoch: 3, step: 900, training loss: 0.00154
Epoch: 3, step: 901, training loss: 0.00099
Epoch: 3, step: 902, training loss: 0.00276
Epoch: 3, step: 903, training loss: 0.00248
Epoch: 3, step: 904, training loss: 0.00293
Epoch: 3, step: 905, training loss: 0.00138
Epoch: 3, step: 906, training loss: 0.00220
Epoch: 3, step: 907, training loss: 0.00241
Epoch: 3, step: 908, training loss: 0.00541
Epoch: 3, step: 909, training loss: 0.00287
Epoch: 3, step: 910, training loss: 0.00266
Epoch: 3, step: 911, training loss: 0.00298
Epoch: 3, step: 912, training loss: 0.00447
Epoch: 3, step: 913, training loss: 0.00156
Epoch: 3, step: 914, training loss: 0.00096
Epoch: 3, step: 915, training loss: 0.00263
Epoch: 3, step: 916, training loss: 0.00177
Epoch: 3, step: 917, training loss: 0.00119
Epoch: 3, step: 918, training loss: 0.00097
Epoch: 3, step: 919, training loss: 0.00288
Epoch: 3, step: 920, training loss: 0.00198
Epoch: 3, step: 921, training loss: 0.00174
Epoch: 3, step: 922, training loss: 0.00122
Epoch: 3, step: 923, training loss: 0.00203
Epoch: 3, step: 924, training loss: 0.00175
Epoch: 3, step: 925, training loss: 0.00166
Epoch: 3, step: 926, training loss: 0.00278
Epoch: 3, step: 927, training loss: 0.00282
Epoch: 3, step: 928, training loss: 0.00197
Epoch: 3, step: 929, training loss: 0.00226
Epoch: 3, step: 930, training loss: 0.00374
Epoch: 3, step: 931, training loss: 0.00117
Epoch: 3, step: 932, training loss: 0.00164
Epoch: 3, step: 933, training loss: 0.00204
Epoch: 3, step: 934, training loss: 0.00280
Epoch: 3, step: 935, training loss: 0.00362
Epoch: 3, step: 936, training loss: 0.00210
Epoch: 3, step: 937, training loss: 0.00154
Epoch: 3, step: 938, training loss: 0.00350
Epoch: 3, step: 939, training loss: 0.00123
Epoch: 3, step: 940, training loss: 0.00436
Epoch: 3, step: 941, training loss: 0.00385
Epoch: 3, step: 942, training loss: 0.00169
Epoch: 3, step: 943, training loss: 0.00129
Epoch: 3, step: 944, training loss: 0.00131
Epoch: 3, step: 945, training loss: 0.00125
Epoch: 3, step: 946, training loss: 0.00169
Epoch: 3, step: 947, training loss: 0.00243
Epoch: 3, step: 948, training loss: 0.00135
Epoch: 3, step: 949, training loss: 0.00428
Epoch: 3, step: 950, training loss: 0.00255
Epoch: 3, step: 951, training loss: 0.00249
Epoch: 3, step: 952, training loss: 0.00312
Epoch: 3, step: 953, training loss: 0.00089
Epoch: 3, step: 954, training loss: 0.00060
Epoch: 3, average training loss: 0.00281
Epoch: 3, F1: 79.50792, average dev loss: 0.00280
Epoch: 4, step: 0, training loss: 0.00131
Epoch: 4, step: 1, training loss: 0.00499
Epoch: 4, step: 2, training loss: 0.00322
Epoch: 4, step: 3, training loss: 0.00137
Epoch: 4, step: 4, training loss: 0.00216
Epoch: 4, step: 5, training loss: 0.00265
Epoch: 4, step: 6, training loss: 0.00178
Epoch: 4, step: 7, training loss: 0.00073
Epoch: 4, step: 8, training loss: 0.00298
Epoch: 4, step: 9, training loss: 0.00165
Epoch: 4, step: 10, training loss: 0.00236
Epoch: 4, step: 11, training loss: 0.00228
Epoch: 4, step: 12, training loss: 0.00184
Epoch: 4, step: 13, training loss: 0.00136
Epoch: 4, step: 14, training loss: 0.00225
Epoch: 4, step: 15, training loss: 0.00218
Epoch: 4, step: 16, training loss: 0.00288
Epoch: 4, step: 17, training loss: 0.00176
Epoch: 4, step: 18, training loss: 0.00213
Epoch: 4, step: 19, training loss: 0.00354
Epoch: 4, step: 20, training loss: 0.00483
Epoch: 4, step: 21, training loss: 0.00214
Epoch: 4, step: 22, training loss: 0.00166
Epoch: 4, step: 23, training loss: 0.00232
Epoch: 4, step: 24, training loss: 0.00315
Epoch: 4, step: 25, training loss: 0.00533
Epoch: 4, step: 26, training loss: 0.00364
Epoch: 4, step: 27, training loss: 0.00233
Epoch: 4, step: 28, training loss: 0.00300
Epoch: 4, step: 29, training loss: 0.00072
Epoch: 4, step: 30, training loss: 0.00281
Epoch: 4, step: 31, training loss: 0.00185
Epoch: 4, step: 32, training loss: 0.00334
Epoch: 4, step: 33, training loss: 0.00342
Epoch: 4, step: 34, training loss: 0.00107
Epoch: 4, step: 35, training loss: 0.00069
Epoch: 4, step: 36, training loss: 0.00162
Epoch: 4, step: 37, training loss: 0.00096
Epoch: 4, step: 38, training loss: 0.00161
Epoch: 4, step: 39, training loss: 0.00429
Epoch: 4, step: 40, training loss: 0.00208
Epoch: 4, step: 41, training loss: 0.00352
Epoch: 4, step: 42, training loss: 0.00283
Epoch: 4, step: 43, training loss: 0.00100
Epoch: 4, step: 44, training loss: 0.00196
Epoch: 4, step: 45, training loss: 0.00067
Epoch: 4, step: 46, training loss: 0.00125
Epoch: 4, step: 47, training loss: 0.00148
Epoch: 4, step: 48, training loss: 0.00140
Epoch: 4, step: 49, training loss: 0.00208
Epoch: 4, step: 50, training loss: 0.00164
Epoch: 4, step: 51, training loss: 0.00140
Epoch: 4, step: 52, training loss: 0.00099
Epoch: 4, step: 53, training loss: 0.00286
Epoch: 4, step: 54, training loss: 0.00161
Epoch: 4, step: 55, training loss: 0.00084
Epoch: 4, step: 56, training loss: 0.00401
Epoch: 4, step: 57, training loss: 0.00132
Epoch: 4, step: 58, training loss: 0.00324
Epoch: 4, step: 59, training loss: 0.00293
Epoch: 4, step: 60, training loss: 0.00138
Epoch: 4, step: 61, training loss: 0.00145
Epoch: 4, step: 62, training loss: 0.00225
Epoch: 4, step: 63, training loss: 0.00295
Epoch: 4, step: 64, training loss: 0.00110
Epoch: 4, step: 65, training loss: 0.00231
Epoch: 4, step: 66, training loss: 0.00243
Epoch: 4, step: 67, training loss: 0.00236
Epoch: 4, step: 68, training loss: 0.00158
Epoch: 4, step: 69, training loss: 0.00163
Epoch: 4, step: 70, training loss: 0.00216
Epoch: 4, step: 71, training loss: 0.00248
Epoch: 4, step: 72, training loss: 0.00077
Epoch: 4, step: 73, training loss: 0.00233
Epoch: 4, step: 74, training loss: 0.00374
Epoch: 4, step: 75, training loss: 0.00167
Epoch: 4, step: 76, training loss: 0.00101
Epoch: 4, step: 77, training loss: 0.00210
Epoch: 4, step: 78, training loss: 0.00470
Epoch: 4, step: 79, training loss: 0.00220
Epoch: 4, step: 80, training loss: 0.00213
Epoch: 4, step: 81, training loss: 0.00207
Epoch: 4, step: 82, training loss: 0.00133
Epoch: 4, step: 83, training loss: 0.00138
Epoch: 4, step: 84, training loss: 0.00072
Epoch: 4, step: 85, training loss: 0.00040
Epoch: 4, step: 86, training loss: 0.00368
Epoch: 4, step: 87, training loss: 0.00111
Epoch: 4, step: 88, training loss: 0.00178
Epoch: 4, step: 89, training loss: 0.00150
Epoch: 4, step: 90, training loss: 0.00190
Epoch: 4, step: 91, training loss: 0.00194
Epoch: 4, step: 92, training loss: 0.00129
Epoch: 4, step: 93, training loss: 0.00096
Epoch: 4, step: 94, training loss: 0.00291
Epoch: 4, step: 95, training loss: 0.00115
Epoch: 4, step: 96, training loss: 0.00201
Epoch: 4, step: 97, training loss: 0.00464
Epoch: 4, step: 98, training loss: 0.00083
Epoch: 4, step: 99, training loss: 0.00153
Epoch: 4, step: 100, training loss: 0.00241
Epoch: 4, step: 101, training loss: 0.00370
Epoch: 4, step: 102, training loss: 0.00081
Epoch: 4, step: 103, training loss: 0.00064
Epoch: 4, step: 104, training loss: 0.00211
Epoch: 4, step: 105, training loss: 0.00227
Epoch: 4, step: 106, training loss: 0.00190
Epoch: 4, step: 107, training loss: 0.00180
Epoch: 4, step: 108, training loss: 0.00354
Epoch: 4, step: 109, training loss: 0.00065
Epoch: 4, step: 110, training loss: 0.00174
Epoch: 4, step: 111, training loss: 0.00334
Epoch: 4, step: 112, training loss: 0.00263
Epoch: 4, step: 113, training loss: 0.00068
Epoch: 4, step: 114, training loss: 0.00150
Epoch: 4, step: 115, training loss: 0.00183
Epoch: 4, step: 116, training loss: 0.00078
Epoch: 4, step: 117, training loss: 0.00112
Epoch: 4, step: 118, training loss: 0.00207
Epoch: 4, step: 119, training loss: 0.00139
Epoch: 4, step: 120, training loss: 0.00293
Epoch: 4, step: 121, training loss: 0.00064
Epoch: 4, step: 122, training loss: 0.00141
Epoch: 4, step: 123, training loss: 0.00244
Epoch: 4, step: 124, training loss: 0.00078
Epoch: 4, step: 125, training loss: 0.00190
Epoch: 4, step: 126, training loss: 0.00327
Epoch: 4, step: 127, training loss: 0.00568
Epoch: 4, step: 128, training loss: 0.00203
Epoch: 4, step: 129, training loss: 0.00108
Epoch: 4, step: 130, training loss: 0.00212
Epoch: 4, step: 131, training loss: 0.00234
Epoch: 4, step: 132, training loss: 0.00134
Epoch: 4, step: 133, training loss: 0.00497
Epoch: 4, step: 134, training loss: 0.00386
Epoch: 4, step: 135, training loss: 0.00227
Epoch: 4, step: 136, training loss: 0.00205
Epoch: 4, step: 137, training loss: 0.00239
Epoch: 4, step: 138, training loss: 0.00414
Epoch: 4, step: 139, training loss: 0.00176
Epoch: 4, step: 140, training loss: 0.00287
Epoch: 4, step: 141, training loss: 0.00200
Epoch: 4, step: 142, training loss: 0.00238
Epoch: 4, step: 143, training loss: 0.00216
Epoch: 4, step: 144, training loss: 0.00135
Epoch: 4, step: 145, training loss: 0.00105
Epoch: 4, step: 146, training loss: 0.00181
Epoch: 4, step: 147, training loss: 0.00237
Epoch: 4, step: 148, training loss: 0.00200
Epoch: 4, step: 149, training loss: 0.00209
Epoch: 4, step: 150, training loss: 0.00200
Epoch: 4, step: 151, training loss: 0.00129
Epoch: 4, step: 152, training loss: 0.00149
Epoch: 4, step: 153, training loss: 0.00210
Epoch: 4, step: 154, training loss: 0.00140
Epoch: 4, step: 155, training loss: 0.00095
Epoch: 4, step: 156, training loss: 0.00351
Epoch: 4, step: 157, training loss: 0.00177
Epoch: 4, step: 158, training loss: 0.00162
Epoch: 4, step: 159, training loss: 0.00232
Epoch: 4, step: 160, training loss: 0.00177
Epoch: 4, step: 161, training loss: 0.00184
Epoch: 4, step: 162, training loss: 0.00278
Epoch: 4, step: 163, training loss: 0.00514
Epoch: 4, step: 164, training loss: 0.00224
Epoch: 4, step: 165, training loss: 0.00162
Epoch: 4, step: 166, training loss: 0.00430
Epoch: 4, step: 167, training loss: 0.00080
Epoch: 4, step: 168, training loss: 0.00112
Epoch: 4, step: 169, training loss: 0.00275
Epoch: 4, step: 170, training loss: 0.00542
Epoch: 4, step: 171, training loss: 0.00231
Epoch: 4, step: 172, training loss: 0.00249
Epoch: 4, step: 173, training loss: 0.00242
Epoch: 4, step: 174, training loss: 0.00241
Epoch: 4, step: 175, training loss: 0.00188
Epoch: 4, step: 176, training loss: 0.00322
Epoch: 4, step: 177, training loss: 0.00093
Epoch: 4, step: 178, training loss: 0.00050
Epoch: 4, step: 179, training loss: 0.00278
Epoch: 4, step: 180, training loss: 0.00297
Epoch: 4, step: 181, training loss: 0.00177
Epoch: 4, step: 182, training loss: 0.00181
Epoch: 4, step: 183, training loss: 0.00153
Epoch: 4, step: 184, training loss: 0.00073
Epoch: 4, step: 185, training loss: 0.00090
Epoch: 4, step: 186, training loss: 0.00106
Epoch: 4, step: 187, training loss: 0.00222
Epoch: 4, step: 188, training loss: 0.00183
Epoch: 4, step: 189, training loss: 0.00193
Epoch: 4, step: 190, training loss: 0.00482
Epoch: 4, step: 191, training loss: 0.00195
Epoch: 4, step: 192, training loss: 0.00255
Epoch: 4, step: 193, training loss: 0.00163
Epoch: 4, step: 194, training loss: 0.00142
Epoch: 4, step: 195, training loss: 0.00157
Epoch: 4, step: 196, training loss: 0.00266
Epoch: 4, step: 197, training loss: 0.00270
Epoch: 4, step: 198, training loss: 0.00242
Epoch: 4, step: 199, training loss: 0.00272
Epoch: 4, step: 200, training loss: 0.00220
Epoch: 4, step: 201, training loss: 0.00112
Epoch: 4, step: 202, training loss: 0.00114
Epoch: 4, step: 203, training loss: 0.00310
Epoch: 4, step: 204, training loss: 0.00132
Epoch: 4, step: 205, training loss: 0.00093
Epoch: 4, step: 206, training loss: 0.00379
Epoch: 4, step: 207, training loss: 0.00074
Epoch: 4, step: 208, training loss: 0.00176
Epoch: 4, step: 209, training loss: 0.00330
Epoch: 4, step: 210, training loss: 0.00130
Epoch: 4, step: 211, training loss: 0.00322
Epoch: 4, step: 212, training loss: 0.00202
Epoch: 4, step: 213, training loss: 0.00477
Epoch: 4, step: 214, training loss: 0.00177
Epoch: 4, step: 215, training loss: 0.00250
Epoch: 4, step: 216, training loss: 0.00190
Epoch: 4, step: 217, training loss: 0.00168
Epoch: 4, step: 218, training loss: 0.00140
Epoch: 4, step: 219, training loss: 0.00427
Epoch: 4, step: 220, training loss: 0.00135
Epoch: 4, step: 221, training loss: 0.00224
Epoch: 4, step: 222, training loss: 0.00091
Epoch: 4, step: 223, training loss: 0.00132
Epoch: 4, step: 224, training loss: 0.00100
Epoch: 4, step: 225, training loss: 0.00173
Epoch: 4, step: 226, training loss: 0.00157
Epoch: 4, step: 227, training loss: 0.00114
Epoch: 4, step: 228, training loss: 0.00058
Epoch: 4, step: 229, training loss: 0.00183
Epoch: 4, step: 230, training loss: 0.00167
Epoch: 4, step: 231, training loss: 0.00246
Epoch: 4, step: 232, training loss: 0.00211
Epoch: 4, step: 233, training loss: 0.00156
Epoch: 4, step: 234, training loss: 0.00155
Epoch: 4, step: 235, training loss: 0.00237
Epoch: 4, step: 236, training loss: 0.00272
Epoch: 4, step: 237, training loss: 0.00107
Epoch: 4, step: 238, training loss: 0.00236
Epoch: 4, step: 239, training loss: 0.00118
Epoch: 4, step: 240, training loss: 0.00259
Epoch: 4, step: 241, training loss: 0.00268
Epoch: 4, step: 242, training loss: 0.00067
Epoch: 4, step: 243, training loss: 0.00093
Epoch: 4, step: 244, training loss: 0.00120
Epoch: 4, step: 245, training loss: 0.00219
Epoch: 4, step: 246, training loss: 0.00246
Epoch: 4, step: 247, training loss: 0.00228
Epoch: 4, step: 248, training loss: 0.00385
Epoch: 4, step: 249, training loss: 0.00429
Epoch: 4, step: 250, training loss: 0.00110
Epoch: 4, step: 251, training loss: 0.00095
Epoch: 4, step: 252, training loss: 0.00228
Epoch: 4, step: 253, training loss: 0.00130
Epoch: 4, step: 254, training loss: 0.00264
Epoch: 4, step: 255, training loss: 0.00116
Epoch: 4, step: 256, training loss: 0.00191
Epoch: 4, step: 257, training loss: 0.00228
Epoch: 4, step: 258, training loss: 0.00159
Epoch: 4, step: 259, training loss: 0.00267
Epoch: 4, step: 260, training loss: 0.00301
Epoch: 4, step: 261, training loss: 0.00098
Epoch: 4, step: 262, training loss: 0.00127
Epoch: 4, step: 263, training loss: 0.00238
Epoch: 4, step: 264, training loss: 0.00128
Epoch: 4, step: 265, training loss: 0.00145
Epoch: 4, step: 266, training loss: 0.00154
Epoch: 4, step: 267, training loss: 0.00401
Epoch: 4, step: 268, training loss: 0.00324
Epoch: 4, step: 269, training loss: 0.00371
Epoch: 4, step: 270, training loss: 0.00065
Epoch: 4, step: 271, training loss: 0.00361
Epoch: 4, step: 272, training loss: 0.00243
Epoch: 4, step: 273, training loss: 0.00163
Epoch: 4, step: 274, training loss: 0.00349
Epoch: 4, step: 275, training loss: 0.00134
Epoch: 4, step: 276, training loss: 0.00149
Epoch: 4, step: 277, training loss: 0.00080
Epoch: 4, step: 278, training loss: 0.00319
Epoch: 4, step: 279, training loss: 0.00234
Epoch: 4, step: 280, training loss: 0.00100
Epoch: 4, step: 281, training loss: 0.00106
Epoch: 4, step: 282, training loss: 0.00162
Epoch: 4, step: 283, training loss: 0.00381
Epoch: 4, step: 284, training loss: 0.00132
Epoch: 4, step: 285, training loss: 0.00035
Epoch: 4, step: 286, training loss: 0.00051
Epoch: 4, step: 287, training loss: 0.00080
Epoch: 4, step: 288, training loss: 0.00154
Epoch: 4, step: 289, training loss: 0.00136
Epoch: 4, step: 290, training loss: 0.00438
Epoch: 4, step: 291, training loss: 0.00233
Epoch: 4, step: 292, training loss: 0.00222
Epoch: 4, step: 293, training loss: 0.00200
Epoch: 4, step: 294, training loss: 0.00430
Epoch: 4, step: 295, training loss: 0.00109
Epoch: 4, step: 296, training loss: 0.00113
Epoch: 4, step: 297, training loss: 0.00089
Epoch: 4, step: 298, training loss: 0.00178
Epoch: 4, step: 299, training loss: 0.00148
Epoch: 4, step: 300, training loss: 0.00124
Epoch: 4, step: 301, training loss: 0.00178
Epoch: 4, step: 302, training loss: 0.00177
Epoch: 4, step: 303, training loss: 0.00168
Epoch: 4, step: 304, training loss: 0.00370
Epoch: 4, step: 305, training loss: 0.00231
Epoch: 4, step: 306, training loss: 0.00487
Epoch: 4, step: 307, training loss: 0.00203
Epoch: 4, step: 308, training loss: 0.00621
Epoch: 4, step: 309, training loss: 0.00081
Epoch: 4, step: 310, training loss: 0.00065
Epoch: 4, step: 311, training loss: 0.00169
Epoch: 4, step: 312, training loss: 0.00453
Epoch: 4, step: 313, training loss: 0.00126
Epoch: 4, step: 314, training loss: 0.00227
Epoch: 4, step: 315, training loss: 0.00191
Epoch: 4, step: 316, training loss: 0.00147
Epoch: 4, step: 317, training loss: 0.00149
Epoch: 4, step: 318, training loss: 0.00170
Epoch: 4, step: 319, training loss: 0.00310
Epoch: 4, step: 320, training loss: 0.00235
Epoch: 4, step: 321, training loss: 0.00255
Epoch: 4, step: 322, training loss: 0.00112
Epoch: 4, step: 323, training loss: 0.00056
Epoch: 4, step: 324, training loss: 0.00090
Epoch: 4, step: 325, training loss: 0.00075
Epoch: 4, step: 326, training loss: 0.00200
Epoch: 4, step: 327, training loss: 0.00164
Epoch: 4, step: 328, training loss: 0.00206
Epoch: 4, step: 329, training loss: 0.00283
Epoch: 4, step: 330, training loss: 0.00303
Epoch: 4, step: 331, training loss: 0.00456
Epoch: 4, step: 332, training loss: 0.00107
Epoch: 4, step: 333, training loss: 0.00151
Epoch: 4, step: 334, training loss: 0.00150
Epoch: 4, step: 335, training loss: 0.00111
Epoch: 4, step: 336, training loss: 0.00226
Epoch: 4, step: 337, training loss: 0.00099
Epoch: 4, step: 338, training loss: 0.00292
Epoch: 4, step: 339, training loss: 0.00137
Epoch: 4, step: 340, training loss: 0.00157
Epoch: 4, step: 341, training loss: 0.00151
Epoch: 4, step: 342, training loss: 0.00289
Epoch: 4, step: 343, training loss: 0.00134
Epoch: 4, step: 344, training loss: 0.00084
Epoch: 4, step: 345, training loss: 0.00310
Epoch: 4, step: 346, training loss: 0.00498
Epoch: 4, step: 347, training loss: 0.00207
Epoch: 4, step: 348, training loss: 0.00115
Epoch: 4, step: 349, training loss: 0.00178
Epoch: 4, step: 350, training loss: 0.00217
Epoch: 4, step: 351, training loss: 0.00214
Epoch: 4, step: 352, training loss: 0.00373
Epoch: 4, step: 353, training loss: 0.00153
Epoch: 4, step: 354, training loss: 0.00035
Epoch: 4, step: 355, training loss: 0.00223
Epoch: 4, step: 356, training loss: 0.00185
Epoch: 4, step: 357, training loss: 0.00341
Epoch: 4, step: 358, training loss: 0.00325
Epoch: 4, step: 359, training loss: 0.00045
Epoch: 4, step: 360, training loss: 0.00163
Epoch: 4, step: 361, training loss: 0.00208
Epoch: 4, step: 362, training loss: 0.00051
Epoch: 4, step: 363, training loss: 0.00174
Epoch: 4, step: 364, training loss: 0.00242
Epoch: 4, step: 365, training loss: 0.00209
Epoch: 4, step: 366, training loss: 0.00161
Epoch: 4, step: 367, training loss: 0.00389
Epoch: 4, step: 368, training loss: 0.00363
Epoch: 4, step: 369, training loss: 0.00216
Epoch: 4, step: 370, training loss: 0.00141
Epoch: 4, step: 371, training loss: 0.00253
Epoch: 4, step: 372, training loss: 0.00121
Epoch: 4, step: 373, training loss: 0.00034
Epoch: 4, step: 374, training loss: 0.00374
Epoch: 4, step: 375, training loss: 0.00210
Epoch: 4, step: 376, training loss: 0.00168
Epoch: 4, step: 377, training loss: 0.00126
Epoch: 4, step: 378, training loss: 0.00230
Epoch: 4, step: 379, training loss: 0.00166
Epoch: 4, step: 380, training loss: 0.00193
Epoch: 4, step: 381, training loss: 0.00134
Epoch: 4, step: 382, training loss: 0.00329
Epoch: 4, step: 383, training loss: 0.00226
Epoch: 4, step: 384, training loss: 0.00114
Epoch: 4, step: 385, training loss: 0.00166
Epoch: 4, step: 386, training loss: 0.00091
Epoch: 4, step: 387, training loss: 0.00121
Epoch: 4, step: 388, training loss: 0.00163
Epoch: 4, step: 389, training loss: 0.00133
Epoch: 4, step: 390, training loss: 0.00331
Epoch: 4, step: 391, training loss: 0.00422
Epoch: 4, step: 392, training loss: 0.00294
Epoch: 4, step: 393, training loss: 0.00315
Epoch: 4, step: 394, training loss: 0.00180
Epoch: 4, step: 395, training loss: 0.00117
Epoch: 4, step: 396, training loss: 0.00080
Epoch: 4, step: 397, training loss: 0.00078
Epoch: 4, step: 398, training loss: 0.00251
Epoch: 4, step: 399, training loss: 0.00344
Epoch: 4, step: 400, training loss: 0.00059
Epoch: 4, step: 401, training loss: 0.00166
Epoch: 4, step: 402, training loss: 0.00110
Epoch: 4, step: 403, training loss: 0.00091
Epoch: 4, step: 404, training loss: 0.00287
Epoch: 4, step: 405, training loss: 0.00125
Epoch: 4, step: 406, training loss: 0.00181
Epoch: 4, step: 407, training loss: 0.00232
Epoch: 4, step: 408, training loss: 0.00261
Epoch: 4, step: 409, training loss: 0.00064
Epoch: 4, step: 410, training loss: 0.00107
Epoch: 4, step: 411, training loss: 0.00442
Epoch: 4, step: 412, training loss: 0.00159
Epoch: 4, step: 413, training loss: 0.00332
Epoch: 4, step: 414, training loss: 0.00138
Epoch: 4, step: 415, training loss: 0.00234
Epoch: 4, step: 416, training loss: 0.00122
Epoch: 4, step: 417, training loss: 0.00119
Epoch: 4, step: 418, training loss: 0.00163
Epoch: 4, step: 419, training loss: 0.00174
Epoch: 4, step: 420, training loss: 0.00252
Epoch: 4, step: 421, training loss: 0.00259
Epoch: 4, step: 422, training loss: 0.00092
Epoch: 4, step: 423, training loss: 0.00260
Epoch: 4, step: 424, training loss: 0.00142
Epoch: 4, step: 425, training loss: 0.00046
Epoch: 4, step: 426, training loss: 0.00189
Epoch: 4, step: 427, training loss: 0.00175
Epoch: 4, step: 428, training loss: 0.00045
Epoch: 4, step: 429, training loss: 0.00187
Epoch: 4, step: 430, training loss: 0.00202
Epoch: 4, step: 431, training loss: 0.00248
Epoch: 4, step: 432, training loss: 0.00543
Epoch: 4, step: 433, training loss: 0.00269
Epoch: 4, step: 434, training loss: 0.00157
Epoch: 4, step: 435, training loss: 0.00189
Epoch: 4, step: 436, training loss: 0.00170
Epoch: 4, step: 437, training loss: 0.00162
Epoch: 4, step: 438, training loss: 0.00229
Epoch: 4, step: 439, training loss: 0.00223
Epoch: 4, step: 440, training loss: 0.00099
Epoch: 4, step: 441, training loss: 0.00216
Epoch: 4, step: 442, training loss: 0.00264
Epoch: 4, step: 443, training loss: 0.00157
Epoch: 4, step: 444, training loss: 0.00071
Epoch: 4, step: 445, training loss: 0.00215
Epoch: 4, step: 446, training loss: 0.00567
Epoch: 4, step: 447, training loss: 0.00167
Epoch: 4, step: 448, training loss: 0.00072
Epoch: 4, step: 449, training loss: 0.00194
Epoch: 4, step: 450, training loss: 0.00118
Epoch: 4, step: 451, training loss: 0.00112
Epoch: 4, step: 452, training loss: 0.00220
Epoch: 4, step: 453, training loss: 0.00263
Epoch: 4, step: 454, training loss: 0.00198
Epoch: 4, step: 455, training loss: 0.00151
Epoch: 4, step: 456, training loss: 0.00184
Epoch: 4, step: 457, training loss: 0.00242
Epoch: 4, step: 458, training loss: 0.00228
Epoch: 4, step: 459, training loss: 0.00167
Epoch: 4, step: 460, training loss: 0.00091
Epoch: 4, step: 461, training loss: 0.00185
Epoch: 4, step: 462, training loss: 0.00190
Epoch: 4, step: 463, training loss: 0.00147
Epoch: 4, step: 464, training loss: 0.00159
Epoch: 4, step: 465, training loss: 0.00211
Epoch: 4, step: 466, training loss: 0.00178
Epoch: 4, step: 467, training loss: 0.00235
Epoch: 4, step: 468, training loss: 0.00516
Epoch: 4, step: 469, training loss: 0.00213
Epoch: 4, step: 470, training loss: 0.00411
Epoch: 4, step: 471, training loss: 0.00283
Epoch: 4, step: 472, training loss: 0.00101
Epoch: 4, step: 473, training loss: 0.00226
Epoch: 4, step: 474, training loss: 0.00129
Epoch: 4, step: 475, training loss: 0.00330
Epoch: 4, step: 476, training loss: 0.00226
Epoch: 4, step: 477, training loss: 0.00167
Epoch: 4, step: 478, training loss: 0.00142
Epoch: 4, step: 479, training loss: 0.00211
Epoch: 4, step: 480, training loss: 0.00216
Epoch: 4, step: 481, training loss: 0.00200
Epoch: 4, step: 482, training loss: 0.00280
Epoch: 4, step: 483, training loss: 0.00265
Epoch: 4, step: 484, training loss: 0.00184
Epoch: 4, step: 485, training loss: 0.00216
Epoch: 4, step: 486, training loss: 0.00318
Epoch: 4, step: 487, training loss: 0.00322
Epoch: 4, step: 488, training loss: 0.00265
Epoch: 4, step: 489, training loss: 0.00333
Epoch: 4, step: 490, training loss: 0.00163
Epoch: 4, step: 491, training loss: 0.00188
Epoch: 4, step: 492, training loss: 0.00181
Epoch: 4, step: 493, training loss: 0.00170
Epoch: 4, step: 494, training loss: 0.00386
Epoch: 4, step: 495, training loss: 0.00377
Epoch: 4, step: 496, training loss: 0.00097
Epoch: 4, step: 497, training loss: 0.00294
Epoch: 4, step: 498, training loss: 0.00163
Epoch: 4, step: 499, training loss: 0.00196
Epoch: 4, step: 500, training loss: 0.00128
Epoch: 4, step: 501, training loss: 0.00095
Epoch: 4, step: 502, training loss: 0.00183
Epoch: 4, step: 503, training loss: 0.00095
Epoch: 4, step: 504, training loss: 0.00193
Epoch: 4, step: 505, training loss: 0.00342
Epoch: 4, step: 506, training loss: 0.00071
Epoch: 4, step: 507, training loss: 0.00109
Epoch: 4, step: 508, training loss: 0.00095
Epoch: 4, step: 509, training loss: 0.00125
Epoch: 4, step: 510, training loss: 0.00298
Epoch: 4, step: 511, training loss: 0.00358
Epoch: 4, step: 512, training loss: 0.00093
Epoch: 4, step: 513, training loss: 0.00271
Epoch: 4, step: 514, training loss: 0.00191
Epoch: 4, step: 515, training loss: 0.00176
Epoch: 4, step: 516, training loss: 0.00135
Epoch: 4, step: 517, training loss: 0.00197
Epoch: 4, step: 518, training loss: 0.00143
Epoch: 4, step: 519, training loss: 0.00127
Epoch: 4, step: 520, training loss: 0.00228
Epoch: 4, step: 521, training loss: 0.00126
Epoch: 4, step: 522, training loss: 0.00271
Epoch: 4, step: 523, training loss: 0.00113
Epoch: 4, step: 524, training loss: 0.00149
Epoch: 4, step: 525, training loss: 0.00067
Epoch: 4, step: 526, training loss: 0.00439
Epoch: 4, step: 527, training loss: 0.00153
Epoch: 4, step: 528, training loss: 0.00813
Epoch: 4, step: 529, training loss: 0.00194
Epoch: 4, step: 530, training loss: 0.00145
Epoch: 4, step: 531, training loss: 0.00127
Epoch: 4, step: 532, training loss: 0.00053
Epoch: 4, step: 533, training loss: 0.00238
Epoch: 4, step: 534, training loss: 0.00258
Epoch: 4, step: 535, training loss: 0.00192
Epoch: 4, step: 536, training loss: 0.00119
Epoch: 4, step: 537, training loss: 0.00309
Epoch: 4, step: 538, training loss: 0.00153
Epoch: 4, step: 539, training loss: 0.00198
Epoch: 4, step: 540, training loss: 0.00272
Epoch: 4, step: 541, training loss: 0.00101
Epoch: 4, step: 542, training loss: 0.00197
Epoch: 4, step: 543, training loss: 0.00070
Epoch: 4, step: 544, training loss: 0.00156
Epoch: 4, step: 545, training loss: 0.00212
Epoch: 4, step: 546, training loss: 0.00215
Epoch: 4, step: 547, training loss: 0.00119
Epoch: 4, step: 548, training loss: 0.00117
Epoch: 4, step: 549, training loss: 0.00130
Epoch: 4, step: 550, training loss: 0.00126
Epoch: 4, step: 551, training loss: 0.00144
Epoch: 4, step: 552, training loss: 0.00180
Epoch: 4, step: 553, training loss: 0.00283
Epoch: 4, step: 554, training loss: 0.00123
Epoch: 4, step: 555, training loss: 0.00184
Epoch: 4, step: 556, training loss: 0.00300
Epoch: 4, step: 557, training loss: 0.00158
Epoch: 4, step: 558, training loss: 0.00199
Epoch: 4, step: 559, training loss: 0.00126
Epoch: 4, step: 560, training loss: 0.00210
Epoch: 4, step: 561, training loss: 0.00083
Epoch: 4, step: 562, training loss: 0.00178
Epoch: 4, step: 563, training loss: 0.00087
Epoch: 4, step: 564, training loss: 0.00216
Epoch: 4, step: 565, training loss: 0.00049
Epoch: 4, step: 566, training loss: 0.00200
Epoch: 4, step: 567, training loss: 0.00187
Epoch: 4, step: 568, training loss: 0.00271
Epoch: 4, step: 569, training loss: 0.00295
Epoch: 4, step: 570, training loss: 0.00154
Epoch: 4, step: 571, training loss: 0.00116
Epoch: 4, step: 572, training loss: 0.00231
Epoch: 4, step: 573, training loss: 0.00115
Epoch: 4, step: 574, training loss: 0.00217
Epoch: 4, step: 575, training loss: 0.00104
Epoch: 4, step: 576, training loss: 0.00166
Epoch: 4, step: 577, training loss: 0.00114
Epoch: 4, step: 578, training loss: 0.00205
Epoch: 4, step: 579, training loss: 0.00189
Epoch: 4, step: 580, training loss: 0.00190
Epoch: 4, step: 581, training loss: 0.00124
Epoch: 4, step: 582, training loss: 0.00318
Epoch: 4, step: 583, training loss: 0.00128
Epoch: 4, step: 584, training loss: 0.00087
Epoch: 4, step: 585, training loss: 0.00119
Epoch: 4, step: 586, training loss: 0.00151
Epoch: 4, step: 587, training loss: 0.00166
Epoch: 4, step: 588, training loss: 0.00149
Epoch: 4, step: 589, training loss: 0.00195
Epoch: 4, step: 590, training loss: 0.00139
Epoch: 4, step: 591, training loss: 0.00192
Epoch: 4, step: 592, training loss: 0.00041
Epoch: 4, step: 593, training loss: 0.00185
Epoch: 4, step: 594, training loss: 0.00309
Epoch: 4, step: 595, training loss: 0.00124
Epoch: 4, step: 596, training loss: 0.00162
Epoch: 4, step: 597, training loss: 0.00116
Epoch: 4, step: 598, training loss: 0.00276
Epoch: 4, step: 599, training loss: 0.00100
Epoch: 4, step: 600, training loss: 0.00509
Epoch: 4, step: 601, training loss: 0.00323
Epoch: 4, step: 602, training loss: 0.00121
Epoch: 4, step: 603, training loss: 0.00150
Epoch: 4, step: 604, training loss: 0.00315
Epoch: 4, step: 605, training loss: 0.00177
Epoch: 4, step: 606, training loss: 0.00188
Epoch: 4, step: 607, training loss: 0.00259
Epoch: 4, step: 608, training loss: 0.00120
Epoch: 4, step: 609, training loss: 0.00122
Epoch: 4, step: 610, training loss: 0.00186
Epoch: 4, step: 611, training loss: 0.00100
Epoch: 4, step: 612, training loss: 0.00126
Epoch: 4, step: 613, training loss: 0.00308
Epoch: 4, step: 614, training loss: 0.00090
Epoch: 4, step: 615, training loss: 0.00146
Epoch: 4, step: 616, training loss: 0.00213
Epoch: 4, step: 617, training loss: 0.00166
Epoch: 4, step: 618, training loss: 0.00118
Epoch: 4, step: 619, training loss: 0.00129
Epoch: 4, step: 620, training loss: 0.00140
Epoch: 4, step: 621, training loss: 0.00118
Epoch: 4, step: 622, training loss: 0.00191
Epoch: 4, step: 623, training loss: 0.00136
Epoch: 4, step: 624, training loss: 0.00307
Epoch: 4, step: 625, training loss: 0.00126
Epoch: 4, step: 626, training loss: 0.00228
Epoch: 4, step: 627, training loss: 0.00092
Epoch: 4, step: 628, training loss: 0.00154
Epoch: 4, step: 629, training loss: 0.00069
Epoch: 4, step: 630, training loss: 0.00214
Epoch: 4, step: 631, training loss: 0.00225
Epoch: 4, step: 632, training loss: 0.00173
Epoch: 4, step: 633, training loss: 0.00146
Epoch: 4, step: 634, training loss: 0.00435
Epoch: 4, step: 635, training loss: 0.00406
Epoch: 4, step: 636, training loss: 0.00104
Epoch: 4, step: 637, training loss: 0.00389
Epoch: 4, step: 638, training loss: 0.00192
Epoch: 4, step: 639, training loss: 0.00120
Epoch: 4, step: 640, training loss: 0.00337
Epoch: 4, step: 641, training loss: 0.00241
Epoch: 4, step: 642, training loss: 0.00208
Epoch: 4, step: 643, training loss: 0.00121
Epoch: 4, step: 644, training loss: 0.00239
Epoch: 4, step: 645, training loss: 0.00161
Epoch: 4, step: 646, training loss: 0.00120
Epoch: 4, step: 647, training loss: 0.00200
Epoch: 4, step: 648, training loss: 0.00125
Epoch: 4, step: 649, training loss: 0.00245
Epoch: 4, step: 650, training loss: 0.00184
Epoch: 4, step: 651, training loss: 0.00132
Epoch: 4, step: 652, training loss: 0.00154
Epoch: 4, step: 653, training loss: 0.00281
Epoch: 4, step: 654, training loss: 0.00474
Epoch: 4, step: 655, training loss: 0.00180
Epoch: 4, step: 656, training loss: 0.00092
Epoch: 4, step: 657, training loss: 0.00095
Epoch: 4, step: 658, training loss: 0.00276
Epoch: 4, step: 659, training loss: 0.00148
Epoch: 4, step: 660, training loss: 0.00249
Epoch: 4, step: 661, training loss: 0.00308
Epoch: 4, step: 662, training loss: 0.00247
Epoch: 4, step: 663, training loss: 0.00128
Epoch: 4, step: 664, training loss: 0.00074
Epoch: 4, step: 665, training loss: 0.00105
Epoch: 4, step: 666, training loss: 0.00207
Epoch: 4, step: 667, training loss: 0.00378
Epoch: 4, step: 668, training loss: 0.00144
Epoch: 4, step: 669, training loss: 0.00312
Epoch: 4, step: 670, training loss: 0.00175
Epoch: 4, step: 671, training loss: 0.00274
Epoch: 4, step: 672, training loss: 0.00258
Epoch: 4, step: 673, training loss: 0.00210
Epoch: 4, step: 674, training loss: 0.00235
Epoch: 4, step: 675, training loss: 0.00249
Epoch: 4, step: 676, training loss: 0.00148
Epoch: 4, step: 677, training loss: 0.00403
Epoch: 4, step: 678, training loss: 0.00129
Epoch: 4, step: 679, training loss: 0.00429
Epoch: 4, step: 680, training loss: 0.00167
Epoch: 4, step: 681, training loss: 0.00198
Epoch: 4, step: 682, training loss: 0.00183
Epoch: 4, step: 683, training loss: 0.00295
Epoch: 4, step: 684, training loss: 0.00288
Epoch: 4, step: 685, training loss: 0.00242
Epoch: 4, step: 686, training loss: 0.00305
Epoch: 4, step: 687, training loss: 0.00107
Epoch: 4, step: 688, training loss: 0.00144
Epoch: 4, step: 689, training loss: 0.00319
Epoch: 4, step: 690, training loss: 0.00088
Epoch: 4, step: 691, training loss: 0.00161
Epoch: 4, step: 692, training loss: 0.00578
Epoch: 4, step: 693, training loss: 0.00100
Epoch: 4, step: 694, training loss: 0.00626
Epoch: 4, step: 695, training loss: 0.00091
Epoch: 4, step: 696, training loss: 0.00160
Epoch: 4, step: 697, training loss: 0.00119
Epoch: 4, step: 698, training loss: 0.00164
Epoch: 4, step: 699, training loss: 0.00167
Epoch: 4, step: 700, training loss: 0.00292
Epoch: 4, step: 701, training loss: 0.00124
Epoch: 4, step: 702, training loss: 0.00133
Epoch: 4, step: 703, training loss: 0.00233
Epoch: 4, step: 704, training loss: 0.00231
Epoch: 4, step: 705, training loss: 0.00105
Epoch: 4, step: 706, training loss: 0.00164
Epoch: 4, step: 707, training loss: 0.00162
Epoch: 4, step: 708, training loss: 0.00209
Epoch: 4, step: 709, training loss: 0.00085
Epoch: 4, step: 710, training loss: 0.00084
Epoch: 4, step: 711, training loss: 0.00296
Epoch: 4, step: 712, training loss: 0.00343
Epoch: 4, step: 713, training loss: 0.00119
Epoch: 4, step: 714, training loss: 0.00190
Epoch: 4, step: 715, training loss: 0.00214
Epoch: 4, step: 716, training loss: 0.00403
Epoch: 4, step: 717, training loss: 0.00267
Epoch: 4, step: 718, training loss: 0.00124
Epoch: 4, step: 719, training loss: 0.00266
Epoch: 4, step: 720, training loss: 0.00180
Epoch: 4, step: 721, training loss: 0.00081
Epoch: 4, step: 722, training loss: 0.00362
Epoch: 4, step: 723, training loss: 0.00304
Epoch: 4, step: 724, training loss: 0.00235
Epoch: 4, step: 725, training loss: 0.00169
Epoch: 4, step: 726, training loss: 0.00025
Epoch: 4, step: 727, training loss: 0.00305
Epoch: 4, step: 728, training loss: 0.00188
Epoch: 4, step: 729, training loss: 0.00312
Epoch: 4, step: 730, training loss: 0.00304
Epoch: 4, step: 731, training loss: 0.00050
Epoch: 4, step: 732, training loss: 0.00152
Epoch: 4, step: 733, training loss: 0.00060
Epoch: 4, step: 734, training loss: 0.00567
Epoch: 4, step: 735, training loss: 0.00074
Epoch: 4, step: 736, training loss: 0.00110
Epoch: 4, step: 737, training loss: 0.00218
Epoch: 4, step: 738, training loss: 0.00055
Epoch: 4, step: 739, training loss: 0.00186
Epoch: 4, step: 740, training loss: 0.00122
Epoch: 4, step: 741, training loss: 0.00193
Epoch: 4, step: 742, training loss: 0.00302
Epoch: 4, step: 743, training loss: 0.00101
Epoch: 4, step: 744, training loss: 0.00277
Epoch: 4, step: 745, training loss: 0.00356
Epoch: 4, step: 746, training loss: 0.00067
Epoch: 4, step: 747, training loss: 0.00091
Epoch: 4, step: 748, training loss: 0.00182
Epoch: 4, step: 749, training loss: 0.00353
Epoch: 4, step: 750, training loss: 0.00290
Epoch: 4, step: 751, training loss: 0.00202
Epoch: 4, step: 752, training loss: 0.00147
Epoch: 4, step: 753, training loss: 0.00088
Epoch: 4, step: 754, training loss: 0.00098
Epoch: 4, step: 755, training loss: 0.00106
Epoch: 4, step: 756, training loss: 0.00155
Epoch: 4, step: 757, training loss: 0.00163
Epoch: 4, step: 758, training loss: 0.00135
Epoch: 4, step: 759, training loss: 0.00110
Epoch: 4, step: 760, training loss: 0.00109
Epoch: 4, step: 761, training loss: 0.00145
Epoch: 4, step: 762, training loss: 0.00198
Epoch: 4, step: 763, training loss: 0.00178
Epoch: 4, step: 764, training loss: 0.00168
Epoch: 4, step: 765, training loss: 0.00096
Epoch: 4, step: 766, training loss: 0.00180
Epoch: 4, step: 767, training loss: 0.00283
Epoch: 4, step: 768, training loss: 0.00116
Epoch: 4, step: 769, training loss: 0.00141
Epoch: 4, step: 770, training loss: 0.00212
Epoch: 4, step: 771, training loss: 0.00193
Epoch: 4, step: 772, training loss: 0.00238
Epoch: 4, step: 773, training loss: 0.00092
Epoch: 4, step: 774, training loss: 0.00346
Epoch: 4, step: 775, training loss: 0.00117
Epoch: 4, step: 776, training loss: 0.00179
Epoch: 4, step: 777, training loss: 0.00358
Epoch: 4, step: 778, training loss: 0.00157
Epoch: 4, step: 779, training loss: 0.00178
Epoch: 4, step: 780, training loss: 0.00211
Epoch: 4, step: 781, training loss: 0.00110
Epoch: 4, step: 782, training loss: 0.00134
Epoch: 4, step: 783, training loss: 0.00203
Epoch: 4, step: 784, training loss: 0.00210
Epoch: 4, step: 785, training loss: 0.00219
Epoch: 4, step: 786, training loss: 0.00068
Epoch: 4, step: 787, training loss: 0.00341
Epoch: 4, step: 788, training loss: 0.00215
Epoch: 4, step: 789, training loss: 0.00151
Epoch: 4, step: 790, training loss: 0.00165
Epoch: 4, step: 791, training loss: 0.00101
Epoch: 4, step: 792, training loss: 0.00262
Epoch: 4, step: 793, training loss: 0.00250
Epoch: 4, step: 794, training loss: 0.00123
Epoch: 4, step: 795, training loss: 0.00290
Epoch: 4, step: 796, training loss: 0.00096
Epoch: 4, step: 797, training loss: 0.00092
Epoch: 4, step: 798, training loss: 0.00224
Epoch: 4, step: 799, training loss: 0.00275
Epoch: 4, step: 800, training loss: 0.00091
Epoch: 4, step: 801, training loss: 0.00199
Epoch: 4, step: 802, training loss: 0.00184
Epoch: 4, step: 803, training loss: 0.00248
Epoch: 4, step: 804, training loss: 0.00210
Epoch: 4, step: 805, training loss: 0.00203
Epoch: 4, step: 806, training loss: 0.00584
Epoch: 4, step: 807, training loss: 0.00211
Epoch: 4, step: 808, training loss: 0.00064
Epoch: 4, step: 809, training loss: 0.00154
Epoch: 4, step: 810, training loss: 0.00082
Epoch: 4, step: 811, training loss: 0.00177
Epoch: 4, step: 812, training loss: 0.00196
Epoch: 4, step: 813, training loss: 0.00170
Epoch: 4, step: 814, training loss: 0.00202
Epoch: 4, step: 815, training loss: 0.00168
Epoch: 4, step: 816, training loss: 0.00205
Epoch: 4, step: 817, training loss: 0.00325
Epoch: 4, step: 818, training loss: 0.00177
Epoch: 4, step: 819, training loss: 0.00076
Epoch: 4, step: 820, training loss: 0.00149
Epoch: 4, step: 821, training loss: 0.00277
Epoch: 4, step: 822, training loss: 0.00124
Epoch: 4, step: 823, training loss: 0.00090
Epoch: 4, step: 824, training loss: 0.00111
Epoch: 4, step: 825, training loss: 0.00286
Epoch: 4, step: 826, training loss: 0.00234
Epoch: 4, step: 827, training loss: 0.00052
Epoch: 4, step: 828, training loss: 0.00153
Epoch: 4, step: 829, training loss: 0.00141
Epoch: 4, step: 830, training loss: 0.00156
Epoch: 4, step: 831, training loss: 0.00062
Epoch: 4, step: 832, training loss: 0.00319
Epoch: 4, step: 833, training loss: 0.00221
Epoch: 4, step: 834, training loss: 0.00116
Epoch: 4, step: 835, training loss: 0.00320
Epoch: 4, step: 836, training loss: 0.00065
Epoch: 4, step: 837, training loss: 0.00169
Epoch: 4, step: 838, training loss: 0.00196
Epoch: 4, step: 839, training loss: 0.00093
Epoch: 4, step: 840, training loss: 0.00083
Epoch: 4, step: 841, training loss: 0.00086
Epoch: 4, step: 842, training loss: 0.00142
Epoch: 4, step: 843, training loss: 0.00127
Epoch: 4, step: 844, training loss: 0.00145
Epoch: 4, step: 845, training loss: 0.00205
Epoch: 4, step: 846, training loss: 0.00146
Epoch: 4, step: 847, training loss: 0.00170
Epoch: 4, step: 848, training loss: 0.00191
Epoch: 4, step: 849, training loss: 0.00084
Epoch: 4, step: 850, training loss: 0.00199
Epoch: 4, step: 851, training loss: 0.00087
Epoch: 4, step: 852, training loss: 0.00312
Epoch: 4, step: 853, training loss: 0.00058
Epoch: 4, step: 854, training loss: 0.00152
Epoch: 4, step: 855, training loss: 0.00113
Epoch: 4, step: 856, training loss: 0.00148
Epoch: 4, step: 857, training loss: 0.00109
Epoch: 4, step: 858, training loss: 0.00206
Epoch: 4, step: 859, training loss: 0.00183
Epoch: 4, step: 860, training loss: 0.00107
Epoch: 4, step: 861, training loss: 0.00252
Epoch: 4, step: 862, training loss: 0.00113
Epoch: 4, step: 863, training loss: 0.00128
Epoch: 4, step: 864, training loss: 0.00267
Epoch: 4, step: 865, training loss: 0.00217
Epoch: 4, step: 866, training loss: 0.00208
Epoch: 4, step: 867, training loss: 0.00095
Epoch: 4, step: 868, training loss: 0.00178
Epoch: 4, step: 869, training loss: 0.00215
Epoch: 4, step: 870, training loss: 0.00134
Epoch: 4, step: 871, training loss: 0.00256
Epoch: 4, step: 872, training loss: 0.00108
Epoch: 4, step: 873, training loss: 0.00079
Epoch: 4, step: 874, training loss: 0.00237
Epoch: 4, step: 875, training loss: 0.00341
Epoch: 4, step: 876, training loss: 0.00225
Epoch: 4, step: 877, training loss: 0.00155
Epoch: 4, step: 878, training loss: 0.00232
Epoch: 4, step: 879, training loss: 0.00201
Epoch: 4, step: 880, training loss: 0.00076
Epoch: 4, step: 881, training loss: 0.00117
Epoch: 4, step: 882, training loss: 0.00111
Epoch: 4, step: 883, training loss: 0.00098
Epoch: 4, step: 884, training loss: 0.00125
Epoch: 4, step: 885, training loss: 0.00217
Epoch: 4, step: 886, training loss: 0.00272
Epoch: 4, step: 887, training loss: 0.00165
Epoch: 4, step: 888, training loss: 0.00317
Epoch: 4, step: 889, training loss: 0.00251
Epoch: 4, step: 890, training loss: 0.00075
Epoch: 4, step: 891, training loss: 0.00080
Epoch: 4, step: 892, training loss: 0.00103
Epoch: 4, step: 893, training loss: 0.00223
Epoch: 4, step: 894, training loss: 0.00223
Epoch: 4, step: 895, training loss: 0.00193
Epoch: 4, step: 896, training loss: 0.00153
Epoch: 4, step: 897, training loss: 0.00070
Epoch: 4, step: 898, training loss: 0.00222
Epoch: 4, step: 899, training loss: 0.00129
Epoch: 4, step: 900, training loss: 0.00081
Epoch: 4, step: 901, training loss: 0.00148
Epoch: 4, step: 902, training loss: 0.00145
Epoch: 4, step: 903, training loss: 0.00143
Epoch: 4, step: 904, training loss: 0.00247
Epoch: 4, step: 905, training loss: 0.00081
Epoch: 4, step: 906, training loss: 0.00140
Epoch: 4, step: 907, training loss: 0.00485
Epoch: 4, step: 908, training loss: 0.00128
Epoch: 4, step: 909, training loss: 0.00177
Epoch: 4, step: 910, training loss: 0.00144
Epoch: 4, step: 911, training loss: 0.00130
Epoch: 4, step: 912, training loss: 0.00167
Epoch: 4, step: 913, training loss: 0.00104
Epoch: 4, step: 914, training loss: 0.00130
Epoch: 4, step: 915, training loss: 0.00099
Epoch: 4, step: 916, training loss: 0.00220
Epoch: 4, step: 917, training loss: 0.00157
Epoch: 4, step: 918, training loss: 0.00124
Epoch: 4, step: 919, training loss: 0.00168
Epoch: 4, step: 920, training loss: 0.00177
Epoch: 4, step: 921, training loss: 0.00084
Epoch: 4, step: 922, training loss: 0.00204
Epoch: 4, step: 923, training loss: 0.00104
Epoch: 4, step: 924, training loss: 0.00061
Epoch: 4, step: 925, training loss: 0.00223
Epoch: 4, step: 926, training loss: 0.00099
Epoch: 4, step: 927, training loss: 0.00089
Epoch: 4, step: 928, training loss: 0.00241
Epoch: 4, step: 929, training loss: 0.00061
Epoch: 4, step: 930, training loss: 0.00144
Epoch: 4, step: 931, training loss: 0.00164
Epoch: 4, step: 932, training loss: 0.00132
Epoch: 4, step: 933, training loss: 0.00076
Epoch: 4, step: 934, training loss: 0.00130
Epoch: 4, step: 935, training loss: 0.00116
Epoch: 4, step: 936, training loss: 0.00128
Epoch: 4, step: 937, training loss: 0.00118
Epoch: 4, step: 938, training loss: 0.00235
Epoch: 4, step: 939, training loss: 0.00236
Epoch: 4, step: 940, training loss: 0.00062
Epoch: 4, step: 941, training loss: 0.00125
Epoch: 4, step: 942, training loss: 0.00120
Epoch: 4, step: 943, training loss: 0.00083
Epoch: 4, step: 944, training loss: 0.00355
Epoch: 4, step: 945, training loss: 0.00146
Epoch: 4, step: 946, training loss: 0.00146
Epoch: 4, step: 947, training loss: 0.00117
Epoch: 4, step: 948, training loss: 0.00106
Epoch: 4, step: 949, training loss: 0.00117
Epoch: 4, step: 950, training loss: 0.00391
Epoch: 4, step: 951, training loss: 0.00281
Epoch: 4, step: 952, training loss: 0.00092
Epoch: 4, step: 953, training loss: 0.00163
Epoch: 4, step: 954, training loss: 0.00182
Epoch: 4, average training loss: 0.00196
Epoch: 4, F1: 81.52637, average dev loss: 0.00266
Epoch: 5, step: 0, training loss: 0.00168
Epoch: 5, step: 1, training loss: 0.00075
Epoch: 5, step: 2, training loss: 0.00203
Epoch: 5, step: 3, training loss: 0.00153
Epoch: 5, step: 4, training loss: 0.00105
Epoch: 5, step: 5, training loss: 0.00127
Epoch: 5, step: 6, training loss: 0.00177
Epoch: 5, step: 7, training loss: 0.00221
Epoch: 5, step: 8, training loss: 0.00144
Epoch: 5, step: 9, training loss: 0.00066
Epoch: 5, step: 10, training loss: 0.00118
Epoch: 5, step: 11, training loss: 0.00127
Epoch: 5, step: 12, training loss: 0.00275
Epoch: 5, step: 13, training loss: 0.00296
Epoch: 5, step: 14, training loss: 0.00221
Epoch: 5, step: 15, training loss: 0.00117
Epoch: 5, step: 16, training loss: 0.00126
Epoch: 5, step: 17, training loss: 0.00131
Epoch: 5, step: 18, training loss: 0.00321
Epoch: 5, step: 19, training loss: 0.00079
Epoch: 5, step: 20, training loss: 0.00230
Epoch: 5, step: 21, training loss: 0.00230
Epoch: 5, step: 22, training loss: 0.00095
Epoch: 5, step: 23, training loss: 0.00132
Epoch: 5, step: 24, training loss: 0.00160
Epoch: 5, step: 25, training loss: 0.00134
Epoch: 5, step: 26, training loss: 0.00189
Epoch: 5, step: 27, training loss: 0.00123
Epoch: 5, step: 28, training loss: 0.00068
Epoch: 5, step: 29, training loss: 0.00185
Epoch: 5, step: 30, training loss: 0.00274
Epoch: 5, step: 31, training loss: 0.00152
Epoch: 5, step: 32, training loss: 0.00155
Epoch: 5, step: 33, training loss: 0.00074
Epoch: 5, step: 34, training loss: 0.00088
Epoch: 5, step: 35, training loss: 0.00182
Epoch: 5, step: 36, training loss: 0.00124
Epoch: 5, step: 37, training loss: 0.00149
Epoch: 5, step: 38, training loss: 0.00125
Epoch: 5, step: 39, training loss: 0.00093
Epoch: 5, step: 40, training loss: 0.00054
Epoch: 5, step: 41, training loss: 0.00066
Epoch: 5, step: 42, training loss: 0.00081
Epoch: 5, step: 43, training loss: 0.00135
Epoch: 5, step: 44, training loss: 0.00129
Epoch: 5, step: 45, training loss: 0.00076
Epoch: 5, step: 46, training loss: 0.00045
Epoch: 5, step: 47, training loss: 0.00104
Epoch: 5, step: 48, training loss: 0.00065
Epoch: 5, step: 49, training loss: 0.00176
Epoch: 5, step: 50, training loss: 0.00090
Epoch: 5, step: 51, training loss: 0.00335
Epoch: 5, step: 52, training loss: 0.00311
Epoch: 5, step: 53, training loss: 0.00057
Epoch: 5, step: 54, training loss: 0.00083
Epoch: 5, step: 55, training loss: 0.00047
Epoch: 5, step: 56, training loss: 0.00155
Epoch: 5, step: 57, training loss: 0.00105
Epoch: 5, step: 58, training loss: 0.00097
Epoch: 5, step: 59, training loss: 0.00247
Epoch: 5, step: 60, training loss: 0.00151
Epoch: 5, step: 61, training loss: 0.00104
Epoch: 5, step: 62, training loss: 0.00050
Epoch: 5, step: 63, training loss: 0.00115
Epoch: 5, step: 64, training loss: 0.00033
Epoch: 5, step: 65, training loss: 0.00260
Epoch: 5, step: 66, training loss: 0.00105
Epoch: 5, step: 67, training loss: 0.00142
Epoch: 5, step: 68, training loss: 0.00153
Epoch: 5, step: 69, training loss: 0.00081
Epoch: 5, step: 70, training loss: 0.00275
Epoch: 5, step: 71, training loss: 0.00036
Epoch: 5, step: 72, training loss: 0.00281
Epoch: 5, step: 73, training loss: 0.00447
Epoch: 5, step: 74, training loss: 0.00127
Epoch: 5, step: 75, training loss: 0.00068
Epoch: 5, step: 76, training loss: 0.00256
Epoch: 5, step: 77, training loss: 0.00170
Epoch: 5, step: 78, training loss: 0.00143
Epoch: 5, step: 79, training loss: 0.00187
Epoch: 5, step: 80, training loss: 0.00224
Epoch: 5, step: 81, training loss: 0.00120
Epoch: 5, step: 82, training loss: 0.00178
Epoch: 5, step: 83, training loss: 0.00111
Epoch: 5, step: 84, training loss: 0.00114
Epoch: 5, step: 85, training loss: 0.00148
Epoch: 5, step: 86, training loss: 0.00160
Epoch: 5, step: 87, training loss: 0.00061
Epoch: 5, step: 88, training loss: 0.00144
Epoch: 5, step: 89, training loss: 0.00178
Epoch: 5, step: 90, training loss: 0.00113
Epoch: 5, step: 91, training loss: 0.00184
Epoch: 5, step: 92, training loss: 0.00214
Epoch: 5, step: 93, training loss: 0.00144
Epoch: 5, step: 94, training loss: 0.00084
Epoch: 5, step: 95, training loss: 0.00352
Epoch: 5, step: 96, training loss: 0.00143
Epoch: 5, step: 97, training loss: 0.00070
Epoch: 5, step: 98, training loss: 0.00205
Epoch: 5, step: 99, training loss: 0.00225
Epoch: 5, step: 100, training loss: 0.00354
Epoch: 5, step: 101, training loss: 0.00145
Epoch: 5, step: 102, training loss: 0.00136
Epoch: 5, step: 103, training loss: 0.00196
Epoch: 5, step: 104, training loss: 0.00069
Epoch: 5, step: 105, training loss: 0.00066
Epoch: 5, step: 106, training loss: 0.00090
Epoch: 5, step: 107, training loss: 0.00088
Epoch: 5, step: 108, training loss: 0.00117
Epoch: 5, step: 109, training loss: 0.00130
Epoch: 5, step: 110, training loss: 0.00193
Epoch: 5, step: 111, training loss: 0.00071
Epoch: 5, step: 112, training loss: 0.00131
Epoch: 5, step: 113, training loss: 0.00118
Epoch: 5, step: 114, training loss: 0.00100
Epoch: 5, step: 115, training loss: 0.00070
Epoch: 5, step: 116, training loss: 0.00291
Epoch: 5, step: 117, training loss: 0.00175
Epoch: 5, step: 118, training loss: 0.00250
Epoch: 5, step: 119, training loss: 0.00158
Epoch: 5, step: 120, training loss: 0.00130
Epoch: 5, step: 121, training loss: 0.00377
Epoch: 5, step: 122, training loss: 0.00140
Epoch: 5, step: 123, training loss: 0.00099
Epoch: 5, step: 124, training loss: 0.00108
Epoch: 5, step: 125, training loss: 0.00146
Epoch: 5, step: 126, training loss: 0.00114
Epoch: 5, step: 127, training loss: 0.00082
Epoch: 5, step: 128, training loss: 0.00226
Epoch: 5, step: 129, training loss: 0.00142
Epoch: 5, step: 130, training loss: 0.00128
Epoch: 5, step: 131, training loss: 0.00128
Epoch: 5, step: 132, training loss: 0.00054
Epoch: 5, step: 133, training loss: 0.00137
Epoch: 5, step: 134, training loss: 0.00083
Epoch: 5, step: 135, training loss: 0.00098
Epoch: 5, step: 136, training loss: 0.00175
Epoch: 5, step: 137, training loss: 0.00035
Epoch: 5, step: 138, training loss: 0.00083
Epoch: 5, step: 139, training loss: 0.00062
Epoch: 5, step: 140, training loss: 0.00111
Epoch: 5, step: 141, training loss: 0.00243
Epoch: 5, step: 142, training loss: 0.00212
Epoch: 5, step: 143, training loss: 0.00078
Epoch: 5, step: 144, training loss: 0.00105
Epoch: 5, step: 145, training loss: 0.00080
Epoch: 5, step: 146, training loss: 0.00152
Epoch: 5, step: 147, training loss: 0.00153
Epoch: 5, step: 148, training loss: 0.00208
Epoch: 5, step: 149, training loss: 0.00140
Epoch: 5, step: 150, training loss: 0.00172
Epoch: 5, step: 151, training loss: 0.00209
Epoch: 5, step: 152, training loss: 0.00135
Epoch: 5, step: 153, training loss: 0.00169
Epoch: 5, step: 154, training loss: 0.00365
Epoch: 5, step: 155, training loss: 0.00084
Epoch: 5, step: 156, training loss: 0.00178
Epoch: 5, step: 157, training loss: 0.00120
Epoch: 5, step: 158, training loss: 0.00098
Epoch: 5, step: 159, training loss: 0.00055
Epoch: 5, step: 160, training loss: 0.00361
Epoch: 5, step: 161, training loss: 0.00089
Epoch: 5, step: 162, training loss: 0.00110
Epoch: 5, step: 163, training loss: 0.00816
Epoch: 5, step: 164, training loss: 0.00060
Epoch: 5, step: 165, training loss: 0.00112
Epoch: 5, step: 166, training loss: 0.00038
Epoch: 5, step: 167, training loss: 0.00052
Epoch: 5, step: 168, training loss: 0.00087
Epoch: 5, step: 169, training loss: 0.00159
Epoch: 5, step: 170, training loss: 0.00178
Epoch: 5, step: 171, training loss: 0.00276
Epoch: 5, step: 172, training loss: 0.00066
Epoch: 5, step: 173, training loss: 0.00247
Epoch: 5, step: 174, training loss: 0.00045
Epoch: 5, step: 175, training loss: 0.00255
Epoch: 5, step: 176, training loss: 0.00180
Epoch: 5, step: 177, training loss: 0.00134
Epoch: 5, step: 178, training loss: 0.00121
Epoch: 5, step: 179, training loss: 0.00212
Epoch: 5, step: 180, training loss: 0.00370
Epoch: 5, step: 181, training loss: 0.00180
Epoch: 5, step: 182, training loss: 0.00208
Epoch: 5, step: 183, training loss: 0.00077
Epoch: 5, step: 184, training loss: 0.00117
Epoch: 5, step: 185, training loss: 0.00140
Epoch: 5, step: 186, training loss: 0.00146
Epoch: 5, step: 187, training loss: 0.00079
Epoch: 5, step: 188, training loss: 0.00120
Epoch: 5, step: 189, training loss: 0.00106
Epoch: 5, step: 190, training loss: 0.00251
Epoch: 5, step: 191, training loss: 0.00107
Epoch: 5, step: 192, training loss: 0.00240
Epoch: 5, step: 193, training loss: 0.00127
Epoch: 5, step: 194, training loss: 0.00161
Epoch: 5, step: 195, training loss: 0.00099
Epoch: 5, step: 196, training loss: 0.00068
Epoch: 5, step: 197, training loss: 0.00142
Epoch: 5, step: 198, training loss: 0.00102
Epoch: 5, step: 199, training loss: 0.00059
Epoch: 5, step: 200, training loss: 0.00106
Epoch: 5, step: 201, training loss: 0.00163
Epoch: 5, step: 202, training loss: 0.00119
Epoch: 5, step: 203, training loss: 0.00090
Epoch: 5, step: 204, training loss: 0.00058
Epoch: 5, step: 205, training loss: 0.00248
Epoch: 5, step: 206, training loss: 0.00104
Epoch: 5, step: 207, training loss: 0.00096
Epoch: 5, step: 208, training loss: 0.00082
Epoch: 5, step: 209, training loss: 0.00110
Epoch: 5, step: 210, training loss: 0.00050
Epoch: 5, step: 211, training loss: 0.00060
Epoch: 5, step: 212, training loss: 0.00233
Epoch: 5, step: 213, training loss: 0.00037
Epoch: 5, step: 214, training loss: 0.00074
Epoch: 5, step: 215, training loss: 0.00178
Epoch: 5, step: 216, training loss: 0.00169
Epoch: 5, step: 217, training loss: 0.00122
Epoch: 5, step: 218, training loss: 0.00075
Epoch: 5, step: 219, training loss: 0.00153
Epoch: 5, step: 220, training loss: 0.00184
Epoch: 5, step: 221, training loss: 0.00131
Epoch: 5, step: 222, training loss: 0.00130
Epoch: 5, step: 223, training loss: 0.00330
Epoch: 5, step: 224, training loss: 0.00073
Epoch: 5, step: 225, training loss: 0.00092
Epoch: 5, step: 226, training loss: 0.00123
Epoch: 5, step: 227, training loss: 0.00259
Epoch: 5, step: 228, training loss: 0.00109
Epoch: 5, step: 229, training loss: 0.00059
Epoch: 5, step: 230, training loss: 0.00239
Epoch: 5, step: 231, training loss: 0.00171
Epoch: 5, step: 232, training loss: 0.00105
Epoch: 5, step: 233, training loss: 0.00215
Epoch: 5, step: 234, training loss: 0.00252
Epoch: 5, step: 235, training loss: 0.00122
Epoch: 5, step: 236, training loss: 0.00218
Epoch: 5, step: 237, training loss: 0.00203
Epoch: 5, step: 238, training loss: 0.00271
Epoch: 5, step: 239, training loss: 0.00235
Epoch: 5, step: 240, training loss: 0.00092
Epoch: 5, step: 241, training loss: 0.00052
Epoch: 5, step: 242, training loss: 0.00137
Epoch: 5, step: 243, training loss: 0.00220
Epoch: 5, step: 244, training loss: 0.00213
Epoch: 5, step: 245, training loss: 0.00176
Epoch: 5, step: 246, training loss: 0.00115
Epoch: 5, step: 247, training loss: 0.00092
Epoch: 5, step: 248, training loss: 0.00194
Epoch: 5, step: 249, training loss: 0.00228
Epoch: 5, step: 250, training loss: 0.00078
Epoch: 5, step: 251, training loss: 0.00224
Epoch: 5, step: 252, training loss: 0.00156
Epoch: 5, step: 253, training loss: 0.00193
Epoch: 5, step: 254, training loss: 0.00146
Epoch: 5, step: 255, training loss: 0.00261
Epoch: 5, step: 256, training loss: 0.00349
Epoch: 5, step: 257, training loss: 0.00033
Epoch: 5, step: 258, training loss: 0.00054
Epoch: 5, step: 259, training loss: 0.00150
Epoch: 5, step: 260, training loss: 0.00257
Epoch: 5, step: 261, training loss: 0.00123
Epoch: 5, step: 262, training loss: 0.00217
Epoch: 5, step: 263, training loss: 0.00288
Epoch: 5, step: 264, training loss: 0.00155
Epoch: 5, step: 265, training loss: 0.00163
Epoch: 5, step: 266, training loss: 0.00063
Epoch: 5, step: 267, training loss: 0.00132
Epoch: 5, step: 268, training loss: 0.00184
Epoch: 5, step: 269, training loss: 0.00095
Epoch: 5, step: 270, training loss: 0.00129
Epoch: 5, step: 271, training loss: 0.00162
Epoch: 5, step: 272, training loss: 0.00044
Epoch: 5, step: 273, training loss: 0.00359
Epoch: 5, step: 274, training loss: 0.00154
Epoch: 5, step: 275, training loss: 0.00213
Epoch: 5, step: 276, training loss: 0.00055
Epoch: 5, step: 277, training loss: 0.00078
Epoch: 5, step: 278, training loss: 0.00037
Epoch: 5, step: 279, training loss: 0.00630
Epoch: 5, step: 280, training loss: 0.00252
Epoch: 5, step: 281, training loss: 0.00172
Epoch: 5, step: 282, training loss: 0.00190
Epoch: 5, step: 283, training loss: 0.00284
Epoch: 5, step: 284, training loss: 0.00318
Epoch: 5, step: 285, training loss: 0.00046
Epoch: 5, step: 286, training loss: 0.00095
Epoch: 5, step: 287, training loss: 0.00224
Epoch: 5, step: 288, training loss: 0.00293
Epoch: 5, step: 289, training loss: 0.00172
Epoch: 5, step: 290, training loss: 0.00133
Epoch: 5, step: 291, training loss: 0.00148
Epoch: 5, step: 292, training loss: 0.00121
Epoch: 5, step: 293, training loss: 0.00196
Epoch: 5, step: 294, training loss: 0.00147
Epoch: 5, step: 295, training loss: 0.00031
Epoch: 5, step: 296, training loss: 0.00073
Epoch: 5, step: 297, training loss: 0.00116
Epoch: 5, step: 298, training loss: 0.00186
Epoch: 5, step: 299, training loss: 0.00161
Epoch: 5, step: 300, training loss: 0.00111
Epoch: 5, step: 301, training loss: 0.00256
Epoch: 5, step: 302, training loss: 0.00449
Epoch: 5, step: 303, training loss: 0.00234
Epoch: 5, step: 304, training loss: 0.00498
Epoch: 5, step: 305, training loss: 0.00244
Epoch: 5, step: 306, training loss: 0.00051
Epoch: 5, step: 307, training loss: 0.00142
Epoch: 5, step: 308, training loss: 0.00205
Epoch: 5, step: 309, training loss: 0.00096
Epoch: 5, step: 310, training loss: 0.00293
Epoch: 5, step: 311, training loss: 0.00136
Epoch: 5, step: 312, training loss: 0.00199
Epoch: 5, step: 313, training loss: 0.00160
Epoch: 5, step: 314, training loss: 0.00197
Epoch: 5, step: 315, training loss: 0.00205
Epoch: 5, step: 316, training loss: 0.00208
Epoch: 5, step: 317, training loss: 0.00110
Epoch: 5, step: 318, training loss: 0.00269
Epoch: 5, step: 319, training loss: 0.00121
Epoch: 5, step: 320, training loss: 0.00229
Epoch: 5, step: 321, training loss: 0.00158
Epoch: 5, step: 322, training loss: 0.00031
Epoch: 5, step: 323, training loss: 0.00304
Epoch: 5, step: 324, training loss: 0.00114
Epoch: 5, step: 325, training loss: 0.00175
Epoch: 5, step: 326, training loss: 0.00090
Epoch: 5, step: 327, training loss: 0.00084
Epoch: 5, step: 328, training loss: 0.00322
Epoch: 5, step: 329, training loss: 0.00114
Epoch: 5, step: 330, training loss: 0.00169
Epoch: 5, step: 331, training loss: 0.00140
Epoch: 5, step: 332, training loss: 0.00118
Epoch: 5, step: 333, training loss: 0.00052
Epoch: 5, step: 334, training loss: 0.00071
Epoch: 5, step: 335, training loss: 0.00144
Epoch: 5, step: 336, training loss: 0.00097
Epoch: 5, step: 337, training loss: 0.00054
Epoch: 5, step: 338, training loss: 0.00066
Epoch: 5, step: 339, training loss: 0.00128
Epoch: 5, step: 340, training loss: 0.00097
Epoch: 5, step: 341, training loss: 0.00117
Epoch: 5, step: 342, training loss: 0.00073
Epoch: 5, step: 343, training loss: 0.00237
Epoch: 5, step: 344, training loss: 0.00102
Epoch: 5, step: 345, training loss: 0.00049
Epoch: 5, step: 346, training loss: 0.00340
Epoch: 5, step: 347, training loss: 0.00100
Epoch: 5, step: 348, training loss: 0.00124
Epoch: 5, step: 349, training loss: 0.00137
Epoch: 5, step: 350, training loss: 0.00142
Epoch: 5, step: 351, training loss: 0.00147
Epoch: 5, step: 352, training loss: 0.00074
Epoch: 5, step: 353, training loss: 0.00052
Epoch: 5, step: 354, training loss: 0.00064
Epoch: 5, step: 355, training loss: 0.00136
Epoch: 5, step: 356, training loss: 0.00398
Epoch: 5, step: 357, training loss: 0.00080
Epoch: 5, step: 358, training loss: 0.00286
Epoch: 5, step: 359, training loss: 0.00084
Epoch: 5, step: 360, training loss: 0.00165
Epoch: 5, step: 361, training loss: 0.00089
Epoch: 5, step: 362, training loss: 0.00151
Epoch: 5, step: 363, training loss: 0.00087
Epoch: 5, step: 364, training loss: 0.00363
Epoch: 5, step: 365, training loss: 0.00116
Epoch: 5, step: 366, training loss: 0.00566
Epoch: 5, step: 367, training loss: 0.00252
Epoch: 5, step: 368, training loss: 0.00050
Epoch: 5, step: 369, training loss: 0.00198
Epoch: 5, step: 370, training loss: 0.00453
Epoch: 5, step: 371, training loss: 0.00216
Epoch: 5, step: 372, training loss: 0.00150
Epoch: 5, step: 373, training loss: 0.00165
Epoch: 5, step: 374, training loss: 0.00097
Epoch: 5, step: 375, training loss: 0.00169
Epoch: 5, step: 376, training loss: 0.00118
Epoch: 5, step: 377, training loss: 0.00064
Epoch: 5, step: 378, training loss: 0.00132
Epoch: 5, step: 379, training loss: 0.00115
Epoch: 5, step: 380, training loss: 0.00139
Epoch: 5, step: 381, training loss: 0.00090
Epoch: 5, step: 382, training loss: 0.00142
Epoch: 5, step: 383, training loss: 0.00131
Epoch: 5, step: 384, training loss: 0.00276
Epoch: 5, step: 385, training loss: 0.00071
Epoch: 5, step: 386, training loss: 0.00282
Epoch: 5, step: 387, training loss: 0.00309
Epoch: 5, step: 388, training loss: 0.00084
Epoch: 5, step: 389, training loss: 0.00112
Epoch: 5, step: 390, training loss: 0.00091
Epoch: 5, step: 391, training loss: 0.00302
Epoch: 5, step: 392, training loss: 0.00190
Epoch: 5, step: 393, training loss: 0.00284
Epoch: 5, step: 394, training loss: 0.00295
Epoch: 5, step: 395, training loss: 0.00090
Epoch: 5, step: 396, training loss: 0.00076
Epoch: 5, step: 397, training loss: 0.00269
Epoch: 5, step: 398, training loss: 0.00210
Epoch: 5, step: 399, training loss: 0.00081
Epoch: 5, step: 400, training loss: 0.00171
Epoch: 5, step: 401, training loss: 0.00089
Epoch: 5, step: 402, training loss: 0.00122
Epoch: 5, step: 403, training loss: 0.00199
Epoch: 5, step: 404, training loss: 0.00090
Epoch: 5, step: 405, training loss: 0.00155
Epoch: 5, step: 406, training loss: 0.00255
Epoch: 5, step: 407, training loss: 0.00224
Epoch: 5, step: 408, training loss: 0.00083
Epoch: 5, step: 409, training loss: 0.00092
Epoch: 5, step: 410, training loss: 0.00083
Epoch: 5, step: 411, training loss: 0.00062
Epoch: 5, step: 412, training loss: 0.00131
Epoch: 5, step: 413, training loss: 0.00119
Epoch: 5, step: 414, training loss: 0.00106
Epoch: 5, step: 415, training loss: 0.00141
Epoch: 5, step: 416, training loss: 0.00150
Epoch: 5, step: 417, training loss: 0.00058
Epoch: 5, step: 418, training loss: 0.00226
Epoch: 5, step: 419, training loss: 0.00327
Epoch: 5, step: 420, training loss: 0.00100
Epoch: 5, step: 421, training loss: 0.00243
Epoch: 5, step: 422, training loss: 0.00019
Epoch: 5, step: 423, training loss: 0.00291
Epoch: 5, step: 424, training loss: 0.00047
Epoch: 5, step: 425, training loss: 0.00268
Epoch: 5, step: 426, training loss: 0.00157
Epoch: 5, step: 427, training loss: 0.00203
Epoch: 5, step: 428, training loss: 0.00187
Epoch: 5, step: 429, training loss: 0.00139
Epoch: 5, step: 430, training loss: 0.00128
Epoch: 5, step: 431, training loss: 0.00269
Epoch: 5, step: 432, training loss: 0.00096
Epoch: 5, step: 433, training loss: 0.00097
Epoch: 5, step: 434, training loss: 0.00230
Epoch: 5, step: 435, training loss: 0.00274
Epoch: 5, step: 436, training loss: 0.00126
Epoch: 5, step: 437, training loss: 0.00214
Epoch: 5, step: 438, training loss: 0.00145
Epoch: 5, step: 439, training loss: 0.00188
Epoch: 5, step: 440, training loss: 0.00096
Epoch: 5, step: 441, training loss: 0.00164
Epoch: 5, step: 442, training loss: 0.00092
Epoch: 5, step: 443, training loss: 0.00133
Epoch: 5, step: 444, training loss: 0.00031
Epoch: 5, step: 445, training loss: 0.00096
Epoch: 5, step: 446, training loss: 0.00147
Epoch: 5, step: 447, training loss: 0.00134
Epoch: 5, step: 448, training loss: 0.00110
Epoch: 5, step: 449, training loss: 0.00089
Epoch: 5, step: 450, training loss: 0.00087
Epoch: 5, step: 451, training loss: 0.00134
Epoch: 5, step: 452, training loss: 0.00114
Epoch: 5, step: 453, training loss: 0.00128
Epoch: 5, step: 454, training loss: 0.00012
Epoch: 5, step: 455, training loss: 0.00207
Epoch: 5, step: 456, training loss: 0.00118
Epoch: 5, step: 457, training loss: 0.00102
Epoch: 5, step: 458, training loss: 0.00122
Epoch: 5, step: 459, training loss: 0.00109
Epoch: 5, step: 460, training loss: 0.00094
Epoch: 5, step: 461, training loss: 0.00106
Epoch: 5, step: 462, training loss: 0.00115
Epoch: 5, step: 463, training loss: 0.00182
Epoch: 5, step: 464, training loss: 0.00243
Epoch: 5, step: 465, training loss: 0.00085
Epoch: 5, step: 466, training loss: 0.00070
Epoch: 5, step: 467, training loss: 0.00126
Epoch: 5, step: 468, training loss: 0.00154
Epoch: 5, step: 469, training loss: 0.00172
Epoch: 5, step: 470, training loss: 0.00071
Epoch: 5, step: 471, training loss: 0.00213
Epoch: 5, step: 472, training loss: 0.00254
Epoch: 5, step: 473, training loss: 0.00086
Epoch: 5, step: 474, training loss: 0.00150
Epoch: 5, step: 475, training loss: 0.00072
Epoch: 5, step: 476, training loss: 0.00115
Epoch: 5, step: 477, training loss: 0.00427
Epoch: 5, step: 478, training loss: 0.00382
Epoch: 5, step: 479, training loss: 0.00090
Epoch: 5, step: 480, training loss: 0.00276
Epoch: 5, step: 481, training loss: 0.00233
Epoch: 5, step: 482, training loss: 0.00162
Epoch: 5, step: 483, training loss: 0.00233
Epoch: 5, step: 484, training loss: 0.00213
Epoch: 5, step: 485, training loss: 0.00048
Epoch: 5, step: 486, training loss: 0.00146
Epoch: 5, step: 487, training loss: 0.00120
Epoch: 5, step: 488, training loss: 0.00076
Epoch: 5, step: 489, training loss: 0.00176
Epoch: 5, step: 490, training loss: 0.00080
Epoch: 5, step: 491, training loss: 0.00161
Epoch: 5, step: 492, training loss: 0.00110
Epoch: 5, step: 493, training loss: 0.00190
Epoch: 5, step: 494, training loss: 0.00191
Epoch: 5, step: 495, training loss: 0.00111
Epoch: 5, step: 496, training loss: 0.00059
Epoch: 5, step: 497, training loss: 0.00116
Epoch: 5, step: 498, training loss: 0.00162
Epoch: 5, step: 499, training loss: 0.00124
Epoch: 5, step: 500, training loss: 0.00235
Epoch: 5, step: 501, training loss: 0.00289
Epoch: 5, step: 502, training loss: 0.00218
Epoch: 5, step: 503, training loss: 0.00242
Epoch: 5, step: 504, training loss: 0.00096
Epoch: 5, step: 505, training loss: 0.00329
Epoch: 5, step: 506, training loss: 0.00127
Epoch: 5, step: 507, training loss: 0.00159
Epoch: 5, step: 508, training loss: 0.00088
Epoch: 5, step: 509, training loss: 0.00080
Epoch: 5, step: 510, training loss: 0.00061
Epoch: 5, step: 511, training loss: 0.00163
Epoch: 5, step: 512, training loss: 0.00183
Epoch: 5, step: 513, training loss: 0.00095
Epoch: 5, step: 514, training loss: 0.00081
Epoch: 5, step: 515, training loss: 0.00178
Epoch: 5, step: 516, training loss: 0.00458
Epoch: 5, step: 517, training loss: 0.00353
Epoch: 5, step: 518, training loss: 0.00248
Epoch: 5, step: 519, training loss: 0.00330
Epoch: 5, step: 520, training loss: 0.00067
Epoch: 5, step: 521, training loss: 0.00126
Epoch: 5, step: 522, training loss: 0.00221
Epoch: 5, step: 523, training loss: 0.00091
Epoch: 5, step: 524, training loss: 0.00140
Epoch: 5, step: 525, training loss: 0.00157
Epoch: 5, step: 526, training loss: 0.00214
Epoch: 5, step: 527, training loss: 0.00162
Epoch: 5, step: 528, training loss: 0.00167
Epoch: 5, step: 529, training loss: 0.00055
Epoch: 5, step: 530, training loss: 0.00352
Epoch: 5, step: 531, training loss: 0.00150
Epoch: 5, step: 532, training loss: 0.00140
Epoch: 5, step: 533, training loss: 0.00044
Epoch: 5, step: 534, training loss: 0.00187
Epoch: 5, step: 535, training loss: 0.00115
Epoch: 5, step: 536, training loss: 0.00101
Epoch: 5, step: 537, training loss: 0.00097
Epoch: 5, step: 538, training loss: 0.00151
Epoch: 5, step: 539, training loss: 0.00274
Epoch: 5, step: 540, training loss: 0.00138
Epoch: 5, step: 541, training loss: 0.00230
Epoch: 5, step: 542, training loss: 0.00117
Epoch: 5, step: 543, training loss: 0.00346
Epoch: 5, step: 544, training loss: 0.00046
Epoch: 5, step: 545, training loss: 0.00055
Epoch: 5, step: 546, training loss: 0.00084
Epoch: 5, step: 547, training loss: 0.00184
Epoch: 5, step: 548, training loss: 0.00073
Epoch: 5, step: 549, training loss: 0.00058
Epoch: 5, step: 550, training loss: 0.00145
Epoch: 5, step: 551, training loss: 0.00093
Epoch: 5, step: 552, training loss: 0.00117
Epoch: 5, step: 553, training loss: 0.00184
Epoch: 5, step: 554, training loss: 0.00100
Epoch: 5, step: 555, training loss: 0.00190
Epoch: 5, step: 556, training loss: 0.00187
Epoch: 5, step: 557, training loss: 0.00154
Epoch: 5, step: 558, training loss: 0.00187
Epoch: 5, step: 559, training loss: 0.00064
Epoch: 5, step: 560, training loss: 0.00394
Epoch: 5, step: 561, training loss: 0.00186
Epoch: 5, step: 562, training loss: 0.00179
Epoch: 5, step: 563, training loss: 0.00069
Epoch: 5, step: 564, training loss: 0.00258
Epoch: 5, step: 565, training loss: 0.00043
Epoch: 5, step: 566, training loss: 0.00124
Epoch: 5, step: 567, training loss: 0.00076
Epoch: 5, step: 568, training loss: 0.00163
Epoch: 5, step: 569, training loss: 0.00037
Epoch: 5, step: 570, training loss: 0.00089
Epoch: 5, step: 571, training loss: 0.00025
Epoch: 5, step: 572, training loss: 0.00122
Epoch: 5, step: 573, training loss: 0.00145
Epoch: 5, step: 574, training loss: 0.00169
Epoch: 5, step: 575, training loss: 0.00092
Epoch: 5, step: 576, training loss: 0.00240
Epoch: 5, step: 577, training loss: 0.00072
Epoch: 5, step: 578, training loss: 0.00287
Epoch: 5, step: 579, training loss: 0.00082
Epoch: 5, step: 580, training loss: 0.00163
Epoch: 5, step: 581, training loss: 0.00232
Epoch: 5, step: 582, training loss: 0.00173
Epoch: 5, step: 583, training loss: 0.00117
Epoch: 5, step: 584, training loss: 0.00158
Epoch: 5, step: 585, training loss: 0.00282
Epoch: 5, step: 586, training loss: 0.00113
Epoch: 5, step: 587, training loss: 0.00190
Epoch: 5, step: 588, training loss: 0.00213
Epoch: 5, step: 589, training loss: 0.00083
Epoch: 5, step: 590, training loss: 0.00148
Epoch: 5, step: 591, training loss: 0.00099
Epoch: 5, step: 592, training loss: 0.00244
Epoch: 5, step: 593, training loss: 0.00058
Epoch: 5, step: 594, training loss: 0.00067
Epoch: 5, step: 595, training loss: 0.00084
Epoch: 5, step: 596, training loss: 0.00256
Epoch: 5, step: 597, training loss: 0.00068
Epoch: 5, step: 598, training loss: 0.00101
Epoch: 5, step: 599, training loss: 0.00114
Epoch: 5, step: 600, training loss: 0.00202
Epoch: 5, step: 601, training loss: 0.00203
Epoch: 5, step: 602, training loss: 0.00078
Epoch: 5, step: 603, training loss: 0.00123
Epoch: 5, step: 604, training loss: 0.00119
Epoch: 5, step: 605, training loss: 0.00116
Epoch: 5, step: 606, training loss: 0.00084
Epoch: 5, step: 607, training loss: 0.00051
Epoch: 5, step: 608, training loss: 0.00086
Epoch: 5, step: 609, training loss: 0.00189
Epoch: 5, step: 610, training loss: 0.00078
Epoch: 5, step: 611, training loss: 0.00182
Epoch: 5, step: 612, training loss: 0.00115
Epoch: 5, step: 613, training loss: 0.00100
Epoch: 5, step: 614, training loss: 0.00045
Epoch: 5, step: 615, training loss: 0.00086
Epoch: 5, step: 616, training loss: 0.00142
Epoch: 5, step: 617, training loss: 0.00100
Epoch: 5, step: 618, training loss: 0.00108
Epoch: 5, step: 619, training loss: 0.00196
Epoch: 5, step: 620, training loss: 0.00163
Epoch: 5, step: 621, training loss: 0.00198
Epoch: 5, step: 622, training loss: 0.00256
Epoch: 5, step: 623, training loss: 0.00105
Epoch: 5, step: 624, training loss: 0.00210
Epoch: 5, step: 625, training loss: 0.00068
Epoch: 5, step: 626, training loss: 0.00252
Epoch: 5, step: 627, training loss: 0.00112
Epoch: 5, step: 628, training loss: 0.00057
Epoch: 5, step: 629, training loss: 0.00107
Epoch: 5, step: 630, training loss: 0.00098
Epoch: 5, step: 631, training loss: 0.00215
Epoch: 5, step: 632, training loss: 0.00165
Epoch: 5, step: 633, training loss: 0.00109
Epoch: 5, step: 634, training loss: 0.00166
Epoch: 5, step: 635, training loss: 0.00126
Epoch: 5, step: 636, training loss: 0.00126
Epoch: 5, step: 637, training loss: 0.00091
Epoch: 5, step: 638, training loss: 0.00122
Epoch: 5, step: 639, training loss: 0.00221
Epoch: 5, step: 640, training loss: 0.00158
Epoch: 5, step: 641, training loss: 0.00087
Epoch: 5, step: 642, training loss: 0.00096
Epoch: 5, step: 643, training loss: 0.00222
Epoch: 5, step: 644, training loss: 0.00168
Epoch: 5, step: 645, training loss: 0.00099
Epoch: 5, step: 646, training loss: 0.00079
Epoch: 5, step: 647, training loss: 0.00095
Epoch: 5, step: 648, training loss: 0.00146
Epoch: 5, step: 649, training loss: 0.00650
Epoch: 5, step: 650, training loss: 0.00087
Epoch: 5, step: 651, training loss: 0.00124
Epoch: 5, step: 652, training loss: 0.00123
Epoch: 5, step: 653, training loss: 0.00072
Epoch: 5, step: 654, training loss: 0.00250
Epoch: 5, step: 655, training loss: 0.00124
Epoch: 5, step: 656, training loss: 0.00202
Epoch: 5, step: 657, training loss: 0.00192
Epoch: 5, step: 658, training loss: 0.00238
Epoch: 5, step: 659, training loss: 0.00111
Epoch: 5, step: 660, training loss: 0.00415
Epoch: 5, step: 661, training loss: 0.00081
Epoch: 5, step: 662, training loss: 0.00158
Epoch: 5, step: 663, training loss: 0.00086
Epoch: 5, step: 664, training loss: 0.00206
Epoch: 5, step: 665, training loss: 0.00044
Epoch: 5, step: 666, training loss: 0.00206
Epoch: 5, step: 667, training loss: 0.00035
Epoch: 5, step: 668, training loss: 0.00091
Epoch: 5, step: 669, training loss: 0.00178
Epoch: 5, step: 670, training loss: 0.00132
Epoch: 5, step: 671, training loss: 0.00161
Epoch: 5, step: 672, training loss: 0.00099
Epoch: 5, step: 673, training loss: 0.00312
Epoch: 5, step: 674, training loss: 0.00126
Epoch: 5, step: 675, training loss: 0.00096
Epoch: 5, step: 676, training loss: 0.00433
Epoch: 5, step: 677, training loss: 0.00103
Epoch: 5, step: 678, training loss: 0.00218
Epoch: 5, step: 679, training loss: 0.00074
Epoch: 5, step: 680, training loss: 0.00031
Epoch: 5, step: 681, training loss: 0.00337
Epoch: 5, step: 682, training loss: 0.00071
Epoch: 5, step: 683, training loss: 0.00140
Epoch: 5, step: 684, training loss: 0.00137
Epoch: 5, step: 685, training loss: 0.00038
Epoch: 5, step: 686, training loss: 0.00087
Epoch: 5, step: 687, training loss: 0.00215
Epoch: 5, step: 688, training loss: 0.00129
Epoch: 5, step: 689, training loss: 0.00171
Epoch: 5, step: 690, training loss: 0.00018
Epoch: 5, step: 691, training loss: 0.00210
Epoch: 5, step: 692, training loss: 0.00234
Epoch: 5, step: 693, training loss: 0.00180
Epoch: 5, step: 694, training loss: 0.00029
Epoch: 5, step: 695, training loss: 0.00094
Epoch: 5, step: 696, training loss: 0.00156
Epoch: 5, step: 697, training loss: 0.00083
Epoch: 5, step: 698, training loss: 0.00172
Epoch: 5, step: 699, training loss: 0.00087
Epoch: 5, step: 700, training loss: 0.00120
Epoch: 5, step: 701, training loss: 0.00258
Epoch: 5, step: 702, training loss: 0.00058
Epoch: 5, step: 703, training loss: 0.00239
Epoch: 5, step: 704, training loss: 0.00137
Epoch: 5, step: 705, training loss: 0.00100
Epoch: 5, step: 706, training loss: 0.00042
Epoch: 5, step: 707, training loss: 0.00398
Epoch: 5, step: 708, training loss: 0.00240
Epoch: 5, step: 709, training loss: 0.00264
Epoch: 5, step: 710, training loss: 0.00087
Epoch: 5, step: 711, training loss: 0.00169
Epoch: 5, step: 712, training loss: 0.00061
Epoch: 5, step: 713, training loss: 0.00147
Epoch: 5, step: 714, training loss: 0.00157
Epoch: 5, step: 715, training loss: 0.00125
Epoch: 5, step: 716, training loss: 0.00146
Epoch: 5, step: 717, training loss: 0.00194
Epoch: 5, step: 718, training loss: 0.00181
Epoch: 5, step: 719, training loss: 0.00229
Epoch: 5, step: 720, training loss: 0.00061
Epoch: 5, step: 721, training loss: 0.00018
Epoch: 5, step: 722, training loss: 0.00155
Epoch: 5, step: 723, training loss: 0.00136
Epoch: 5, step: 724, training loss: 0.00097
Epoch: 5, step: 725, training loss: 0.00155
Epoch: 5, step: 726, training loss: 0.00106
Epoch: 5, step: 727, training loss: 0.00214
Epoch: 5, step: 728, training loss: 0.00284
Epoch: 5, step: 729, training loss: 0.00172
Epoch: 5, step: 730, training loss: 0.00124
Epoch: 5, step: 731, training loss: 0.00039
Epoch: 5, step: 732, training loss: 0.00268
Epoch: 5, step: 733, training loss: 0.00207
Epoch: 5, step: 734, training loss: 0.00116
Epoch: 5, step: 735, training loss: 0.00134
Epoch: 5, step: 736, training loss: 0.00137
Epoch: 5, step: 737, training loss: 0.00038
Epoch: 5, step: 738, training loss: 0.00286
Epoch: 5, step: 739, training loss: 0.00110
Epoch: 5, step: 740, training loss: 0.00141
Epoch: 5, step: 741, training loss: 0.00065
Epoch: 5, step: 742, training loss: 0.00046
Epoch: 5, step: 743, training loss: 0.00097
Epoch: 5, step: 744, training loss: 0.00154
Epoch: 5, step: 745, training loss: 0.00220
Epoch: 5, step: 746, training loss: 0.00120
Epoch: 5, step: 747, training loss: 0.00180
Epoch: 5, step: 748, training loss: 0.00147
Epoch: 5, step: 749, training loss: 0.00160
Epoch: 5, step: 750, training loss: 0.00124
Epoch: 5, step: 751, training loss: 0.00106
Epoch: 5, step: 752, training loss: 0.00213
Epoch: 5, step: 753, training loss: 0.00189
Epoch: 5, step: 754, training loss: 0.00073
Epoch: 5, step: 755, training loss: 0.00072
Epoch: 5, step: 756, training loss: 0.00157
Epoch: 5, step: 757, training loss: 0.00115
Epoch: 5, step: 758, training loss: 0.00203
Epoch: 5, step: 759, training loss: 0.00086
Epoch: 5, step: 760, training loss: 0.00096
Epoch: 5, step: 761, training loss: 0.00182
Epoch: 5, step: 762, training loss: 0.00117
Epoch: 5, step: 763, training loss: 0.00202
Epoch: 5, step: 764, training loss: 0.00121
Epoch: 5, step: 765, training loss: 0.00080
Epoch: 5, step: 766, training loss: 0.00143
Epoch: 5, step: 767, training loss: 0.00222
Epoch: 5, step: 768, training loss: 0.00072
Epoch: 5, step: 769, training loss: 0.00200
Epoch: 5, step: 770, training loss: 0.00101
Epoch: 5, step: 771, training loss: 0.00129
Epoch: 5, step: 772, training loss: 0.00200
Epoch: 5, step: 773, training loss: 0.00054
Epoch: 5, step: 774, training loss: 0.00180
Epoch: 5, step: 775, training loss: 0.00101
Epoch: 5, step: 776, training loss: 0.00258
Epoch: 5, step: 777, training loss: 0.00213
Epoch: 5, step: 778, training loss: 0.00094
Epoch: 5, step: 779, training loss: 0.00151
Epoch: 5, step: 780, training loss: 0.00146
Epoch: 5, step: 781, training loss: 0.00169
Epoch: 5, step: 782, training loss: 0.00212
Epoch: 5, step: 783, training loss: 0.00023
Epoch: 5, step: 784, training loss: 0.00041
Epoch: 5, step: 785, training loss: 0.00072
Epoch: 5, step: 786, training loss: 0.00175
Epoch: 5, step: 787, training loss: 0.00163
Epoch: 5, step: 788, training loss: 0.00243
Epoch: 5, step: 789, training loss: 0.00052
Epoch: 5, step: 790, training loss: 0.00080
Epoch: 5, step: 791, training loss: 0.00112
Epoch: 5, step: 792, training loss: 0.00076
Epoch: 5, step: 793, training loss: 0.00156
Epoch: 5, step: 794, training loss: 0.00159
Epoch: 5, step: 795, training loss: 0.00128
Epoch: 5, step: 796, training loss: 0.00314
Epoch: 5, step: 797, training loss: 0.00048
Epoch: 5, step: 798, training loss: 0.00120
Epoch: 5, step: 799, training loss: 0.00125
Epoch: 5, step: 800, training loss: 0.00125
Epoch: 5, step: 801, training loss: 0.00201
Epoch: 5, step: 802, training loss: 0.00059
Epoch: 5, step: 803, training loss: 0.00163
Epoch: 5, step: 804, training loss: 0.00061
Epoch: 5, step: 805, training loss: 0.00121
Epoch: 5, step: 806, training loss: 0.00125
Epoch: 5, step: 807, training loss: 0.00102
Epoch: 5, step: 808, training loss: 0.00147
Epoch: 5, step: 809, training loss: 0.00110
Epoch: 5, step: 810, training loss: 0.00149
Epoch: 5, step: 811, training loss: 0.00098
Epoch: 5, step: 812, training loss: 0.00079
Epoch: 5, step: 813, training loss: 0.00200
Epoch: 5, step: 814, training loss: 0.00410
Epoch: 5, step: 815, training loss: 0.00216
Epoch: 5, step: 816, training loss: 0.00233
Epoch: 5, step: 817, training loss: 0.00194
Epoch: 5, step: 818, training loss: 0.00086
Epoch: 5, step: 819, training loss: 0.00039
Epoch: 5, step: 820, training loss: 0.00174
Epoch: 5, step: 821, training loss: 0.00204
Epoch: 5, step: 822, training loss: 0.00266
Epoch: 5, step: 823, training loss: 0.00116
Epoch: 5, step: 824, training loss: 0.00102
Epoch: 5, step: 825, training loss: 0.00167
Epoch: 5, step: 826, training loss: 0.00099
Epoch: 5, step: 827, training loss: 0.00070
Epoch: 5, step: 828, training loss: 0.00140
Epoch: 5, step: 829, training loss: 0.00158
Epoch: 5, step: 830, training loss: 0.00081
Epoch: 5, step: 831, training loss: 0.00198
Epoch: 5, step: 832, training loss: 0.00264
Epoch: 5, step: 833, training loss: 0.00229
Epoch: 5, step: 834, training loss: 0.00094
Epoch: 5, step: 835, training loss: 0.00061
Epoch: 5, step: 836, training loss: 0.00167
Epoch: 5, step: 837, training loss: 0.00207
Epoch: 5, step: 838, training loss: 0.00078
Epoch: 5, step: 839, training loss: 0.00070
Epoch: 5, step: 840, training loss: 0.00111
Epoch: 5, step: 841, training loss: 0.00116
Epoch: 5, step: 842, training loss: 0.00123
Epoch: 5, step: 843, training loss: 0.00092
Epoch: 5, step: 844, training loss: 0.00098
Epoch: 5, step: 845, training loss: 0.00162
Epoch: 5, step: 846, training loss: 0.00230
Epoch: 5, step: 847, training loss: 0.00176
Epoch: 5, step: 848, training loss: 0.00058
Epoch: 5, step: 849, training loss: 0.00149
Epoch: 5, step: 850, training loss: 0.00217
Epoch: 5, step: 851, training loss: 0.00083
Epoch: 5, step: 852, training loss: 0.00141
Epoch: 5, step: 853, training loss: 0.00114
Epoch: 5, step: 854, training loss: 0.00224
Epoch: 5, step: 855, training loss: 0.00089
Epoch: 5, step: 856, training loss: 0.00101
Epoch: 5, step: 857, training loss: 0.00094
Epoch: 5, step: 858, training loss: 0.00112
Epoch: 5, step: 859, training loss: 0.00285
Epoch: 5, step: 860, training loss: 0.00157
Epoch: 5, step: 861, training loss: 0.00136
Epoch: 5, step: 862, training loss: 0.00091
Epoch: 5, step: 863, training loss: 0.00050
Epoch: 5, step: 864, training loss: 0.00138
Epoch: 5, step: 865, training loss: 0.00098
Epoch: 5, step: 866, training loss: 0.00055
Epoch: 5, step: 867, training loss: 0.00094
Epoch: 5, step: 868, training loss: 0.00075
Epoch: 5, step: 869, training loss: 0.00092
Epoch: 5, step: 870, training loss: 0.00092
Epoch: 5, step: 871, training loss: 0.00115
Epoch: 5, step: 872, training loss: 0.00225
Epoch: 5, step: 873, training loss: 0.00194
Epoch: 5, step: 874, training loss: 0.00054
Epoch: 5, step: 875, training loss: 0.00111
Epoch: 5, step: 876, training loss: 0.00127
Epoch: 5, step: 877, training loss: 0.00081
Epoch: 5, step: 878, training loss: 0.00110
Epoch: 5, step: 879, training loss: 0.00121
Epoch: 5, step: 880, training loss: 0.00164
Epoch: 5, step: 881, training loss: 0.00120
Epoch: 5, step: 882, training loss: 0.00074
Epoch: 5, step: 883, training loss: 0.00195
Epoch: 5, step: 884, training loss: 0.00050
Epoch: 5, step: 885, training loss: 0.00074
Epoch: 5, step: 886, training loss: 0.00195
Epoch: 5, step: 887, training loss: 0.00070
Epoch: 5, step: 888, training loss: 0.00093
Epoch: 5, step: 889, training loss: 0.00022
Epoch: 5, step: 890, training loss: 0.00079
Epoch: 5, step: 891, training loss: 0.00030
Epoch: 5, step: 892, training loss: 0.00097
Epoch: 5, step: 893, training loss: 0.00107
Epoch: 5, step: 894, training loss: 0.00076
Epoch: 5, step: 895, training loss: 0.00054
Epoch: 5, step: 896, training loss: 0.00159
Epoch: 5, step: 897, training loss: 0.00063
Epoch: 5, step: 898, training loss: 0.00085
Epoch: 5, step: 899, training loss: 0.00078
Epoch: 5, step: 900, training loss: 0.00130
Epoch: 5, step: 901, training loss: 0.00123
Epoch: 5, step: 902, training loss: 0.00243
Epoch: 5, step: 903, training loss: 0.00170
Epoch: 5, step: 904, training loss: 0.00501
Epoch: 5, step: 905, training loss: 0.00084
Epoch: 5, step: 906, training loss: 0.00100
Epoch: 5, step: 907, training loss: 0.00156
Epoch: 5, step: 908, training loss: 0.00050
Epoch: 5, step: 909, training loss: 0.00190
Epoch: 5, step: 910, training loss: 0.00245
Epoch: 5, step: 911, training loss: 0.00095
Epoch: 5, step: 912, training loss: 0.00091
Epoch: 5, step: 913, training loss: 0.00108
Epoch: 5, step: 914, training loss: 0.00192
Epoch: 5, step: 915, training loss: 0.00149
Epoch: 5, step: 916, training loss: 0.00069
Epoch: 5, step: 917, training loss: 0.00030
Epoch: 5, step: 918, training loss: 0.00139
Epoch: 5, step: 919, training loss: 0.00080
Epoch: 5, step: 920, training loss: 0.00179
Epoch: 5, step: 921, training loss: 0.00063
Epoch: 5, step: 922, training loss: 0.00164
Epoch: 5, step: 923, training loss: 0.00042
Epoch: 5, step: 924, training loss: 0.00177
Epoch: 5, step: 925, training loss: 0.00182
Epoch: 5, step: 926, training loss: 0.00195
Epoch: 5, step: 927, training loss: 0.00259
Epoch: 5, step: 928, training loss: 0.00203
Epoch: 5, step: 929, training loss: 0.00094
Epoch: 5, step: 930, training loss: 0.00227
Epoch: 5, step: 931, training loss: 0.00078
Epoch: 5, step: 932, training loss: 0.00196
Epoch: 5, step: 933, training loss: 0.00180
Epoch: 5, step: 934, training loss: 0.00053
Epoch: 5, step: 935, training loss: 0.00185
Epoch: 5, step: 936, training loss: 0.00185
Epoch: 5, step: 937, training loss: 0.00159
Epoch: 5, step: 938, training loss: 0.00285
Epoch: 5, step: 939, training loss: 0.00474
Epoch: 5, step: 940, training loss: 0.00160
Epoch: 5, step: 941, training loss: 0.00176
Epoch: 5, step: 942, training loss: 0.00056
Epoch: 5, step: 943, training loss: 0.00074
Epoch: 5, step: 944, training loss: 0.00221
Epoch: 5, step: 945, training loss: 0.00238
Epoch: 5, step: 946, training loss: 0.00083
Epoch: 5, step: 947, training loss: 0.00086
Epoch: 5, step: 948, training loss: 0.00085
Epoch: 5, step: 949, training loss: 0.00101
Epoch: 5, step: 950, training loss: 0.00078
Epoch: 5, step: 951, training loss: 0.00114
Epoch: 5, step: 952, training loss: 0.00156
Epoch: 5, step: 953, training loss: 0.00217
Epoch: 5, step: 954, training loss: 0.00022
Epoch: 5, average training loss: 0.00150
Epoch: 5, F1: 82.05882, average dev loss: 0.00258
Epoch: 6, step: 0, training loss: 0.00043
Epoch: 6, step: 1, training loss: 0.00100
Epoch: 6, step: 2, training loss: 0.00125
Epoch: 6, step: 3, training loss: 0.00040
Epoch: 6, step: 4, training loss: 0.00078
Epoch: 6, step: 5, training loss: 0.00119
Epoch: 6, step: 6, training loss: 0.00114
Epoch: 6, step: 7, training loss: 0.00102
Epoch: 6, step: 8, training loss: 0.00078
Epoch: 6, step: 9, training loss: 0.00035
Epoch: 6, step: 10, training loss: 0.00088
Epoch: 6, step: 11, training loss: 0.00107
Epoch: 6, step: 12, training loss: 0.00088
Epoch: 6, step: 13, training loss: 0.00052
Epoch: 6, step: 14, training loss: 0.00154
Epoch: 6, step: 15, training loss: 0.00115
Epoch: 6, step: 16, training loss: 0.00085
Epoch: 6, step: 17, training loss: 0.00196
Epoch: 6, step: 18, training loss: 0.00072
Epoch: 6, step: 19, training loss: 0.00145
Epoch: 6, step: 20, training loss: 0.00099
Epoch: 6, step: 21, training loss: 0.00145
Epoch: 6, step: 22, training loss: 0.00085
Epoch: 6, step: 23, training loss: 0.00153
Epoch: 6, step: 24, training loss: 0.00057
Epoch: 6, step: 25, training loss: 0.00083
Epoch: 6, step: 26, training loss: 0.00160
Epoch: 6, step: 27, training loss: 0.00073
Epoch: 6, step: 28, training loss: 0.00139
Epoch: 6, step: 29, training loss: 0.00166
Epoch: 6, step: 30, training loss: 0.00101
Epoch: 6, step: 31, training loss: 0.00103
Epoch: 6, step: 32, training loss: 0.00096
Epoch: 6, step: 33, training loss: 0.00073
Epoch: 6, step: 34, training loss: 0.00177
Epoch: 6, step: 35, training loss: 0.00133
Epoch: 6, step: 36, training loss: 0.00213
Epoch: 6, step: 37, training loss: 0.00155
Epoch: 6, step: 38, training loss: 0.00187
Epoch: 6, step: 39, training loss: 0.00086
Epoch: 6, step: 40, training loss: 0.00107
Epoch: 6, step: 41, training loss: 0.00056
Epoch: 6, step: 42, training loss: 0.00039
Epoch: 6, step: 43, training loss: 0.00182
Epoch: 6, step: 44, training loss: 0.00127
Epoch: 6, step: 45, training loss: 0.00151
Epoch: 6, step: 46, training loss: 0.00043
Epoch: 6, step: 47, training loss: 0.00035
Epoch: 6, step: 48, training loss: 0.00094
Epoch: 6, step: 49, training loss: 0.00013
Epoch: 6, step: 50, training loss: 0.00055
Epoch: 6, step: 51, training loss: 0.00203
Epoch: 6, step: 52, training loss: 0.00208
Epoch: 6, step: 53, training loss: 0.00088
Epoch: 6, step: 54, training loss: 0.00135
Epoch: 6, step: 55, training loss: 0.00021
Epoch: 6, step: 56, training loss: 0.00051
Epoch: 6, step: 57, training loss: 0.00100
Epoch: 6, step: 58, training loss: 0.00118
Epoch: 6, step: 59, training loss: 0.00082
Epoch: 6, step: 60, training loss: 0.00133
Epoch: 6, step: 61, training loss: 0.00148
Epoch: 6, step: 62, training loss: 0.00182
Epoch: 6, step: 63, training loss: 0.00043
Epoch: 6, step: 64, training loss: 0.00176
Epoch: 6, step: 65, training loss: 0.00162
Epoch: 6, step: 66, training loss: 0.00082
Epoch: 6, step: 67, training loss: 0.00180
Epoch: 6, step: 68, training loss: 0.00046
Epoch: 6, step: 69, training loss: 0.00113
Epoch: 6, step: 70, training loss: 0.00058
Epoch: 6, step: 71, training loss: 0.00278
Epoch: 6, step: 72, training loss: 0.00170
Epoch: 6, step: 73, training loss: 0.00148
Epoch: 6, step: 74, training loss: 0.00132
Epoch: 6, step: 75, training loss: 0.00120
Epoch: 6, step: 76, training loss: 0.00110
Epoch: 6, step: 77, training loss: 0.00074
Epoch: 6, step: 78, training loss: 0.00100
Epoch: 6, step: 79, training loss: 0.00097
Epoch: 6, step: 80, training loss: 0.00152
Epoch: 6, step: 81, training loss: 0.00213
Epoch: 6, step: 82, training loss: 0.00171
Epoch: 6, step: 83, training loss: 0.00066
Epoch: 6, step: 84, training loss: 0.00135
Epoch: 6, step: 85, training loss: 0.00040
Epoch: 6, step: 86, training loss: 0.00066
Epoch: 6, step: 87, training loss: 0.00088
Epoch: 6, step: 88, training loss: 0.00179
Epoch: 6, step: 89, training loss: 0.00102
Epoch: 6, step: 90, training loss: 0.00103
Epoch: 6, step: 91, training loss: 0.00129
Epoch: 6, step: 92, training loss: 0.00129
Epoch: 6, step: 93, training loss: 0.00118
Epoch: 6, step: 94, training loss: 0.00142
Epoch: 6, step: 95, training loss: 0.00055
Epoch: 6, step: 96, training loss: 0.00093
Epoch: 6, step: 97, training loss: 0.00049
Epoch: 6, step: 98, training loss: 0.00059
Epoch: 6, step: 99, training loss: 0.00247
Epoch: 6, step: 100, training loss: 0.00082
Epoch: 6, step: 101, training loss: 0.00156
Epoch: 6, step: 102, training loss: 0.00119
Epoch: 6, step: 103, training loss: 0.00084
Epoch: 6, step: 104, training loss: 0.00084
Epoch: 6, step: 105, training loss: 0.00054
Epoch: 6, step: 106, training loss: 0.00064
Epoch: 6, step: 107, training loss: 0.00114
Epoch: 6, step: 108, training loss: 0.00019
Epoch: 6, step: 109, training loss: 0.00173
Epoch: 6, step: 110, training loss: 0.00064
Epoch: 6, step: 111, training loss: 0.00056
Epoch: 6, step: 112, training loss: 0.00103
Epoch: 6, step: 113, training loss: 0.00076
Epoch: 6, step: 114, training loss: 0.00033
Epoch: 6, step: 115, training loss: 0.00081
Epoch: 6, step: 116, training loss: 0.00062
Epoch: 6, step: 117, training loss: 0.00601
Epoch: 6, step: 118, training loss: 0.00153
Epoch: 6, step: 119, training loss: 0.00187
Epoch: 6, step: 120, training loss: 0.00224
Epoch: 6, step: 121, training loss: 0.00055
Epoch: 6, step: 122, training loss: 0.00162
Epoch: 6, step: 123, training loss: 0.00076
Epoch: 6, step: 124, training loss: 0.00128
Epoch: 6, step: 125, training loss: 0.00126
Epoch: 6, step: 126, training loss: 0.00137
Epoch: 6, step: 127, training loss: 0.00071
Epoch: 6, step: 128, training loss: 0.00197
Epoch: 6, step: 129, training loss: 0.00069
Epoch: 6, step: 130, training loss: 0.00142
Epoch: 6, step: 131, training loss: 0.00041
Epoch: 6, step: 132, training loss: 0.00105
Epoch: 6, step: 133, training loss: 0.00191
Epoch: 6, step: 134, training loss: 0.00348
Epoch: 6, step: 135, training loss: 0.00174
Epoch: 6, step: 136, training loss: 0.00140
Epoch: 6, step: 137, training loss: 0.00186
Epoch: 6, step: 138, training loss: 0.00106
Epoch: 6, step: 139, training loss: 0.00126
Epoch: 6, step: 140, training loss: 0.00341
Epoch: 6, step: 141, training loss: 0.00106
Epoch: 6, step: 142, training loss: 0.00134
Epoch: 6, step: 143, training loss: 0.00087
Epoch: 6, step: 144, training loss: 0.00196
Epoch: 6, step: 145, training loss: 0.00250
Epoch: 6, step: 146, training loss: 0.00216
Epoch: 6, step: 147, training loss: 0.00161
Epoch: 6, step: 148, training loss: 0.00123
Epoch: 6, step: 149, training loss: 0.00328
Epoch: 6, step: 150, training loss: 0.00065
Epoch: 6, step: 151, training loss: 0.00479
Epoch: 6, step: 152, training loss: 0.00120
Epoch: 6, step: 153, training loss: 0.00283
Epoch: 6, step: 154, training loss: 0.00090
Epoch: 6, step: 155, training loss: 0.00026
Epoch: 6, step: 156, training loss: 0.00140
Epoch: 6, step: 157, training loss: 0.00061
Epoch: 6, step: 158, training loss: 0.00064
Epoch: 6, step: 159, training loss: 0.00114
Epoch: 6, step: 160, training loss: 0.00078
Epoch: 6, step: 161, training loss: 0.00091
Epoch: 6, step: 162, training loss: 0.00121
Epoch: 6, step: 163, training loss: 0.00053
Epoch: 6, step: 164, training loss: 0.00178
Epoch: 6, step: 165, training loss: 0.00068
Epoch: 6, step: 166, training loss: 0.00138
Epoch: 6, step: 167, training loss: 0.00153
Epoch: 6, step: 168, training loss: 0.00123
Epoch: 6, step: 169, training loss: 0.00166
Epoch: 6, step: 170, training loss: 0.00043
Epoch: 6, step: 171, training loss: 0.00246
Epoch: 6, step: 172, training loss: 0.00070
Epoch: 6, step: 173, training loss: 0.00154
Epoch: 6, step: 174, training loss: 0.00134
Epoch: 6, step: 175, training loss: 0.00109
Epoch: 6, step: 176, training loss: 0.00252
Epoch: 6, step: 177, training loss: 0.00237
Epoch: 6, step: 178, training loss: 0.00127
Epoch: 6, step: 179, training loss: 0.00065
Epoch: 6, step: 180, training loss: 0.00153
Epoch: 6, step: 181, training loss: 0.00199
Epoch: 6, step: 182, training loss: 0.00147
Epoch: 6, step: 183, training loss: 0.00038
Epoch: 6, step: 184, training loss: 0.00088
Epoch: 6, step: 185, training loss: 0.00152
Epoch: 6, step: 186, training loss: 0.00059
Epoch: 6, step: 187, training loss: 0.00191
Epoch: 6, step: 188, training loss: 0.00203
Epoch: 6, step: 189, training loss: 0.00074
Epoch: 6, step: 190, training loss: 0.00189
Epoch: 6, step: 191, training loss: 0.00052
Epoch: 6, step: 192, training loss: 0.00194
Epoch: 6, step: 193, training loss: 0.00071
Epoch: 6, step: 194, training loss: 0.00153
Epoch: 6, step: 195, training loss: 0.00174
Epoch: 6, step: 196, training loss: 0.00178
Epoch: 6, step: 197, training loss: 0.00103
Epoch: 6, step: 198, training loss: 0.00112
Epoch: 6, step: 199, training loss: 0.00159
Epoch: 6, step: 200, training loss: 0.00099
Epoch: 6, step: 201, training loss: 0.00220
Epoch: 6, step: 202, training loss: 0.00216
Epoch: 6, step: 203, training loss: 0.00173
Epoch: 6, step: 204, training loss: 0.00146
Epoch: 6, step: 205, training loss: 0.00136
Epoch: 6, step: 206, training loss: 0.00187
Epoch: 6, step: 207, training loss: 0.00161
Epoch: 6, step: 208, training loss: 0.00188
Epoch: 6, step: 209, training loss: 0.00181
Epoch: 6, step: 210, training loss: 0.00092
Epoch: 6, step: 211, training loss: 0.00049
Epoch: 6, step: 212, training loss: 0.00139
Epoch: 6, step: 213, training loss: 0.00084
Epoch: 6, step: 214, training loss: 0.00224
Epoch: 6, step: 215, training loss: 0.00143
Epoch: 6, step: 216, training loss: 0.00185
Epoch: 6, step: 217, training loss: 0.00095
Epoch: 6, step: 218, training loss: 0.00152
Epoch: 6, step: 219, training loss: 0.00147
Epoch: 6, step: 220, training loss: 0.00150
Epoch: 6, step: 221, training loss: 0.00099
Epoch: 6, step: 222, training loss: 0.00108
Epoch: 6, step: 223, training loss: 0.00361
Epoch: 6, step: 224, training loss: 0.00171
Epoch: 6, step: 225, training loss: 0.00108
Epoch: 6, step: 226, training loss: 0.00113
Epoch: 6, step: 227, training loss: 0.00143
Epoch: 6, step: 228, training loss: 0.00078
Epoch: 6, step: 229, training loss: 0.00230
Epoch: 6, step: 230, training loss: 0.00091
Epoch: 6, step: 231, training loss: 0.00073
Epoch: 6, step: 232, training loss: 0.00063
Epoch: 6, step: 233, training loss: 0.00075
Epoch: 6, step: 234, training loss: 0.00145
Epoch: 6, step: 235, training loss: 0.00221
Epoch: 6, step: 236, training loss: 0.00077
Epoch: 6, step: 237, training loss: 0.00076
Epoch: 6, step: 238, training loss: 0.00382
Epoch: 6, step: 239, training loss: 0.00236
Epoch: 6, step: 240, training loss: 0.00125
Epoch: 6, step: 241, training loss: 0.00248
Epoch: 6, step: 242, training loss: 0.00103
Epoch: 6, step: 243, training loss: 0.00135
Epoch: 6, step: 244, training loss: 0.00069
Epoch: 6, step: 245, training loss: 0.00195
Epoch: 6, step: 246, training loss: 0.00197
Epoch: 6, step: 247, training loss: 0.00177
Epoch: 6, step: 248, training loss: 0.00122
Epoch: 6, step: 249, training loss: 0.00110
Epoch: 6, step: 250, training loss: 0.00132
Epoch: 6, step: 251, training loss: 0.00206
Epoch: 6, step: 252, training loss: 0.00095
Epoch: 6, step: 253, training loss: 0.00226
Epoch: 6, step: 254, training loss: 0.00117
Epoch: 6, step: 255, training loss: 0.00228
Epoch: 6, step: 256, training loss: 0.00208
Epoch: 6, step: 257, training loss: 0.00042
Epoch: 6, step: 258, training loss: 0.00059
Epoch: 6, step: 259, training loss: 0.00185
Epoch: 6, step: 260, training loss: 0.00046
Epoch: 6, step: 261, training loss: 0.00150
Epoch: 6, step: 262, training loss: 0.00164
Epoch: 6, step: 263, training loss: 0.00082
Epoch: 6, step: 264, training loss: 0.00075
Epoch: 6, step: 265, training loss: 0.00054
Epoch: 6, step: 266, training loss: 0.00092
Epoch: 6, step: 267, training loss: 0.00120
Epoch: 6, step: 268, training loss: 0.00100
Epoch: 6, step: 269, training loss: 0.00230
Epoch: 6, step: 270, training loss: 0.00132
Epoch: 6, step: 271, training loss: 0.00357
Epoch: 6, step: 272, training loss: 0.00125
Epoch: 6, step: 273, training loss: 0.00181
Epoch: 6, step: 274, training loss: 0.00083
Epoch: 6, step: 275, training loss: 0.00091
Epoch: 6, step: 276, training loss: 0.00101
Epoch: 6, step: 277, training loss: 0.00125
Epoch: 6, step: 278, training loss: 0.00074
Epoch: 6, step: 279, training loss: 0.00111
Epoch: 6, step: 280, training loss: 0.00048
Epoch: 6, step: 281, training loss: 0.00135
Epoch: 6, step: 282, training loss: 0.00149
Epoch: 6, step: 283, training loss: 0.00071
Epoch: 6, step: 284, training loss: 0.00063
Epoch: 6, step: 285, training loss: 0.00340
Epoch: 6, step: 286, training loss: 0.00222
Epoch: 6, step: 287, training loss: 0.00116
Epoch: 6, step: 288, training loss: 0.00144
Epoch: 6, step: 289, training loss: 0.00106
Epoch: 6, step: 290, training loss: 0.00108
Epoch: 6, step: 291, training loss: 0.00228
Epoch: 6, step: 292, training loss: 0.00036
Epoch: 6, step: 293, training loss: 0.00165
Epoch: 6, step: 294, training loss: 0.00149
Epoch: 6, step: 295, training loss: 0.00089
Epoch: 6, step: 296, training loss: 0.00173
Epoch: 6, step: 297, training loss: 0.00102
Epoch: 6, step: 298, training loss: 0.00261
Epoch: 6, step: 299, training loss: 0.00087
Epoch: 6, step: 300, training loss: 0.00157
Epoch: 6, step: 301, training loss: 0.00287
Epoch: 6, step: 302, training loss: 0.00118
Epoch: 6, step: 303, training loss: 0.00093
Epoch: 6, step: 304, training loss: 0.00082
Epoch: 6, step: 305, training loss: 0.00174
Epoch: 6, step: 306, training loss: 0.00107
Epoch: 6, step: 307, training loss: 0.00113
Epoch: 6, step: 308, training loss: 0.00073
Epoch: 6, step: 309, training loss: 0.00083
Epoch: 6, step: 310, training loss: 0.00269
Epoch: 6, step: 311, training loss: 0.00109
Epoch: 6, step: 312, training loss: 0.00075
Epoch: 6, step: 313, training loss: 0.00206
Epoch: 6, step: 314, training loss: 0.00110
Epoch: 6, step: 315, training loss: 0.00028
Epoch: 6, step: 316, training loss: 0.00217
Epoch: 6, step: 317, training loss: 0.00127
Epoch: 6, step: 318, training loss: 0.00102
Epoch: 6, step: 319, training loss: 0.00315
Epoch: 6, step: 320, training loss: 0.00055
Epoch: 6, step: 321, training loss: 0.00142
Epoch: 6, step: 322, training loss: 0.00218
Epoch: 6, step: 323, training loss: 0.00108
Epoch: 6, step: 324, training loss: 0.00111
Epoch: 6, step: 325, training loss: 0.00096
Epoch: 6, step: 326, training loss: 0.00033
Epoch: 6, step: 327, training loss: 0.00164
Epoch: 6, step: 328, training loss: 0.00112
Epoch: 6, step: 329, training loss: 0.00108
Epoch: 6, step: 330, training loss: 0.00067
Epoch: 6, step: 331, training loss: 0.00108
Epoch: 6, step: 332, training loss: 0.00146
Epoch: 6, step: 333, training loss: 0.00186
Epoch: 6, step: 334, training loss: 0.00045
Epoch: 6, step: 335, training loss: 0.00093
Epoch: 6, step: 336, training loss: 0.00091
Epoch: 6, step: 337, training loss: 0.00068
Epoch: 6, step: 338, training loss: 0.00064
Epoch: 6, step: 339, training loss: 0.00110
Epoch: 6, step: 340, training loss: 0.00314
Epoch: 6, step: 341, training loss: 0.00187
Epoch: 6, step: 342, training loss: 0.00054
Epoch: 6, step: 343, training loss: 0.00177
Epoch: 6, step: 344, training loss: 0.00173
Epoch: 6, step: 345, training loss: 0.00108
Epoch: 6, step: 346, training loss: 0.00177
Epoch: 6, step: 347, training loss: 0.00082
Epoch: 6, step: 348, training loss: 0.00093
Epoch: 6, step: 349, training loss: 0.00042
Epoch: 6, step: 350, training loss: 0.00133
Epoch: 6, step: 351, training loss: 0.00133
Epoch: 6, step: 352, training loss: 0.00099
Epoch: 6, step: 353, training loss: 0.00065
Epoch: 6, step: 354, training loss: 0.00187
Epoch: 6, step: 355, training loss: 0.00358
Epoch: 6, step: 356, training loss: 0.00093
Epoch: 6, step: 357, training loss: 0.00157
Epoch: 6, step: 358, training loss: 0.00055
Epoch: 6, step: 359, training loss: 0.00130
Epoch: 6, step: 360, training loss: 0.00066
Epoch: 6, step: 361, training loss: 0.00029
Epoch: 6, step: 362, training loss: 0.00192
Epoch: 6, step: 363, training loss: 0.00089
Epoch: 6, step: 364, training loss: 0.00034
Epoch: 6, step: 365, training loss: 0.00080
Epoch: 6, step: 366, training loss: 0.00224
Epoch: 6, step: 367, training loss: 0.00126
Epoch: 6, step: 368, training loss: 0.00115
Epoch: 6, step: 369, training loss: 0.00169
Epoch: 6, step: 370, training loss: 0.00103
Epoch: 6, step: 371, training loss: 0.00094
Epoch: 6, step: 372, training loss: 0.00175
Epoch: 6, step: 373, training loss: 0.00048
Epoch: 6, step: 374, training loss: 0.00058
Epoch: 6, step: 375, training loss: 0.00066
Epoch: 6, step: 376, training loss: 0.00135
Epoch: 6, step: 377, training loss: 0.00062
Epoch: 6, step: 378, training loss: 0.00053
Epoch: 6, step: 379, training loss: 0.00081
Epoch: 6, step: 380, training loss: 0.00100
Epoch: 6, step: 381, training loss: 0.00146
Epoch: 6, step: 382, training loss: 0.00182
Epoch: 6, step: 383, training loss: 0.00153
Epoch: 6, step: 384, training loss: 0.00054
Epoch: 6, step: 385, training loss: 0.00100
Epoch: 6, step: 386, training loss: 0.00130
Epoch: 6, step: 387, training loss: 0.00087
Epoch: 6, step: 388, training loss: 0.00172
Epoch: 6, step: 389, training loss: 0.00078
Epoch: 6, step: 390, training loss: 0.00096
Epoch: 6, step: 391, training loss: 0.00077
Epoch: 6, step: 392, training loss: 0.00162
Epoch: 6, step: 393, training loss: 0.00066
Epoch: 6, step: 394, training loss: 0.00302
Epoch: 6, step: 395, training loss: 0.00172
Epoch: 6, step: 396, training loss: 0.00064
Epoch: 6, step: 397, training loss: 0.00146
Epoch: 6, step: 398, training loss: 0.00066
Epoch: 6, step: 399, training loss: 0.00102
Epoch: 6, step: 400, training loss: 0.00039
Epoch: 6, step: 401, training loss: 0.00140
Epoch: 6, step: 402, training loss: 0.00119
Epoch: 6, step: 403, training loss: 0.00117
Epoch: 6, step: 404, training loss: 0.00076
Epoch: 6, step: 405, training loss: 0.00173
Epoch: 6, step: 406, training loss: 0.00136
Epoch: 6, step: 407, training loss: 0.00160
Epoch: 6, step: 408, training loss: 0.00124
Epoch: 6, step: 409, training loss: 0.00140
Epoch: 6, step: 410, training loss: 0.00086
Epoch: 6, step: 411, training loss: 0.00169
Epoch: 6, step: 412, training loss: 0.00084
Epoch: 6, step: 413, training loss: 0.00103
Epoch: 6, step: 414, training loss: 0.00112
Epoch: 6, step: 415, training loss: 0.00084
Epoch: 6, step: 416, training loss: 0.00114
Epoch: 6, step: 417, training loss: 0.00115
Epoch: 6, step: 418, training loss: 0.00179
Epoch: 6, step: 419, training loss: 0.00048
Epoch: 6, step: 420, training loss: 0.00069
Epoch: 6, step: 421, training loss: 0.00042
Epoch: 6, step: 422, training loss: 0.00179
Epoch: 6, step: 423, training loss: 0.00056
Epoch: 6, step: 424, training loss: 0.00091
Epoch: 6, step: 425, training loss: 0.00081
Epoch: 6, step: 426, training loss: 0.00149
Epoch: 6, step: 427, training loss: 0.00077
Epoch: 6, step: 428, training loss: 0.00103
Epoch: 6, step: 429, training loss: 0.00064
Epoch: 6, step: 430, training loss: 0.00087
Epoch: 6, step: 431, training loss: 0.00117
Epoch: 6, step: 432, training loss: 0.00062
Epoch: 6, step: 433, training loss: 0.00121
Epoch: 6, step: 434, training loss: 0.00094
Epoch: 6, step: 435, training loss: 0.00096
Epoch: 6, step: 436, training loss: 0.00107
Epoch: 6, step: 437, training loss: 0.00250
Epoch: 6, step: 438, training loss: 0.00156
Epoch: 6, step: 439, training loss: 0.00093
Epoch: 6, step: 440, training loss: 0.00079
Epoch: 6, step: 441, training loss: 0.00185
Epoch: 6, step: 442, training loss: 0.00082
Epoch: 6, step: 443, training loss: 0.00084
Epoch: 6, step: 444, training loss: 0.00085
Epoch: 6, step: 445, training loss: 0.00274
Epoch: 6, step: 446, training loss: 0.00113
Epoch: 6, step: 447, training loss: 0.00061
Epoch: 6, step: 448, training loss: 0.00053
Epoch: 6, step: 449, training loss: 0.00198
Epoch: 6, step: 450, training loss: 0.00051
Epoch: 6, step: 451, training loss: 0.00183
Epoch: 6, step: 452, training loss: 0.00024
Epoch: 6, step: 453, training loss: 0.00121
Epoch: 6, step: 454, training loss: 0.00286
Epoch: 6, step: 455, training loss: 0.00081
Epoch: 6, step: 456, training loss: 0.00221
Epoch: 6, step: 457, training loss: 0.00143
Epoch: 6, step: 458, training loss: 0.00413
Epoch: 6, step: 459, training loss: 0.00141
Epoch: 6, step: 460, training loss: 0.00078
Epoch: 6, step: 461, training loss: 0.00037
Epoch: 6, step: 462, training loss: 0.00206
Epoch: 6, step: 463, training loss: 0.00106
Epoch: 6, step: 464, training loss: 0.00234
Epoch: 6, step: 465, training loss: 0.00064
Epoch: 6, step: 466, training loss: 0.00059
Epoch: 6, step: 467, training loss: 0.00069
Epoch: 6, step: 468, training loss: 0.00240
Epoch: 6, step: 469, training loss: 0.00103
Epoch: 6, step: 470, training loss: 0.00084
Epoch: 6, step: 471, training loss: 0.00231
Epoch: 6, step: 472, training loss: 0.00114
Epoch: 6, step: 473, training loss: 0.00020
Epoch: 6, step: 474, training loss: 0.00177
Epoch: 6, step: 475, training loss: 0.00076
Epoch: 6, step: 476, training loss: 0.00176
Epoch: 6, step: 477, training loss: 0.00217
Epoch: 6, step: 478, training loss: 0.00119
Epoch: 6, step: 479, training loss: 0.00126
Epoch: 6, step: 480, training loss: 0.00237
Epoch: 6, step: 481, training loss: 0.00153
Epoch: 6, step: 482, training loss: 0.00083
Epoch: 6, step: 483, training loss: 0.00080
Epoch: 6, step: 484, training loss: 0.00093
Epoch: 6, step: 485, training loss: 0.00150
Epoch: 6, step: 486, training loss: 0.00168
Epoch: 6, step: 487, training loss: 0.00076
Epoch: 6, step: 488, training loss: 0.00167
Epoch: 6, step: 489, training loss: 0.00055
Epoch: 6, step: 490, training loss: 0.00030
Epoch: 6, step: 491, training loss: 0.00086
Epoch: 6, step: 492, training loss: 0.00219
Epoch: 6, step: 493, training loss: 0.00070
Epoch: 6, step: 494, training loss: 0.00067
Epoch: 6, step: 495, training loss: 0.00216
Epoch: 6, step: 496, training loss: 0.00054
Epoch: 6, step: 497, training loss: 0.00072
Epoch: 6, step: 498, training loss: 0.00173
Epoch: 6, step: 499, training loss: 0.00170
Epoch: 6, step: 500, training loss: 0.00111
Epoch: 6, step: 501, training loss: 0.00178
Epoch: 6, step: 502, training loss: 0.00215
Epoch: 6, step: 503, training loss: 0.00151
Epoch: 6, step: 504, training loss: 0.00182
Epoch: 6, step: 505, training loss: 0.00233
Epoch: 6, step: 506, training loss: 0.00099
Epoch: 6, step: 507, training loss: 0.00086
Epoch: 6, step: 508, training loss: 0.00129
Epoch: 6, step: 509, training loss: 0.00052
Epoch: 6, step: 510, training loss: 0.00091
Epoch: 6, step: 511, training loss: 0.00143
Epoch: 6, step: 512, training loss: 0.00240
Epoch: 6, step: 513, training loss: 0.00111
Epoch: 6, step: 514, training loss: 0.00167
Epoch: 6, step: 515, training loss: 0.00019
Epoch: 6, step: 516, training loss: 0.00112
Epoch: 6, step: 517, training loss: 0.00106
Epoch: 6, step: 518, training loss: 0.00244
Epoch: 6, step: 519, training loss: 0.00070
Epoch: 6, step: 520, training loss: 0.00115
Epoch: 6, step: 521, training loss: 0.00151
Epoch: 6, step: 522, training loss: 0.00060
Epoch: 6, step: 523, training loss: 0.00094
Epoch: 6, step: 524, training loss: 0.00108
Epoch: 6, step: 525, training loss: 0.00232
Epoch: 6, step: 526, training loss: 0.00387
Epoch: 6, step: 527, training loss: 0.00096
Epoch: 6, step: 528, training loss: 0.00206
Epoch: 6, step: 529, training loss: 0.00113
Epoch: 6, step: 530, training loss: 0.00035
Epoch: 6, step: 531, training loss: 0.00026
Epoch: 6, step: 532, training loss: 0.00082
Epoch: 6, step: 533, training loss: 0.00306
Epoch: 6, step: 534, training loss: 0.00076
Epoch: 6, step: 535, training loss: 0.00099
Epoch: 6, step: 536, training loss: 0.00019
Epoch: 6, step: 537, training loss: 0.00120
Epoch: 6, step: 538, training loss: 0.00037
Epoch: 6, step: 539, training loss: 0.00153
Epoch: 6, step: 540, training loss: 0.00129
Epoch: 6, step: 541, training loss: 0.00181
Epoch: 6, step: 542, training loss: 0.00074
Epoch: 6, step: 543, training loss: 0.00180
Epoch: 6, step: 544, training loss: 0.00135
Epoch: 6, step: 545, training loss: 0.00123
Epoch: 6, step: 546, training loss: 0.00083
Epoch: 6, step: 547, training loss: 0.00133
Epoch: 6, step: 548, training loss: 0.00148
Epoch: 6, step: 549, training loss: 0.00094
Epoch: 6, step: 550, training loss: 0.00124
Epoch: 6, step: 551, training loss: 0.00167
Epoch: 6, step: 552, training loss: 0.00096
Epoch: 6, step: 553, training loss: 0.00135
Epoch: 6, step: 554, training loss: 0.00182
Epoch: 6, step: 555, training loss: 0.00092
Epoch: 6, step: 556, training loss: 0.00130
Epoch: 6, step: 557, training loss: 0.00063
Epoch: 6, step: 558, training loss: 0.00040
Epoch: 6, step: 559, training loss: 0.00222
Epoch: 6, step: 560, training loss: 0.00057
Epoch: 6, step: 561, training loss: 0.00062
Epoch: 6, step: 562, training loss: 0.00111
Epoch: 6, step: 563, training loss: 0.00081
Epoch: 6, step: 564, training loss: 0.00097
Epoch: 6, step: 565, training loss: 0.00137
Epoch: 6, step: 566, training loss: 0.00143
Epoch: 6, step: 567, training loss: 0.00160
Epoch: 6, step: 568, training loss: 0.00070
Epoch: 6, step: 569, training loss: 0.00089
Epoch: 6, step: 570, training loss: 0.00076
Epoch: 6, step: 571, training loss: 0.00125
Epoch: 6, step: 572, training loss: 0.00066
Epoch: 6, step: 573, training loss: 0.00165
Epoch: 6, step: 574, training loss: 0.00165
Epoch: 6, step: 575, training loss: 0.00112
Epoch: 6, step: 576, training loss: 0.00111
Epoch: 6, step: 577, training loss: 0.00122
Epoch: 6, step: 578, training loss: 0.00152
Epoch: 6, step: 579, training loss: 0.00065
Epoch: 6, step: 580, training loss: 0.00152
Epoch: 6, step: 581, training loss: 0.00089
Epoch: 6, step: 582, training loss: 0.00019
Epoch: 6, step: 583, training loss: 0.00019
Epoch: 6, step: 584, training loss: 0.00112
Epoch: 6, step: 585, training loss: 0.00089
Epoch: 6, step: 586, training loss: 0.00189
Epoch: 6, step: 587, training loss: 0.00101
Epoch: 6, step: 588, training loss: 0.00073
Epoch: 6, step: 589, training loss: 0.00104
Epoch: 6, step: 590, training loss: 0.00229
Epoch: 6, step: 591, training loss: 0.00129
Epoch: 6, step: 592, training loss: 0.00190
Epoch: 6, step: 593, training loss: 0.00029
Epoch: 6, step: 594, training loss: 0.00091
Epoch: 6, step: 595, training loss: 0.00087
Epoch: 6, step: 596, training loss: 0.00062
Epoch: 6, step: 597, training loss: 0.00052
Epoch: 6, step: 598, training loss: 0.00106
Epoch: 6, step: 599, training loss: 0.00123
Epoch: 6, step: 600, training loss: 0.00171
Epoch: 6, step: 601, training loss: 0.00212
Epoch: 6, step: 602, training loss: 0.00330
Epoch: 6, step: 603, training loss: 0.00099
Epoch: 6, step: 604, training loss: 0.00201
Epoch: 6, step: 605, training loss: 0.00050
Epoch: 6, step: 606, training loss: 0.00029
Epoch: 6, step: 607, training loss: 0.00161
Epoch: 6, step: 608, training loss: 0.00197
Epoch: 6, step: 609, training loss: 0.00098
Epoch: 6, step: 610, training loss: 0.00126
Epoch: 6, step: 611, training loss: 0.00104
Epoch: 6, step: 612, training loss: 0.00050
Epoch: 6, step: 613, training loss: 0.00074
Epoch: 6, step: 614, training loss: 0.00125
Epoch: 6, step: 615, training loss: 0.00101
Epoch: 6, step: 616, training loss: 0.00161
Epoch: 6, step: 617, training loss: 0.00023
Epoch: 6, step: 618, training loss: 0.00173
Epoch: 6, step: 619, training loss: 0.00175
Epoch: 6, step: 620, training loss: 0.00126
Epoch: 6, step: 621, training loss: 0.00039
Epoch: 6, step: 622, training loss: 0.00194
Epoch: 6, step: 623, training loss: 0.00302
Epoch: 6, step: 624, training loss: 0.00059
Epoch: 6, step: 625, training loss: 0.00167
Epoch: 6, step: 626, training loss: 0.00075
Epoch: 6, step: 627, training loss: 0.00094
Epoch: 6, step: 628, training loss: 0.00165
Epoch: 6, step: 629, training loss: 0.00073
Epoch: 6, step: 630, training loss: 0.00119
Epoch: 6, step: 631, training loss: 0.00145
Epoch: 6, step: 632, training loss: 0.00080
Epoch: 6, step: 633, training loss: 0.00058
Epoch: 6, step: 634, training loss: 0.00158
Epoch: 6, step: 635, training loss: 0.00110
Epoch: 6, step: 636, training loss: 0.00059
Epoch: 6, step: 637, training loss: 0.00078
Epoch: 6, step: 638, training loss: 0.00081
Epoch: 6, step: 639, training loss: 0.00130
Epoch: 6, step: 640, training loss: 0.00072
Epoch: 6, step: 641, training loss: 0.00170
Epoch: 6, step: 642, training loss: 0.00268
Epoch: 6, step: 643, training loss: 0.00233
Epoch: 6, step: 644, training loss: 0.00099
Epoch: 6, step: 645, training loss: 0.00235
Epoch: 6, step: 646, training loss: 0.00066
Epoch: 6, step: 647, training loss: 0.00114
Epoch: 6, step: 648, training loss: 0.00159
Epoch: 6, step: 649, training loss: 0.00068
Epoch: 6, step: 650, training loss: 0.00064
Epoch: 6, step: 651, training loss: 0.00193
Epoch: 6, step: 652, training loss: 0.00162
Epoch: 6, step: 653, training loss: 0.00170
Epoch: 6, step: 654, training loss: 0.00074
Epoch: 6, step: 655, training loss: 0.00073
Epoch: 6, step: 656, training loss: 0.00278
Epoch: 6, step: 657, training loss: 0.00044
Epoch: 6, step: 658, training loss: 0.00074
Epoch: 6, step: 659, training loss: 0.00075
Epoch: 6, step: 660, training loss: 0.00170
Epoch: 6, step: 661, training loss: 0.00110
Epoch: 6, step: 662, training loss: 0.00145
Epoch: 6, step: 663, training loss: 0.00194
Epoch: 6, step: 664, training loss: 0.00130
Epoch: 6, step: 665, training loss: 0.00073
Epoch: 6, step: 666, training loss: 0.00161
Epoch: 6, step: 667, training loss: 0.00079
Epoch: 6, step: 668, training loss: 0.00086
Epoch: 6, step: 669, training loss: 0.00099
Epoch: 6, step: 670, training loss: 0.00091
Epoch: 6, step: 671, training loss: 0.00097
Epoch: 6, step: 672, training loss: 0.00215
Epoch: 6, step: 673, training loss: 0.00094
Epoch: 6, step: 674, training loss: 0.00204
Epoch: 6, step: 675, training loss: 0.00165
Epoch: 6, step: 676, training loss: 0.00114
Epoch: 6, step: 677, training loss: 0.00076
Epoch: 6, step: 678, training loss: 0.00069
Epoch: 6, step: 679, training loss: 0.00222
Epoch: 6, step: 680, training loss: 0.00277
Epoch: 6, step: 681, training loss: 0.00186
Epoch: 6, step: 682, training loss: 0.00068
Epoch: 6, step: 683, training loss: 0.00115
Epoch: 6, step: 684, training loss: 0.00141
Epoch: 6, step: 685, training loss: 0.00058
Epoch: 6, step: 686, training loss: 0.00200
Epoch: 6, step: 687, training loss: 0.00085
Epoch: 6, step: 688, training loss: 0.00745
Epoch: 6, step: 689, training loss: 0.00159
Epoch: 6, step: 690, training loss: 0.00084
Epoch: 6, step: 691, training loss: 0.00121
Epoch: 6, step: 692, training loss: 0.00085
Epoch: 6, step: 693, training loss: 0.00111
Epoch: 6, step: 694, training loss: 0.00150
Epoch: 6, step: 695, training loss: 0.00110
Epoch: 6, step: 696, training loss: 0.00093
Epoch: 6, step: 697, training loss: 0.00082
Epoch: 6, step: 698, training loss: 0.00090
Epoch: 6, step: 699, training loss: 0.00048
Epoch: 6, step: 700, training loss: 0.00063
Epoch: 6, step: 701, training loss: 0.00057
Epoch: 6, step: 702, training loss: 0.00089
Epoch: 6, step: 703, training loss: 0.00177
Epoch: 6, step: 704, training loss: 0.00132
Epoch: 6, step: 705, training loss: 0.00068
Epoch: 6, step: 706, training loss: 0.00270
Epoch: 6, step: 707, training loss: 0.00145
Epoch: 6, step: 708, training loss: 0.00087
Epoch: 6, step: 709, training loss: 0.00057
Epoch: 6, step: 710, training loss: 0.00163
Epoch: 6, step: 711, training loss: 0.00065
Epoch: 6, step: 712, training loss: 0.00127
Epoch: 6, step: 713, training loss: 0.00044
Epoch: 6, step: 714, training loss: 0.00062
Epoch: 6, step: 715, training loss: 0.00064
Epoch: 6, step: 716, training loss: 0.00163
Epoch: 6, step: 717, training loss: 0.00072
Epoch: 6, step: 718, training loss: 0.00016
Epoch: 6, step: 719, training loss: 0.00101
Epoch: 6, step: 720, training loss: 0.00072
Epoch: 6, step: 721, training loss: 0.00141
Epoch: 6, step: 722, training loss: 0.00100
Epoch: 6, step: 723, training loss: 0.00179
Epoch: 6, step: 724, training loss: 0.00107
Epoch: 6, step: 725, training loss: 0.00078
Epoch: 6, step: 726, training loss: 0.00201
Epoch: 6, step: 727, training loss: 0.00135
Epoch: 6, step: 728, training loss: 0.00224
Epoch: 6, step: 729, training loss: 0.00263
Epoch: 6, step: 730, training loss: 0.00122
Epoch: 6, step: 731, training loss: 0.00072
Epoch: 6, step: 732, training loss: 0.00093
Epoch: 6, step: 733, training loss: 0.00096
Epoch: 6, step: 734, training loss: 0.00110
Epoch: 6, step: 735, training loss: 0.00065
Epoch: 6, step: 736, training loss: 0.00033
Epoch: 6, step: 737, training loss: 0.00249
Epoch: 6, step: 738, training loss: 0.00172
Epoch: 6, step: 739, training loss: 0.00160
Epoch: 6, step: 740, training loss: 0.00318
Epoch: 6, step: 741, training loss: 0.00111
Epoch: 6, step: 742, training loss: 0.00072
Epoch: 6, step: 743, training loss: 0.00255
Epoch: 6, step: 744, training loss: 0.00205
Epoch: 6, step: 745, training loss: 0.00073
Epoch: 6, step: 746, training loss: 0.00147
Epoch: 6, step: 747, training loss: 0.00172
Epoch: 6, step: 748, training loss: 0.00279
Epoch: 6, step: 749, training loss: 0.00111
Epoch: 6, step: 750, training loss: 0.00113
Epoch: 6, step: 751, training loss: 0.00101
Epoch: 6, step: 752, training loss: 0.00205
Epoch: 6, step: 753, training loss: 0.00164
Epoch: 6, step: 754, training loss: 0.00114
Epoch: 6, step: 755, training loss: 0.00103
Epoch: 6, step: 756, training loss: 0.00091
Epoch: 6, step: 757, training loss: 0.00257
Epoch: 6, step: 758, training loss: 0.00124
Epoch: 6, step: 759, training loss: 0.00330
Epoch: 6, step: 760, training loss: 0.00117
Epoch: 6, step: 761, training loss: 0.00109
Epoch: 6, step: 762, training loss: 0.00125
Epoch: 6, step: 763, training loss: 0.00118
Epoch: 6, step: 764, training loss: 0.00039
Epoch: 6, step: 765, training loss: 0.00134
Epoch: 6, step: 766, training loss: 0.00056
Epoch: 6, step: 767, training loss: 0.00050
Epoch: 6, step: 768, training loss: 0.00196
Epoch: 6, step: 769, training loss: 0.00093
Epoch: 6, step: 770, training loss: 0.00052
Epoch: 6, step: 771, training loss: 0.00157
Epoch: 6, step: 772, training loss: 0.00096
Epoch: 6, step: 773, training loss: 0.00234
Epoch: 6, step: 774, training loss: 0.00079
Epoch: 6, step: 775, training loss: 0.00116
Epoch: 6, step: 776, training loss: 0.00368
Epoch: 6, step: 777, training loss: 0.00119
Epoch: 6, step: 778, training loss: 0.00060
Epoch: 6, step: 779, training loss: 0.00102
Epoch: 6, step: 780, training loss: 0.00199
Epoch: 6, step: 781, training loss: 0.00114
Epoch: 6, step: 782, training loss: 0.00144
Epoch: 6, step: 783, training loss: 0.00013
Epoch: 6, step: 784, training loss: 0.00075
Epoch: 6, step: 785, training loss: 0.00175
Epoch: 6, step: 786, training loss: 0.00147
Epoch: 6, step: 787, training loss: 0.00174
Epoch: 6, step: 788, training loss: 0.00173
Epoch: 6, step: 789, training loss: 0.00128
Epoch: 6, step: 790, training loss: 0.00224
Epoch: 6, step: 791, training loss: 0.00142
Epoch: 6, step: 792, training loss: 0.00059
Epoch: 6, step: 793, training loss: 0.00107
Epoch: 6, step: 794, training loss: 0.00223
Epoch: 6, step: 795, training loss: 0.00051
Epoch: 6, step: 796, training loss: 0.00149
Epoch: 6, step: 797, training loss: 0.00074
Epoch: 6, step: 798, training loss: 0.00064
Epoch: 6, step: 799, training loss: 0.00086
Epoch: 6, step: 800, training loss: 0.00291
Epoch: 6, step: 801, training loss: 0.00071
Epoch: 6, step: 802, training loss: 0.00156
Epoch: 6, step: 803, training loss: 0.00195
Epoch: 6, step: 804, training loss: 0.00187
Epoch: 6, step: 805, training loss: 0.00067
Epoch: 6, step: 806, training loss: 0.00068
Epoch: 6, step: 807, training loss: 0.00075
Epoch: 6, step: 808, training loss: 0.00069
Epoch: 6, step: 809, training loss: 0.00044
Epoch: 6, step: 810, training loss: 0.00036
Epoch: 6, step: 811, training loss: 0.00043
Epoch: 6, step: 812, training loss: 0.00075
Epoch: 6, step: 813, training loss: 0.00172
Epoch: 6, step: 814, training loss: 0.00135
Epoch: 6, step: 815, training loss: 0.00227
Epoch: 6, step: 816, training loss: 0.00125
Epoch: 6, step: 817, training loss: 0.00119
Epoch: 6, step: 818, training loss: 0.00208
Epoch: 6, step: 819, training loss: 0.00044
Epoch: 6, step: 820, training loss: 0.00087
Epoch: 6, step: 821, training loss: 0.00088
Epoch: 6, step: 822, training loss: 0.00037
Epoch: 6, step: 823, training loss: 0.00104
Epoch: 6, step: 824, training loss: 0.00159
Epoch: 6, step: 825, training loss: 0.00101
Epoch: 6, step: 826, training loss: 0.00103
Epoch: 6, step: 827, training loss: 0.00162
Epoch: 6, step: 828, training loss: 0.00105
Epoch: 6, step: 829, training loss: 0.00143
Epoch: 6, step: 830, training loss: 0.00119
Epoch: 6, step: 831, training loss: 0.00098
Epoch: 6, step: 832, training loss: 0.00072
Epoch: 6, step: 833, training loss: 0.00166
Epoch: 6, step: 834, training loss: 0.00252
Epoch: 6, step: 835, training loss: 0.00204
Epoch: 6, step: 836, training loss: 0.00101
Epoch: 6, step: 837, training loss: 0.00256
Epoch: 6, step: 838, training loss: 0.00183
Epoch: 6, step: 839, training loss: 0.00218
Epoch: 6, step: 840, training loss: 0.00200
Epoch: 6, step: 841, training loss: 0.00111
Epoch: 6, step: 842, training loss: 0.00026
Epoch: 6, step: 843, training loss: 0.00232
Epoch: 6, step: 844, training loss: 0.00037
Epoch: 6, step: 845, training loss: 0.00114
Epoch: 6, step: 846, training loss: 0.00251
Epoch: 6, step: 847, training loss: 0.00114
Epoch: 6, step: 848, training loss: 0.00138
Epoch: 6, step: 849, training loss: 0.00176
Epoch: 6, step: 850, training loss: 0.00072
Epoch: 6, step: 851, training loss: 0.00253
Epoch: 6, step: 852, training loss: 0.00052
Epoch: 6, step: 853, training loss: 0.00017
Epoch: 6, step: 854, training loss: 0.00119
Epoch: 6, step: 855, training loss: 0.00056
Epoch: 6, step: 856, training loss: 0.00089
Epoch: 6, step: 857, training loss: 0.00224
Epoch: 6, step: 858, training loss: 0.00017
Epoch: 6, step: 859, training loss: 0.00180
Epoch: 6, step: 860, training loss: 0.00073
Epoch: 6, step: 861, training loss: 0.00094
Epoch: 6, step: 862, training loss: 0.00204
Epoch: 6, step: 863, training loss: 0.00085
Epoch: 6, step: 864, training loss: 0.00308
Epoch: 6, step: 865, training loss: 0.00033
Epoch: 6, step: 866, training loss: 0.00382
Epoch: 6, step: 867, training loss: 0.00096
Epoch: 6, step: 868, training loss: 0.00178
Epoch: 6, step: 869, training loss: 0.00097
Epoch: 6, step: 870, training loss: 0.00133
Epoch: 6, step: 871, training loss: 0.00096
Epoch: 6, step: 872, training loss: 0.00078
Epoch: 6, step: 873, training loss: 0.00111
Epoch: 6, step: 874, training loss: 0.00071
Epoch: 6, step: 875, training loss: 0.00129
Epoch: 6, step: 876, training loss: 0.00059
Epoch: 6, step: 877, training loss: 0.00055
Epoch: 6, step: 878, training loss: 0.00199
Epoch: 6, step: 879, training loss: 0.00236
Epoch: 6, step: 880, training loss: 0.00075
Epoch: 6, step: 881, training loss: 0.00168
Epoch: 6, step: 882, training loss: 0.00060
Epoch: 6, step: 883, training loss: 0.00073
Epoch: 6, step: 884, training loss: 0.00328
Epoch: 6, step: 885, training loss: 0.00143
Epoch: 6, step: 886, training loss: 0.00041
Epoch: 6, step: 887, training loss: 0.00079
Epoch: 6, step: 888, training loss: 0.00096
Epoch: 6, step: 889, training loss: 0.00107
Epoch: 6, step: 890, training loss: 0.00091
Epoch: 6, step: 891, training loss: 0.00159
Epoch: 6, step: 892, training loss: 0.00209
Epoch: 6, step: 893, training loss: 0.00160
Epoch: 6, step: 894, training loss: 0.00068
Epoch: 6, step: 895, training loss: 0.00167
Epoch: 6, step: 896, training loss: 0.00073
Epoch: 6, step: 897, training loss: 0.00220
Epoch: 6, step: 898, training loss: 0.00043
Epoch: 6, step: 899, training loss: 0.00016
Epoch: 6, step: 900, training loss: 0.00236
Epoch: 6, step: 901, training loss: 0.00053
Epoch: 6, step: 902, training loss: 0.00244
Epoch: 6, step: 903, training loss: 0.00199
Epoch: 6, step: 904, training loss: 0.00235
Epoch: 6, step: 905, training loss: 0.00139
Epoch: 6, step: 906, training loss: 0.00255
Epoch: 6, step: 907, training loss: 0.00092
Epoch: 6, step: 908, training loss: 0.00124
Epoch: 6, step: 909, training loss: 0.00100
Epoch: 6, step: 910, training loss: 0.00079
Epoch: 6, step: 911, training loss: 0.00199
Epoch: 6, step: 912, training loss: 0.00133
Epoch: 6, step: 913, training loss: 0.00061
Epoch: 6, step: 914, training loss: 0.00145
Epoch: 6, step: 915, training loss: 0.00164
Epoch: 6, step: 916, training loss: 0.00049
Epoch: 6, step: 917, training loss: 0.00118
Epoch: 6, step: 918, training loss: 0.00147
Epoch: 6, step: 919, training loss: 0.00104
Epoch: 6, step: 920, training loss: 0.00068
Epoch: 6, step: 921, training loss: 0.00154
Epoch: 6, step: 922, training loss: 0.00086
Epoch: 6, step: 923, training loss: 0.00156
Epoch: 6, step: 924, training loss: 0.00128
Epoch: 6, step: 925, training loss: 0.00132
Epoch: 6, step: 926, training loss: 0.00200
Epoch: 6, step: 927, training loss: 0.00113
Epoch: 6, step: 928, training loss: 0.00326
Epoch: 6, step: 929, training loss: 0.00089
Epoch: 6, step: 930, training loss: 0.00150
Epoch: 6, step: 931, training loss: 0.00101
Epoch: 6, step: 932, training loss: 0.00027
Epoch: 6, step: 933, training loss: 0.00098
Epoch: 6, step: 934, training loss: 0.00114
Epoch: 6, step: 935, training loss: 0.00153
Epoch: 6, step: 936, training loss: 0.00133
Epoch: 6, step: 937, training loss: 0.00114
Epoch: 6, step: 938, training loss: 0.00103
Epoch: 6, step: 939, training loss: 0.00125
Epoch: 6, step: 940, training loss: 0.00133
Epoch: 6, step: 941, training loss: 0.00177
Epoch: 6, step: 942, training loss: 0.00093
Epoch: 6, step: 943, training loss: 0.00144
Epoch: 6, step: 944, training loss: 0.00172
Epoch: 6, step: 945, training loss: 0.00014
Epoch: 6, step: 946, training loss: 0.00064
Epoch: 6, step: 947, training loss: 0.00040
Epoch: 6, step: 948, training loss: 0.00109
Epoch: 6, step: 949, training loss: 0.00170
Epoch: 6, step: 950, training loss: 0.00084
Epoch: 6, step: 951, training loss: 0.00096
Epoch: 6, step: 952, training loss: 0.00085
Epoch: 6, step: 953, training loss: 0.00077
Epoch: 6, step: 954, training loss: 0.00088
Epoch: 6, average training loss: 0.00128
Epoch: 6, F1: 82.72166, average dev loss: 0.00257
Loading best check point...
Evaluating on dev set...

DEV F1: 82.72166, avg loss: 0.00257
Evaluating on test set...

Test F1: 38.87531, avg loss: 0.02799
