# GigaBERT
This repo contains code and data for [GigaBERT](https://arxiv.org/pdf/2004.14519.pdf):

	@inproceedings{lan2020gigabert,
	  author     = {Lan, Wuwei and Chen, Yang and Xu, Wei and Ritter, Alan},
  	  title      = {GigaBERT: Zero-shot Transfer Learning from English to Arabic},
  	  booktitle  = {Proceedings of The 2020 Conference on Empirical Methods on Natural Language Processing (EMNLP)},
  	  year       = {2020}
  	} 

# Fine-tuning Experiments
Navigate to the task fodler and check run.sh for sample command. 
Modify the file path for configuration file, model checkpoint and vocabulary.

# Checkpoints
The pre-trained GigaBERT-v3 can be found here: https://drive.google.com/drive/folders/1zgUXz8FQPHmWVNR7tHyPq1E6SmrMuPv6?usp=sharing. 
The pre-trained GigaBERT-v4 can be found here: https://drive.google.com/drive/folders/1uFGzMuTOD7iNsmKQYp_zVuvsJwOaIdar?usp=sharing.

